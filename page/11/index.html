<!doctype html>
<html lang="de"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Iggie Wang&#039;s Cyberspace</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Iggie Wang&#039;s Cyberspace"><meta name="msapplication-TileImage" content="/img/dog.jpg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Iggie Wang&#039;s Cyberspace"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Iggie Wang&#039;s Cyberspace"><meta property="og:url" content="https://hey-kong.github.io/"><meta property="og:site_name" content="Iggie Wang&#039;s Cyberspace"><meta property="og:image" content="https://hey-kong.github.io/img/og_image.png"><meta property="article:author" content="王亮"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hey-kong.github.io"},"headline":"Iggie Wang's Cyberspace","image":["https://hey-kong.github.io/img/og_image.png"],"author":{"@type":"Person","name":"王亮"},"publisher":{"@type":"Organization","name":"Iggie Wang's Cyberspace","logo":{"@type":"ImageObject","url":null}},"description":""}</script><link rel="icon" href="/img/dog.jpg"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.15.2/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Iggie Wang&#039;s Cyberspace</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item search" title="Buscar" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Publicado hace&nbsp;<time dateTime="2021-04-05T08:33:37.000Z" title="2021/4/5 下午4:33:37">2021-04-05</time></span><span class="level-item">16 minutes de lectura (Aproximadamente 2329 palabras)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/04/05/RocksDB-WriteImpl-%E6%B5%81%E7%A8%8B/">RocksDB WriteImpl 流程</a></h1><div class="content"><p><code>本文对 RocksDB 6.7.3 版本的 WriteImpl 流程进行分析。</code></p>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>RocksDB 写入实现主要在 DBImpl::WriteImpl 中，过程主要分为以下三步：</p>
<ul>
<li>把 WriteBatch 加入队列，多个 WriteBatch 成为一个 WriteGroup</li>
<li>将该 WriteGroup 所有的记录对应的日志写到 WAL 文件中</li>
<li>将该 WriteGroup 所有的 WriteBatch 中的一条或者多条记录写到内存中的 Memtable 中</li>
</ul>
<p>其中，每个 WriteBatch 代表一个事务的提交，可以包含多条操作，可以通过调用 WriteBatch::Put/Delete 等操作将对应多条的 key/value 记录加入 WriteBatch 中。</p>
<h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h2><h3 id="WriteThread-JoinBatchGroup"><a href="#WriteThread-JoinBatchGroup" class="headerlink" title="WriteThread::JoinBatchGroup"></a>WriteThread::JoinBatchGroup</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">static WriteThread::AdaptationContext jbg_ctx(<span class="string">&quot;JoinBatchGroup&quot;</span>);</span><br><span class="line">void WriteThread::JoinBatchGroup(Writer* w) &#123;</span><br><span class="line">  TEST_SYNC_POINT_CALLBACK(<span class="string">&quot;WriteThread::JoinBatchGroup:Start&quot;</span>, w);</span><br><span class="line">  assert(w-&gt;batch != nullptr);</span><br><span class="line"></span><br><span class="line">  bool linked_as_leader = LinkOne(w, &amp;newest_writer_);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (linked_as_leader) &#123;</span><br><span class="line">    SetState(w, STATE_GROUP_LEADER);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  TEST_SYNC_POINT_CALLBACK(<span class="string">&quot;WriteThread::JoinBatchGroup:Wait&quot;</span>, w);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (!linked_as_leader) &#123;</span><br><span class="line">    /**</span><br><span class="line">     * Wait util:</span><br><span class="line">     * 1) An existing leader pick us as the new leader when it finishes</span><br><span class="line">     * 2) An existing leader pick us as its follewer and</span><br><span class="line">     * 2.1) finishes the memtable writes on our behalf</span><br><span class="line">     * 2.2) Or tell us to finish the memtable writes <span class="keyword">in</span> pralallel</span><br><span class="line">     * 3) (pipelined write) An existing leader pick us as its follower and</span><br><span class="line">     *    finish book-keeping and WAL write <span class="keyword">for</span> us, enqueue us as pending</span><br><span class="line">     *    memtable writer, and</span><br><span class="line">     * 3.1) we become memtable writer group leader, or</span><br><span class="line">     * 3.2) an existing memtable writer group leader tell us to finish memtable</span><br><span class="line">     *      writes <span class="keyword">in</span> parallel.</span><br><span class="line">     */</span><br><span class="line">    TEST_SYNC_POINT_CALLBACK(<span class="string">&quot;WriteThread::JoinBatchGroup:BeganWaiting&quot;</span>, w);</span><br><span class="line">    AwaitState(w, STATE_GROUP_LEADER | STATE_MEMTABLE_WRITER_LEADER |</span><br><span class="line">                      STATE_PARALLEL_MEMTABLE_WRITER | STATE_COMPLETED,</span><br><span class="line">               &amp;jbg_ctx);</span><br><span class="line">    TEST_SYNC_POINT_CALLBACK(<span class="string">&quot;WriteThread::JoinBatchGroup:DoneWaiting&quot;</span>, w);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>每个事务提交请求都会生成一个 WriteBatch 对象，进入 WriteImpl 函数后各自的线程首先调用 JoinBatchGroup 来加入到队列。该队列主要核心的实现在于 LinkOne 函数，通过 CAS 无锁将多个线程的请求组成请求链表：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">bool WriteThread::LinkOne(Writer* w, std::atomic&lt;Writer*&gt;* newest_writer) &#123;</span><br><span class="line">  assert(newest_writer != nullptr);</span><br><span class="line">  assert(w-&gt;state == STATE_INIT);</span><br><span class="line">  Writer* writers = newest_writer-&gt;load(std::memory_order_relaxed);</span><br><span class="line">  <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    // If write stall <span class="keyword">in</span> effect, and w-&gt;no_slowdown is not <span class="literal">true</span>,</span><br><span class="line">    // block here until stall is cleared. If its <span class="literal">true</span>, <span class="keyword">then</span> <span class="built_in">return</span></span><br><span class="line">    // immediately</span><br><span class="line">    <span class="keyword">if</span> (writers == &amp;write_stall_dummy_) &#123;</span><br><span class="line">      <span class="keyword">if</span> (w-&gt;no_slowdown) &#123;</span><br><span class="line">        w-&gt;status = Status::Incomplete(<span class="string">&quot;Write stall&quot;</span>);</span><br><span class="line">        SetState(w, STATE_COMPLETED);</span><br><span class="line">        <span class="built_in">return</span> <span class="literal">false</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      // Since no_slowdown is <span class="literal">false</span>, <span class="built_in">wait</span> here to be notified of the write</span><br><span class="line">      // stall clearing</span><br><span class="line">      &#123;</span><br><span class="line">        MutexLock lock(&amp;stall_mu_);</span><br><span class="line">        writers = newest_writer-&gt;load(std::memory_order_relaxed);</span><br><span class="line">        <span class="keyword">if</span> (writers == &amp;write_stall_dummy_) &#123;</span><br><span class="line">          stall_cv_.Wait();</span><br><span class="line">          // Load newest_writers_ again since it may have changed</span><br><span class="line">          writers = newest_writer-&gt;load(std::memory_order_relaxed);</span><br><span class="line">          <span class="built_in">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    w-&gt;link_older = writers;</span><br><span class="line">    <span class="keyword">if</span> (newest_writer-&gt;compare_exchange_weak(writers, w)) &#123;</span><br><span class="line">      <span class="built_in">return</span> (writers == nullptr);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>write_group 链表结构如下：</p>
<img src="/2021/04/05/RocksDB-WriteImpl-%E6%B5%81%E7%A8%8B/write_group.png" style="zoom:100%;">

<p>每个 writer 在头部插入，插入时如果发现 link_older 为空，则此 writer 成为 write_group 的 Leader（即链表尾为 Leader）。</p>
<p>在 JoinBatchGroup 中，如果 writer 不是 Leader（在后文把不是 Leader 的 writer 称为 Follower），则会调用 AwaitState 等待被唤醒。</p>
<blockquote>
<p>PS：由于条件锁 Context Switches 代价高，Rocksdb 在 AwaitState 也做了优化，将 pthread_cond_wait 拆成 3 步来做，本文不对该优化进行详细描述。</p>
</blockquote>
<h3 id="WriteImpl-写日志"><a href="#WriteImpl-写日志" class="headerlink" title="WriteImpl 写日志"></a>WriteImpl 写日志</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (w.state == WriteThread::STATE_GROUP_LEADER) &#123;</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  last_batch_group_size_ =</span><br><span class="line">      write_thread_.EnterAsBatchGroupLeader(&amp;w, &amp;wal_write_group);</span><br><span class="line">  const SequenceNumber current_sequence =</span><br><span class="line">      write_thread_.UpdateLastSequence(versions_-&gt;LastSequence()) + 1;</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (w.status.ok() &amp;&amp; !write_options.disableWAL) &#123;</span><br><span class="line">    PERF_TIMER_GUARD(write_wal_time);</span><br><span class="line">    stats-&gt;AddDBStats(InternalStats::kIntStatsWriteDoneBySelf, 1);</span><br><span class="line">    RecordTick(stats_, WRITE_DONE_BY_SELF, 1);</span><br><span class="line">    <span class="keyword">if</span> (wal_write_group.size &gt; 1) &#123;</span><br><span class="line">      stats-&gt;AddDBStats(InternalStats::kIntStatsWriteDoneByOther,</span><br><span class="line">                        wal_write_group.size - 1);</span><br><span class="line">      RecordTick(stats_, WRITE_DONE_BY_OTHER, wal_write_group.size - 1);</span><br><span class="line">    &#125;</span><br><span class="line">    w.status = WriteToWAL(wal_write_group, log_writer, log_used,</span><br><span class="line">                          need_log_sync, need_log_dir_sync, current_sequence);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  write_thread_.ExitAsBatchGroupLeader(wal_write_group, w.status);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>成为 Leader 的 writer，负责批量写入 WAL。在写 WAL 前，首先调用 EnterAsBatchGroupLeader 函数：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">size_t WriteThread::EnterAsBatchGroupLeader(Writer* leader,</span><br><span class="line">                                            WriteGroup* write_group) &#123;</span><br><span class="line">  assert(leader-&gt;link_older == nullptr);</span><br><span class="line">  assert(leader-&gt;batch != nullptr);</span><br><span class="line">  assert(write_group != nullptr);</span><br><span class="line"></span><br><span class="line">  size_t size = WriteBatchInternal::ByteSize(leader-&gt;batch);</span><br><span class="line"></span><br><span class="line">  // Allow the group to grow up to a maximum size, but <span class="keyword">if</span> the</span><br><span class="line">  // original write is small, <span class="built_in">limit</span> the growth so we <span class="keyword">do</span> not slow</span><br><span class="line">  // down the small write too much.</span><br><span class="line">  size_t max_size = max_write_batch_group_size_bytes;</span><br><span class="line">  const uint64_t min_batch_size_bytes = max_write_batch_group_size_bytes / 8;</span><br><span class="line">  <span class="keyword">if</span> (size &lt;= min_batch_size_bytes) &#123;</span><br><span class="line">    max_size = size + min_batch_size_bytes;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  leader-&gt;write_group = write_group;</span><br><span class="line">  write_group-&gt;leader = leader;</span><br><span class="line">  write_group-&gt;last_writer = leader;</span><br><span class="line">  write_group-&gt;size = 1;</span><br><span class="line">  Writer* newest_writer = newest_writer_.load(std::memory_order_acquire);</span><br><span class="line"></span><br><span class="line">  // This is safe regardless of any db mutex status of the <span class="built_in">caller</span>. Previous</span><br><span class="line">  // calls to ExitAsGroupLeader either didn<span class="string">&#x27;t call CreateMissingNewerLinks</span></span><br><span class="line"><span class="string">  // (they emptied the list and then we added ourself as leader) or had to</span></span><br><span class="line"><span class="string">  // explicitly wake us up (the list was non-empty when we added ourself,</span></span><br><span class="line"><span class="string">  // so we have already received our MarkJoined).</span></span><br><span class="line"><span class="string">  CreateMissingNewerLinks(newest_writer);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  // Tricky. Iteration start (leader) is exclusive and finish</span></span><br><span class="line"><span class="string">  // (newest_writer) is inclusive. Iteration goes from old to new.</span></span><br><span class="line"><span class="string">  Writer* w = leader;</span></span><br><span class="line"><span class="string">  while (w != newest_writer) &#123;</span></span><br><span class="line"><span class="string">    w = w-&gt;link_newer;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    if (w-&gt;sync &amp;&amp; !leader-&gt;sync) &#123;</span></span><br><span class="line"><span class="string">      // Do not include a sync write into a batch handled by a non-sync write.</span></span><br><span class="line"><span class="string">      break;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    if (w-&gt;no_slowdown != leader-&gt;no_slowdown) &#123;</span></span><br><span class="line"><span class="string">      // Do not mix writes that are ok with delays with the ones that</span></span><br><span class="line"><span class="string">      // request fail on delays.</span></span><br><span class="line"><span class="string">      break;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    if (w-&gt;disable_wal != leader-&gt;disable_wal) &#123;</span></span><br><span class="line"><span class="string">      // Do not mix writes that enable WAL with the ones whose</span></span><br><span class="line"><span class="string">      // WAL disabled.</span></span><br><span class="line"><span class="string">      break;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    if (w-&gt;batch == nullptr) &#123;</span></span><br><span class="line"><span class="string">      // Do not include those writes with nullptr batch. Those are not writes,</span></span><br><span class="line"><span class="string">      // those are something else. They want to be alone</span></span><br><span class="line"><span class="string">      break;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    if (w-&gt;callback != nullptr &amp;&amp; !w-&gt;callback-&gt;AllowWriteBatching()) &#123;</span></span><br><span class="line"><span class="string">      // dont batch writes that don&#x27;</span>t want to be batched</span><br><span class="line">      <span class="built_in">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    auto batch_size = WriteBatchInternal::ByteSize(w-&gt;batch);</span><br><span class="line">    <span class="keyword">if</span> (size + batch_size &gt; max_size) &#123;</span><br><span class="line">      // Do not make batch too big</span><br><span class="line">      <span class="built_in">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    w-&gt;write_group = write_group;</span><br><span class="line">    size += batch_size;</span><br><span class="line">    write_group-&gt;last_writer = w;</span><br><span class="line">    write_group-&gt;size++;</span><br><span class="line">  &#125;</span><br><span class="line">  TEST_SYNC_POINT_CALLBACK(<span class="string">&quot;WriteThread::EnterAsBatchGroupLeader:End&quot;</span>, w);</span><br><span class="line">  <span class="built_in">return</span> size;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在这里，通过 CreateMissingNewerLinks 函数来生成一个双向链表，使得可以从 Leader 开始顺序写。创建完成反向写请求链表之后，则开始计算有多少个写请求可以批量的进行，同时更新 write_group 中的批量写尺寸以及个数等信息，EnterAsBatchGroupLeader 取队列时会把此刻所有的 writer 一次性全取完。</p>
<p>该操作完成之后，则进入写 WAL 的流程了。调用 WriteToWAL，在 MergeBatch 函数中，将根据 write_group 生成一个 merged_batch，该 merged_batch 中记录着应当被写入 WAL 的内容。接着就通过 WriteToWAL 将 merged_batch 写入 WAL 中，这里会根据是否设置了 sync 来决定是否对 WAL 进行落盘操作。</p>
<blockquote>
<p>PS：这里有一个优化点，在生成 merged_batch 的时候，假设该写请求的尺寸为一并且该请求需要写 WAL，则 merged_batch 直接复用了该写请求；反之则会复用一个 tmp_batch_ 对象避免频繁的生成 WriteBatch 对象。在写完 WAL 之后，假设复用了 tmp_batch_，则会清空该对象。</p>
</blockquote>
<p>最后，调用 ExitAsBatchGroupLeader，该函数会决定该 Leader 是否为 STATE_MEMTABLE_WRITER_LEADER（MEMTABLE_WRITER_LEADER数量 &lt;= GROUP_LEADER数量），从而进行写 Memtable 流程。</p>
<h3 id="WriteImpl-写-Memtable"><a href="#WriteImpl-写-Memtable" class="headerlink" title="WriteImpl 写 Memtable"></a>WriteImpl 写 Memtable</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">WriteThread::WriteGroup memtable_write_group;</span><br><span class="line">  <span class="keyword">if</span> (w.state == WriteThread::STATE_MEMTABLE_WRITER_LEADER) &#123;</span><br><span class="line">    PERF_TIMER_GUARD(write_memtable_time);</span><br><span class="line">    assert(w.ShouldWriteToMemtable());</span><br><span class="line">    write_thread_.EnterAsMemTableWriter(&amp;w, &amp;memtable_write_group);</span><br><span class="line">    <span class="keyword">if</span> (memtable_write_group.size &gt; 1 &amp;&amp;</span><br><span class="line">        immutable_db_options_.allow_concurrent_memtable_write) &#123;</span><br><span class="line">      write_thread_.LaunchParallelMemTableWriters(&amp;memtable_write_group);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      memtable_write_group.status = WriteBatchInternal::InsertInto(</span><br><span class="line">          memtable_write_group, w.sequence, column_family_memtables_.get(),</span><br><span class="line">          &amp;flush_scheduler_, &amp;trim_history_scheduler_,</span><br><span class="line">          write_options.ignore_missing_column_families, 0 /*log_number*/, this,</span><br><span class="line">          <span class="literal">false</span> /*concurrent_memtable_writes*/, seq_per_batch_, batch_per_txn_);</span><br><span class="line">      versions_-&gt;SetLastSequence(memtable_write_group.last_sequence);</span><br><span class="line">      write_thread_.ExitAsMemTableWriter(&amp;w, memtable_write_group);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (w.state == WriteThread::STATE_PARALLEL_MEMTABLE_WRITER) &#123;</span><br><span class="line">    assert(w.ShouldWriteToMemtable());</span><br><span class="line">    ColumnFamilyMemTablesImpl column_family_memtables(</span><br><span class="line">        versions_-&gt;GetColumnFamilySet());</span><br><span class="line">    w.status = WriteBatchInternal::InsertInto(</span><br><span class="line">        &amp;w, w.sequence, &amp;column_family_memtables, &amp;flush_scheduler_,</span><br><span class="line">        &amp;trim_history_scheduler_, write_options.ignore_missing_column_families,</span><br><span class="line">        0 /*log_number*/, this, <span class="literal">true</span> /*concurrent_memtable_writes*/,</span><br><span class="line">        <span class="literal">false</span> /*seq_per_batch*/, 0 /*batch_cnt*/, <span class="literal">true</span> /*batch_per_txn*/,</span><br><span class="line">        write_options.memtable_insert_hint_per_batch);</span><br><span class="line">    <span class="keyword">if</span> (write_thread_.CompleteParallelMemTableWriter(&amp;w)) &#123;</span><br><span class="line">      MemTableInsertStatusCheck(w.status);</span><br><span class="line">      versions_-&gt;SetLastSequence(w.write_group-&gt;last_sequence);</span><br><span class="line">      write_thread_.ExitAsMemTableWriter(&amp;w, *w.write_group);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>RocksDB 有一个 allow_concurrent_memtable_write 的配置项，开启后可以并发写 memtable（memtable 能设置并发写，但是 WAL 文件不能，因为 WAL 是一个追加写的文件，多个 writer 必须要串行化），所以接下来分为串行写和并行写来进行分析。</p>
<h4 id="串行写-Memtable"><a href="#串行写-Memtable" class="headerlink" title="串行写 Memtable"></a>串行写 Memtable</h4><p>Leader 调用 InsertInto，对 write_group 进行遍历，将 Leader 和 Follower 的 WriteBatch 写入。之后调用 ExitAsMemTableWriter，把所有 Follower 的状态设置为 STATE_COMPLETED，将它们唤醒，最后再把 Leader 的状态设置为 STATE_COMPLETED。</p>
<h4 id="并行写-Memtable"><a href="#并行写-Memtable" class="headerlink" title="并行写 Memtable"></a>并行写 Memtable</h4><p>调用 LaunchParallelMemTableWriters，遍历 write_group 把 Leader 和 Follower 的状态都设置为 STATE_PARALLEL_MEMTABLE_WRITER，将等待的线程唤醒。最后所有 writer 通过调用 InsertInto 来将 WriteBatch 写入 MemTable 中。writer 完成了 MemTable 的写操作之后，都会调用 CompleteParallelMemTableWriter 函数。该函数会将该 write_group 中运行的任务数减一，当运行中的任务数为零的时候就代表了所有的线程都完成了操作，调用 ExitAsMemTableWriter 把 Leader 的状态设置为 STATE_COMPLETED，反之则会进入等待状态，等待当前其他的写任务完成。</p>
<p>无论是串行写还是并行写，写入 MemTable 完成之后，还有一项工作，就是在取队列时获取 newest_writer_ 和当前时间点处，可能又有很多的写请求产生了，所以批量任务中最后一个完成的线程必须负责重新指定 Leader 给堆积写请求链表的尾部，让其接过 Leader 角色继续进行批量提交。可以看到，串行写和并行写最后都会调用 ExitAsMemTableWriter，正是在该函数中完成了该项工作。</p>
<blockquote>
<p>PS：在高并发场景下，Follow 调用 AwaitState 的平均等待时延差不多是写 WAL 时延的两倍。因为获取 newest_writer_ 后，可能又来了许多写请求，这些写请求先要等待此时的 Leader 完成写流程，还要等待下个 Leader，也就是和这些写请求是同一个 write_group 的 Leader 完成写 WAL 才能被唤醒。</p>
</blockquote>
<h2 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h2><img src="/2021/04/05/RocksDB-WriteImpl-%E6%B5%81%E7%A8%8B/rocksdb_write.jpg" style="zoom:100%;">

<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/facebook/rocksdb/tree/v6.7.3">Rocksdb Source Code 6.7.3</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_43479736/article/details/109056437">rocksdb写流程DBImpl::WriteImpl()源代码分析</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/161797527">RocksDB写入流程</a></li>
<li><a target="_blank" rel="noopener" href="https://gocode.cc/project/13/article/183">RocksDB 写流程分析</a></li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Publicado hace&nbsp;<time dateTime="2021-03-28T07:45:42.000Z" title="2021/3/28 下午3:45:42">2021-03-28</time></span><span class="level-item">13 minutes de lectura (Aproximadamente 1977 palabras)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/03/28/Vectorization-vs-Compilation-in-Query-Execution/">Vectorization vs. Compilation in Query Execution</a></h1><div class="content"><h2 id="当代-CPU-特性"><a href="#当代-CPU-特性" class="headerlink" title="当代 CPU 特性"></a>当代 CPU 特性</h2><h3 id="超标量流水线与乱序执行"><a href="#超标量流水线与乱序执行" class="headerlink" title="超标量流水线与乱序执行"></a>超标量流水线与乱序执行</h3><p>CPU指令的执行可以分为5个阶段：取指令、指令译码、执行指令、访存取数、结果写回。</p>
<p><strong>流水线</strong>：一套控制单元可以同时执行多条指令，不需要等到上一条指令执行完就可以执行下一条指令。</p>
<p><strong>超标量</strong>：一个 CPU 核有多套控制单元，因此可以有多条 pipeline 并发执行。CPU 还会维护一个乱序执行的指令窗口，窗口中的无数据依赖的指令就可以被取来并发执行。并发指令越多越好，因为这样指令之间没有依赖，并发流水线的执行会更加的流畅。</p>
<h3 id="分支预测"><a href="#分支预测" class="headerlink" title="分支预测"></a>分支预测</h3><p>遇到 if/switch 这种判断跳转的指令时会产生分支预测，分支预测系统会决定流水线接下来是载入紧挨着判断指令的下一条指令，还是载入跳转到另一个地址的指令。如果 CPU 的预测是正确的，那么判断指令结果出来的那一刻，真正需要执行的指令已经执行到尾声了，这时候只需要继续执行即可；如果CPU的预测是错误的，那么会把执行到尾声的错误指令全部清空，恢复到从未执行过的状态，然后再执行正确的指令。</p>
<p>程序分支越少或者是分支预测成功率越高，对流水线的执行就越有利，因为如果预测失败了，是要丢弃当前 pipeline 的所有指令重新 flush，这个过程往往会消耗掉十几个 CPU 周期。</p>
<h3 id="多级存储与数据预取"><a href="#多级存储与数据预取" class="headerlink" title="多级存储与数据预取"></a>多级存储与数据预取</h3><p>当数据在寄存器，cache 或者内存中，CPU 取数据的速度并不是在一个个数量级上的。CPU 取指令/数据的时候并不是直接从内存中取的，通常 CPU 和内存中会有多级缓存，分别为 L1，L2，L3 cache，其中 L1 cache 又可以分为 L1-data cache，L1-instruction cache。先从 cache 中取数据，若不存在，才访问内存。访问内存的时候会同时把访问数据相邻的一些数据一起加载进 cache 中。</p>
<p>预取指的是若数据存在线性访问的模式，CPU会主动把后续的内存块预先加载进cache中。</p>
<h3 id="SIMD"><a href="#SIMD" class="headerlink" title="SIMD"></a>SIMD</h3><p>单指令多数据流，对于数据密集型的程序来说，可能会需要对大量不同的数据进行相同的运算。SIMD 引入了一组大容量的寄存器，比如 128 位，256 位。可以将这多个数据按次序同时放到一个寄存器。同时，CPU 新增了处理这种大容量寄存器的指令，可以在一个指令周期内完成多个数据的运算。</p>
<h2 id="早期解释执行模型"><a href="#早期解释执行模型" class="headerlink" title="早期解释执行模型"></a>早期解释执行模型</h2><p>大多数的 query 解释器模型都是使用基于迭代器的火山模型，如下图所示。每个算子看成一个 iterator，iterator 会提供一个 next 方法，每个 next 方法只会产生一个 tuple，可以理解为一行数据。查询执行的时候，查询树自顶向下调用 next 接口，数据则自底向上被拉取处理，层层计算返回结果。所以火山模型属于 pull 模型。</p>
<img src="/2021/03/28/Vectorization-vs-Compilation-in-Query-Execution/Volcano.jpg" style="zoom:100%;">

<p>Volcano 模型简单灵活，且这种设计不用占用过多的内存。火山模型将更多的内存资源用于磁盘 IO 的缓存设计而没有优化 CPU 的执行效率，这在当时的硬件基础上是很自然的权衡。但是现在 CPU 的硬件环境与大数据场景下，性能表现却差强人意。主要有如下几点原因：</p>
<ol>
<li><p>时间都花在了query plan上，而不是计算上<br>next 函数实现为虚函数，调用虚函数的时候要去查虚函数表，编译器无法对虚函数进行 inline 优化。同时会带来分支预测的开销，导致一次错误的 CPU 分支预测，需要多花费十几个 CPU 周期的开销。</p>
</li>
<li><p>CPU cache利用率低<br>next 方法一次只返回一个元组，元组通常采用行存储，如果仅需访问其中某个字段但是每次都将整行数据填入 CPU cache，将导致那些不会被访问的字段也放在了 Cache 中，使得 cache 利用率非常低。</p>
</li>
</ol>
<h2 id="编译执行"><a href="#编译执行" class="headerlink" title="编译执行"></a>编译执行</h2><p>编译执行指的是运行时期的代码生成生成技术。在执行过程中生成编译执行代码，避免过多的虚函数调用和解析执行，因为在执行之初我们是知道关系代数的 Schema 信息。在具备 Schema 信息的情况下，事先生成好的代码，可以有效减少很多执行分支预测开销。</p>
<img src="/2021/03/28/Vectorization-vs-Compilation-in-Query-Execution/Compilation.jpg" style="zoom:100%;">

<p>上图右边的代码非常紧凑，有效消除了字段个数，字段大小，字段类型，对于数据量特别多的处理场景，可以大大减少CPU开销，提高性能。</p>
<p>编译执行以数据为中心，消灭了火山模型中的大量虚函数调用开销。甚至使大部分指令执行，可以直接从寄存器取数，极大提高了执行效率。</p>
<p>在 Java 中通过 JIT 来实现，在 C++ 中通过 LLVM 来实现 codegen，对于 OLAP 这种运行时间较长的 query 来说，通常编译的时间是可以忽略的。</p>
<h2 id="向量化执行"><a href="#向量化执行" class="headerlink" title="向量化执行"></a>向量化执行</h2><p>向量可以理解为按列组织的一组数据，连续存储的一列数据，在内存中可以表示为一个向量。</p>
<img src="/2021/03/28/Vectorization-vs-Compilation-in-Query-Execution/Vectorization.png" style="zoom:100%;">

<p>向量模型和火山模型的本质区别就在于，数据处理的基本单元不再是按行组织的 tuple，而是按列组织的多个向量，我们常说的一个 chunk 其实就是多个 vector 的集合，就是多个列的意思。</p>
<p>向量化执行好处是：由于每次 next 都是处理一批数据，那么大大减少了虚函数调用的次数，分支预测的成功概率会提升，减少了分支预测的开销，并且充分发挥 SIMD 指令并行计算的优势；还可以和列式存储有效结合在一起，减少数据额外转换的 overhead。</p>
<h2 id="向量化和编译执行比较"><a href="#向量化和编译执行比较" class="headerlink" title="向量化和编译执行比较"></a>向量化和编译执行比较</h2><p>向量化执行的主要访存开销在于像 join 这种算子的物化开销，物化就是从寄存器把数据读到内存中。而编译执行，tuple 可以一直留在寄存器中，一个 operator 处理完后，给另外一个 operator 继续处理。除非遇到不得不物化的情况。</p>
<p>向量化执行模型的循环较短，并发度高，可以同时有更多的指令等待取数。编译执行循环内部会包含多个 operator 的运算，这些有依赖关系的指令占据了大部分的乱序执行窗口，并发度低。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li>Sompolski, J. ,  M. Zukowski , and  P. A. Boncz . “Vectorization vs. Compilation in Query Execution.” International Workshop on Data Management on New Hardware ACM, 2011.</li>
<li>S. Wanderman-Milne and N. Li, “Runtime Code Generation in Cloudera Impala,” IEEE Data Eng. Bull., vol. 37, no. 1, pp. 31–37, 2014.</li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/63996040">向量化与编译执行浅析</a></li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Publicado hace&nbsp;<time dateTime="2021-02-21T08:28:03.000Z" title="2021/2/21 下午4:28:03">2021-02-21</time></span><span class="level-item">11 minutes de lectura (Aproximadamente 1601 palabras)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/02/21/%E7%BC%93%E5%AD%98%E8%AE%BE%E8%AE%A1/">缓存设计</a></h1><div class="content"><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>在设计与开发高性能的系统时，基本都离不开缓存的设计。没有缓存对系统的加速和阻挡大量的请求直接落到系统的底层，系统是很难撑住高并发的冲击。无论是在 CPU 的 L1,L2,L3 缓存，数据库的 sql 语句执行缓存，系统应用的本地缓存，缓存总是解决性能的一把利器。本文主要探讨缓存带来的问题以及缓存方案的设计。</p>
<h2 id="缓存带来的问题"><a href="#缓存带来的问题" class="headerlink" title="缓存带来的问题"></a>缓存带来的问题</h2><h3 id="缓存一致性"><a href="#缓存一致性" class="headerlink" title="缓存一致性"></a>缓存一致性</h3><p>引入缓存后，主要是解决读的性能问题，但是数据总是要更新的，会存在<strong>操作隔离性</strong>和<strong>更新原子性</strong>的问题，是先更新缓存还是先更新数据库呢？</p>
<ul>
<li><p>操作隔离性：一条数据的更新涉及到存储和缓存两套系统，如果多个线程同时操作一条数据，并且没有方案保证多个操作之间的有序执行，就可能会发生更新顺序错乱导致数据不一致的问题</p>
<img src="/2021/02/21/%E7%BC%93%E5%AD%98%E8%AE%BE%E8%AE%A1/1.png" style="zoom:100%;">
</li>
<li><p>更新原子性：引入缓存后，我们需要保证缓存和存储要么同时更新成功，要么同时更新失败，否则部分更新成功就会导致缓存和存储数据不一致的问题</p>
<img src="/2021/02/21/%E7%BC%93%E5%AD%98%E8%AE%BE%E8%AE%A1/2.png" style="zoom:100%;">
</li>
</ul>
<ol>
<li>先更新缓存再更新数据库：更新缓存后，后续的读操作都会先从缓存获取从而获取的是最新的数据，但是如果第二步更新数据库失败，那么数据需要回滚，导致先前获取的数据是脏数据来带不可逆的业务影响</li>
<li>先更新数据库后更新缓存：先更新数据库，但是缓存没有更新，再将数据从数据库同步到缓存这一过程中，所有的读操作读的都是旧数据，会带来一定问题，牺牲小概率的一致性</li>
</ol>
<h3 id="缓存击穿"><a href="#缓存击穿" class="headerlink" title="缓存击穿"></a>缓存击穿</h3><p>缓存击穿是指：业务操作访问缓存时，没有访问到数据，又去访问数据库，但是从数据库也没有查询到数据，也不写入缓存，从而导致这些操作每次都需要访问数据库，造成缓存击穿。</p>
<p>解决办法一般有两种：</p>
<ol>
<li>将每次从数据库获取的数据，即使是空值也先写入缓存，但是过期时间设置得比较短，后续的访问都直接从缓存中获取空值返回即可</li>
<li>通过 Bloom filter 记录 key 是否存在，从而避免无效数据的查询</li>
</ol>
<h3 id="缓存雪崩"><a href="#缓存雪崩" class="headerlink" title="缓存雪崩"></a>缓存雪崩</h3><p>缓存雪崩是指：由于大量的热数据设置了相同或接近的过期时间，导致缓存在某一时刻密集失效，大量请求全部转发到数据库，或者是某个冷数据瞬间涌入大量访问数据库。</p>
<p>主要解决方法：</p>
<ol>
<li>所有数据的过期时间不要设置成一样，防止出现数据批量失效，导致缓存雪崩的情况</li>
<li>采用互斥锁的方式：这里需要使用到分布式锁，在缓存失效后，如果访问同一数据的操作需要访问数据并去更新缓存时，对这些操作都加锁，保证只有一个线程去访问数据并更新缓存，后续所有操作还是从缓存中获取数据，如果一定时间没有获取到就返回默认值或返回空值。这样可以防止数据库压力增大，但是用户体验会降低</li>
<li>后台更新：业务操作需要访问缓存没有获取到数据时，不访问数据库更新缓存，只返回默认值。通过后台线程去更新缓存，这里有两种更新方式：<ul>
<li>启动定时任务定时扫描所有缓存，如果不存在就更新，该方法导致扫描 key 间隔时间过长，数据更新不实时，期间业务操作一直会返回默认值，用户体验比较差</li>
<li>业务线程发现缓存失效后通过消息队列去更新缓存，这里因为是分布式的所以可能有很多条消息，需要考虑消息的幂等性。这种方式依赖消息队列，但是缓存更新及时，用户体验比较好，缺点是系统复杂度增高了</li>
</ul>
</li>
</ol>
<h2 id="缓存方案的设计"><a href="#缓存方案的设计" class="headerlink" title="缓存方案的设计"></a>缓存方案的设计</h2><h3 id="读取"><a href="#读取" class="headerlink" title="读取"></a>读取</h3><p>读数据流程很简单，先去缓存读取数据，如果缓存 MISS，则需要从存储中读取数据，并将数据更新到缓存系统中，整个流程如下所示：</p>
<img src="/2021/02/21/%E7%BC%93%E5%AD%98%E8%AE%BE%E8%AE%A1/3.png" style="zoom:100%;">

<h3 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h3><p>通常选择以下方案，保障数据可靠性，尽量减少数据不一致的出现，通过 TTL 超时机制在一定时间段后自动解决数据不一致现象：</p>
<ol>
<li>更新数据库，保证数据可靠性</li>
<li>更新缓存，有以下 2 个策略：<ul>
<li>惰性更新：删除缓存中对应的 item，等待下次读 MISS 再缓存（推荐）</li>
<li>积极更新：将最新的数据更新到缓存</li>
</ul>
</li>
</ol>
<img src="/2021/02/21/%E7%BC%93%E5%AD%98%E8%AE%BE%E8%AE%A1/4.png" style="zoom:100%;">

<h3 id="淘汰"><a href="#淘汰" class="headerlink" title="淘汰"></a>淘汰</h3><p>缓存的作用是将热点数据缓存到内存实现加速，内存的成本要远高于磁盘，因此我们通常仅仅缓存热数据在内存，冷数据需要定期的从内存淘汰，数据的淘汰通常有两种方案：</p>
<ol>
<li>主动淘汰。通过对 Key 设置 TTL 的方式来让 Key 定期淘汰，以保障冷数据不会长久的占有内存（推荐）</li>
<li>被动淘汰。当缓存已用内存超过 Maxmemory 限定时触发淘汰，在 Maxmemory 的场景下缓存的质量是不可控的，因为每次缓存一个 Key 都可能需要去淘汰一个 Key</li>
</ol>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/ziJ8OFiLA1it3sQvycN7Mg">翻越缓存的三座大山</a></li>
</ol>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/10/">Anterior</a></div><div class="pagination-next"><a href="/page/12/">Siguiente</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/10/">10</a></li><li><a class="pagination-link is-current" href="/page/11/">11</a></li><li><a class="pagination-link" href="/page/12/">12</a></li><li><a class="pagination-link" href="/page/13/">13</a></li><li><a class="pagination-link" href="/page/14/">14</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/logo.jpg" alt="Iggie Wang (王亮)"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Iggie Wang (王亮)</p><p class="is-size-6 is-block">WNLO, HUST</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Wuhan, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Entradas</p><a href="/archives"><p class="title">42</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categorías</p><a href="/categories"><p class="title">0</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Etiquetas</p><a href="/tags"><p class="title">15</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/hey-kong" target="_blank" rel="noopener">SEGUIR</a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Enlaces</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/hey-kong" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="iggiewang@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Email</span></span><span class="level-right"><span class="level-item tag">iggiewang@gmail.com</span></span></a></li></ul></div></div></div><!--!--><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archivos</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/08/"><span class="level-start"><span class="level-item">August 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/07/"><span class="level-start"><span class="level-item">July 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/12/"><span class="level-start"><span class="level-item">December 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/06/"><span class="level-start"><span class="level-item">June 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/05/"><span class="level-start"><span class="level-item">May 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/03/"><span class="level-start"><span class="level-item">March 2022</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/02/"><span class="level-start"><span class="level-item">February 2022</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/01/"><span class="level-start"><span class="level-item">January 2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/10/"><span class="level-start"><span class="level-item">October 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/09/"><span class="level-start"><span class="level-item">September 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/08/"><span class="level-start"><span class="level-item">August 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/07/"><span class="level-start"><span class="level-item">July 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/05/"><span class="level-start"><span class="level-item">May 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/04/"><span class="level-start"><span class="level-item">April 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/03/"><span class="level-start"><span class="level-item">March 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/02/"><span class="level-start"><span class="level-item">February 2021</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/01/"><span class="level-start"><span class="level-item">January 2021</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Etiquetas</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/CSAPP/"><span class="tag">CSAPP</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Cache/"><span class="tag">Cache</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Chinese/"><span class="tag">Chinese</span><span class="tag">37</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Database/"><span class="tag">Database</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Edge-computing/"><span class="tag">Edge computing</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/English/"><span class="tag">English</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/File-System/"><span class="tag">File System</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Key-Value-Store/"><span class="tag">Key-Value Store</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/KubeEdge/"><span class="tag">KubeEdge</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Learned-Index/"><span class="tag">Learned Index</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RDMA/"><span class="tag">RDMA</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Rocksdb/"><span class="tag">Rocksdb</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/docker/"><span class="tag">docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/etcd/"><span class="tag">etcd</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/k8s/"><span class="tag">k8s</span><span class="tag">4</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Iggie Wang&#039;s Cyberspace</a><p class="is-size-7"><span>&copy; 2024 王亮</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" defer></script><script>moment.locale("default");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Volver arriba" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "Este sitio web utiliza cookies para mejorar su experiencia.",
          dismiss: "¡Entendido!",
          allow: "Permitir cookies",
          deny: "Descenso",
          link: "Aprende más",
          policy: "Política de cookies",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Teclea algo..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Teclea algo...","untitled":"(Sin título)","posts":"Entradas","pages":"Páginas","categories":"Categorías","tags":"Etiquetas"});
        });</script></body></html>