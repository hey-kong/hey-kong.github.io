<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Iggie Wang&#039;s Cyberspace</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Iggie Wang&#039;s Cyberspace"><meta name="msapplication-TileImage" content="/img/dog.jpg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Iggie Wang&#039;s Cyberspace"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Iggie Wang&#039;s Cyberspace"><meta property="og:url" content="https://www.iggiewang.cn/"><meta property="og:site_name" content="Iggie Wang&#039;s Cyberspace"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://www.iggiewang.cn/img/og_image.png"><meta property="article:author" content="王亮"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.iggiewang.cn"},"headline":"Iggie Wang's Cyberspace","image":["https://www.iggiewang.cn/img/og_image.png"],"author":{"@type":"Person","name":"王亮"},"publisher":{"@type":"Organization","name":"Iggie Wang's Cyberspace","logo":{"@type":"ImageObject","url":null}},"description":""}</script><link rel="icon" href="/img/dog.jpg"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.15.2/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Iggie Wang&#039;s Cyberspace</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/2021/01/16/About-me">About</a></div><div class="navbar-end"><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-07-13T09:09:37.000Z" title="2021/7/13 下午5:09:37">2021-07-13</time>发表</span><span class="level-item">13 分钟读完 (大约1976个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/07/13/Optimization-of-Common-Table-Expressions-in-MPP-Database-Systems-%E6%A6%82%E8%BF%B0/">Optimization of Common Table Expressions in MPP Database Systems 概述</a></h1><div class="content"><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>论文基于 Orca 对非递归的 CTE 进行了形式化表达和优化，贡献总结如下：</p>
<ul>
<li>在查询中使用 CTE 的上下文中优化 CTE</li>
<li>对于查询中的每个 CTE 引用，CTE 不会每次重新优化，仅在需要的时候才进行，例如下推 fitlers 或者 sort 操作</li>
<li>基于 cost 来决定是否对 CTE 进行内联</li>
<li>减少 plan 的搜索空间，加速查询执行，包括下推 predicates 到 CTE，如果 CTE 被引用一次则始终内联，消除掉没有被引用的 CTE</li>
<li>避免死锁，保证 CTE producer 在 CTE consumer 之前执行</li>
</ul>
<h2 id="REPRESENTATION-OF-CTEs"><a href="#REPRESENTATION-OF-CTEs" class="headerlink" title="REPRESENTATION OF CTEs"></a>REPRESENTATION OF CTEs</h2><ol>
<li>CTEProducer：一个 CTE 定义对应一个 CTEProducer</li>
<li>CTEConsumer：query 中引用 CTE 的地方</li>
<li>CTEAnchor：query 中定义 CTE 的 node，CTE 只能被该 CTEAnchor 的子树中引用</li>
<li>Sequence：按序执行它的孩子节点，先执行左节点，再执行右节点，并把右节点做为返回值</li>
</ol>
<p>对于此查询：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">WITH v AS (SELECT i_brand FROM item WHERE i_color &#x3D; ’red’)</span><br><span class="line">SELECT * FROM v as v1, v as v2</span><br><span class="line">WHERE v1.i_brand &#x3D; v2.i_brand;</span><br></pre></td></tr></table></figure>
<p>它的 Logical representation 如下所示：</p>
<div align="center">
<img src="/2021/07/13/Optimization-of-Common-Table-Expressions-in-MPP-Database-Systems-%E6%A6%82%E8%BF%B0/1.png" style="zoom:100%;">
</div>

<p>它的 Execution plans 如下所示：</p>
<div align="center">
<img src="/2021/07/13/Optimization-of-Common-Table-Expressions-in-MPP-Database-Systems-%E6%A6%82%E8%BF%B0/2.png" style="zoom:100%;">
</div>

<h2 id="PLAN-ENUMERATION"><a href="#PLAN-ENUMERATION" class="headerlink" title="PLAN ENUMERATION"></a>PLAN ENUMERATION</h2><p>CTE 是否内联需要取决于 cost，因此需要枚举出 CTE 在不同引用地方内联前后的计划代价。Orca 中定义了 Memo，下图是初始的逻辑查询在 Memo 中的结构，每个编号就是一个 Memo Group：</p>
<div align="center">
<img src="/2021/07/13/Optimization-of-Common-Table-Expressions-in-MPP-Database-Systems-%E6%A6%82%E8%BF%B0/3.png" style="zoom:100%;">
</div>

<h3 id="Transformation-Rules"><a href="#Transformation-Rules" class="headerlink" title="Transformation Rules"></a>Transformation Rules</h3><p>Transformation Rules 可以看做 Memo Group 一个输入（或者一个函数），Memo Group 根据这个规则展开产生另一些 expression 放在同一个 Memo Group 中。对于每个 CTE，我们生成内联或不内联 CTE 的备选方案。</p>
<div align="center">
<img src="/2021/07/13/Optimization-of-Common-Table-Expressions-in-MPP-Database-Systems-%E6%A6%82%E8%BF%B0/4.png" style="zoom:100%;">
</div>

<ul>
<li><p>第一条规则应用于 CTEAnchor 运算符。它在 Group 0 中生成一个 Sequence 操作符，这样序列的左节点就是表示 CTE 定义的整个 Plan Tree —— 根据需要创建尽可能多的新 Group (Group 4、5和6) —— 序列的右节点就是 CTEAnchor (Group 1) 的原始节点。</p>
</li>
<li><p>第二条规则也应用于 CTEAnchor，生成 Group 0 中的 NoOp 运算符，其孩子节点是 CTEAnchor 的孩子节点（Group 1）。</p>
</li>
<li><p>第三条规则应用于 CTEConsumer 运算符，生成 CTE 定义的副本，该副本与 CTEConsumer 属于同一 Group。例如，Group 2 中的 CTEConsumer，添加了 CTE 定义 Select 操作符，并将其子操作符 TableScan 添加到新Group（Group 7）。</p>
</li>
</ul>
<p>该方法产生的 Plan Tree 的组合中并不都是有效的，比如：</p>
<div align="center">
<img src="/2021/07/13/Optimization-of-Common-Table-Expressions-in-MPP-Database-Systems-%E6%A6%82%E8%BF%B0/5.png" style="zoom:100%;">
</div>

<p>a 和 b 都没有 CTEProducer；c 有一个 CTEProducer，没有 CTEConsumer；d 中的 Plan 是有效的，但是只有一个 CTEConsumer 对应于包含的 CTEProducer，是一个失败的 Plan。</p>
<p>通过 Memo 的机制来表达不同的 Plan，基于 cost 选择是否内联。在一个 query 中，CTE 可能有的内联，而有的不内联。内联的好处是能进行普通 query 的优化，比如：下推 predicates，distribution，sorting 等。例如：</p>
<div align="center">
<img src="/2021/07/13/Optimization-of-Common-Table-Expressions-in-MPP-Database-Systems-%E6%A6%82%E8%BF%B0/6.png" style="zoom:100%;">
</div>

<p>CTEConsumer 上游有 predicate: i_color=’red’，Orca在默认情况下会将谓词下推，使其从表达式 c 变为表达式 d。</p>
<h3 id="Avoiding-Invalid-Plans"><a href="#Avoiding-Invalid-Plans" class="headerlink" title="Avoiding Invalid Plans"></a>Avoiding Invalid Plans</h3><p>上述产生的 Plan Tree 会很多，所以需要裁剪掉一些无效的 Plan，例如，使用了 CTEConsumer 却没有 CTEProducer。裁剪算法如下：</p>
<div align="center">
<img src="/2021/07/13/Optimization-of-Common-Table-Expressions-in-MPP-Database-Systems-%E6%A6%82%E8%BF%B0/7.png" style="zoom:100%;">
</div>

<p>CTESpec 表示一个 CTE 的属性对(id, type)，比如：(1, ‘c’)，cteid = 1，type 是 CTEConsumer。该算法简单来说就是遍历 Tree，检查 CTEConsumer 和 CTEProduct 是否配对。具体描述如下：</p>
<ol>
<li>先计算自身的 CTESpec；</li>
<li>遍历所有子节点：<ol>
<li>计算对于该子节点的 CTESpec 的 Request，输入是：前面兄弟节点以及父节点的 specList，来自父节点的 reqParent，得到该子节点应该满足的 reqChild；</li>
<li>子节点调用该函数 DeriveCTEs(child, reqChild)，递归返回子节点的有效的 CTESpecs，即 specChild；</li>
<li>把子节点 DeriveCTE 返回的 specChild 追加到 specList。如果发现有一对 CTEProducer 和 CTEConsumer就从 specList 中去除掉。</li>
</ol>
</li>
<li>对比遍历所有子节点后得到的 specList 与传入的 reqParent 是否 match。如果匹配，则返回当前的 specList。</li>
</ol>
<h3 id="Optimizations-Across-Consumers"><a href="#Optimizations-Across-Consumers" class="headerlink" title="Optimizations Across Consumers"></a>Optimizations Across Consumers</h3><p>上述算法可以枚举出所有 CTE 是否内联的 Plan，另外还有一些其他优化 CTE 的方法。</p>
<h4 id="Predicate-Push-down"><a href="#Predicate-Push-down" class="headerlink" title="Predicate Push-down"></a>Predicate Push-down</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">WITH v as (SELECT i_brand, i_color FROM item</span><br><span class="line">           WHERE i_current_price &lt; 50)</span><br><span class="line">SELECT * FROM v v1, v v2</span><br><span class="line">WHERE v1.i_brand &#x3D; v2.i_brand</span><br><span class="line">AND v1.i_color &#x3D; ’red’</span><br><span class="line">AND v2.i_color &#x3D; ’blue’;</span><br></pre></td></tr></table></figure>
<p>把一个 CTEProducer 对应所有的 CTEConsumer 的 predicates，下推到该 CTEProducer 上，条件通过 OR 组合起来，减少物化的数据量。（注意：因为下推到 CTEProducer 的 predicate 是通过 OR 连接的，因此 CTEConsumer 仍然需要执行原来的 predicate。）</p>
<div align="center">
<img src="/2021/07/13/Optimization-of-Common-Table-Expressions-in-MPP-Database-Systems-%E6%A6%82%E8%BF%B0/8.png" style="zoom:100%;">
</div>

<h4 id="Always-Inlining-Single-use-CTEs"><a href="#Always-Inlining-Single-use-CTEs" class="headerlink" title="Always Inlining Single-use CTEs"></a>Always Inlining Single-use CTEs</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">WITH v as (SELECT i_color FROM item </span><br><span class="line">WHERE i_current_price &lt; 50)</span><br><span class="line">SELECT * FROM v</span><br><span class="line">WHERE v.i_color &#x3D; ’red’;</span><br></pre></td></tr></table></figure>
<p>如果只有一个 CTEConsumer，则始终内联 CTE。</p>
<h4 id="Elimination-of-unused-CTEs"><a href="#Elimination-of-unused-CTEs" class="headerlink" title="Elimination of unused CTEs"></a>Elimination of unused CTEs</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">WITH v as (SELECT i_color FROM item </span><br><span class="line">WHERE i_current_price &lt; 50)</span><br><span class="line">SELECT * FROM item</span><br><span class="line">WHERE item.i_color &#x3D; ’red’;</span><br></pre></td></tr></table></figure>
<p>CTE v 在上述 query 中没有被使用，这种情况可以消除 CTE。另外，对于如下 query：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">WITH v as (SELECT i_current_price p FROM item</span><br><span class="line">           WHERE i_current_price &lt; 50),</span><br><span class="line">     w as (SELECT v1.p FROM v as v1, v as v2</span><br><span class="line">           WHERE v1.p &lt; v2.p)</span><br><span class="line">SELECT * FROM item</span><br><span class="line">WHERE item.i_color &#x3D; ’red’;</span><br></pre></td></tr></table></figure>
<p>CTE v 被引用了两次，而 CTE w 从未被引用。因此，我们可以消除 w 的定义。并且，这样做去掉了对 v 的唯一引用，这意味着我们还可以消除 v 的定义。</p>
<h2 id="CONTEXTUALIZED-OPTIMIZATION"><a href="#CONTEXTUALIZED-OPTIMIZATION" class="headerlink" title="CONTEXTUALIZED OPTIMIZATION"></a>CONTEXTUALIZED OPTIMIZATION</h2><p>对 CTE 是否内联进行枚举之后，Plan 中不同的 CTEConsumer 可能使用不同的优化方案（内联或不内联、下推等）。</p>
<h3 id="Enforcing-Physical-Properties"><a href="#Enforcing-Physical-Properties" class="headerlink" title="Enforcing Physical Properties"></a>Enforcing Physical Properties</h3><p>Orca 通过 top-down 发送处理 Memo Group 中的优化请求来优化候选计划。优化请求是一组表达式要满足的 Physical Properties 上的要求，包括 sort order, distribution, rewindability, CTEs 和 data partitioning 等，也可以没有（即 ANY）。下图以 distribution 为例子，CTE 需要在不同的上下文中满足不同的 Physical Properties。</p>
<div align="center">
<img src="/2021/07/13/Optimization-of-Common-Table-Expressions-in-MPP-Database-Systems-%E6%A6%82%E8%BF%B0/9.png" style="zoom:100%;">
</div>

<ol>
<li>Sequence 算子对 CTEProducer 发射 ANY 的 prop 请求，返回 Hashed(i_sk) 的 prop（表 item 按 i_sk 这一列进行哈希分布）；</li>
<li>上述的 prop 发送到右子树中（结合自身 prop 和父节点的 prop），右子树中的 HashJoin 节点的连接条件需要子节点的数据基于 i_brand 哈希分布，发送请求到 group 2 和 group 3 的 CTEConsumer 中，而 CTEConsumer 并不满足 i_brand 哈希分布的要求，而父节点又需要此 prop，这时就需要在两个 CTEConsumer 分别添加 Redistribute 的算子，把数据按 i_brand 进行哈希，这样才能满足 HashJoin 的要求。</li>
</ol>
<div align="center">
<img src="/2021/07/13/Optimization-of-Common-Table-Expressions-in-MPP-Database-Systems-%E6%A6%82%E8%BF%B0/10.png" style="zoom:100%;">
</div>

<p>与 (a) 相比，(b) 中可以一开始就要求 CTE 按 i_brand 哈希分布，CTEProducer 会发现数据分布不满足要求，然后就可以在 group 5 中添加 Redistribute 的算子，CTEProducer 返回 Hashed(i_brand)，这样 CTEConsumer 就不需要加上 Redistribute 的算子，最终得到一个最优的计划（CTEProducer 只需要计算一遍并保存数据，两个 CTEConsumer 意味着需要读取两遍数据）。</p>
<h3 id="Cost-Estimation"><a href="#Cost-Estimation" class="headerlink" title="Cost Estimation"></a>Cost Estimation</h3><p>CTEProducer 和 CTEConsumer 的 cost 分开计算：</p>
<ul>
<li>CTEProducer 的 cost 是 CTE 自身的 cost，加上物化写磁盘的 cost</li>
<li>CTEConsumer 的 cost 是读取物化结果的 cost，类似 scan 算子</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li>El-Helw A, Raghavan V, Soliman M A, et al. Optimization of common table expressions in mpp database systems[J]. Proceedings of the VLDB Endowment, 2015, 8(12): 1704-1715.</li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/372776415">《Optimization of Common Table Expressions in MPP Database Systems》论文导读</a></li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-05-06T06:03:34.000Z" title="2021/5/6 下午2:03:34">2021-05-06</time>发表</span><span class="level-item">11 分钟读完 (大约1575个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/05/06/WiscKey-Separating-Keys-from-Values-in-SSD-conscious-Storage/">WiscKey: Separating Keys from Values in SSD-conscious Storage</a></h1><div class="content"><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="读写放大"><a href="#读写放大" class="headerlink" title="读写放大"></a>读写放大</h3><p>LSM-Tree key-value 存储存在读写放大的问题，例如对于LevelDB来说：</p>
<ul>
<li><p>写放大：假如每一层的大小是上一层的 10 倍，那么当把 i-1 层中的一个文件合并到 i 层中时，LevelDB 需要读取 i 层中的文件的数量多达 10 个，排序后再将他们写回到 i 层中去。所以这个时候的写放大是 10。对于一个很大的数据集，生成一个新的 SSTable 文件可能会导致 L0-L6 中相邻层之间发生合并操作，这个时候的写放大就是50（L1-L6中每一层是10）。</p>
</li>
<li><p>读放大：(1) 查找一个 key-value 对时，LevelDB 可能需要在多个层中去查找。在最坏的情况下，LevelDB 在 L0 中需要查找 8 个文件，在 L1-L6 每层中需要查找 1 个文件，累计就需要查找 14 个文件。(2) 在一个 SSTable 文件中查找一个 key-value 对时，LevelDB 需要读取该文件的多个元数据块。所以实际读取的数据量应该是：<code>index block + bloom-filter blocks + data block</code>。例如，当查找 1KB 的 key-value 对时，LevelDB 需要读取 16KB 的 index block，4KB的 bloom-filter block 和 4KB 的 data block，总共要读取 24 KB 的数据。在最差的情况下需要读取 14 个 SSTable 文件，所以这个时候的写放大就是 24*14=336。较小的 key-value 对会带来更高的读放大。</p>
</li>
</ul>
<p>WiscKey 论文中针对 LevelDB 测试的读写放大数据：</p>
<div align="center">
<img src="/2021/05/06/WiscKey-Separating-Keys-from-Values-in-SSD-conscious-Storage/1.png" style="zoom:100%;">
</div>

<h3 id="存储硬件"><a href="#存储硬件" class="headerlink" title="存储硬件"></a>存储硬件</h3><p>在 SSD 上，顺序和随机读写性能差异不大。对于写操作而言，由于随机写会对 SSD 的寿命造成影响，顺序写的特性应该保留，对于读操作来说，顺序读和随机读的性能测试如下图所示：</p>
<div align="center">
<img src="/2021/05/06/WiscKey-Separating-Keys-from-Values-in-SSD-conscious-Storage/2.png" style="zoom:100%;">
</div>

<p>每次请求数据的 size 越大，SSD 的随机读与顺序读差距越小，并发数越大，SSD 的随机读与顺序读差距也越小。</p>
<h2 id="WiscKey"><a href="#WiscKey" class="headerlink" title="WiscKey"></a>WiscKey</h2><p>WiscKey 包括四个关键思想：</p>
<p>(1) KV 分离，只有 key 在 LSM-Tree 上。<br>(2) 在 KV 分离后，value 采用顺序追加写，不保序。因此范围查询中，WiscKey 使用并行 SSD 设备的随机读特性查询 value。<br>(3) 使用 crash-consistency 和 garbage-collection 有效管理 value log。<br>(4) 通过删除 LSM-Tree 日志而不牺牲一致性来优化性能。</p>
<h3 id="KV-分离"><a href="#KV-分离" class="headerlink" title="KV 分离"></a>KV 分离</h3><div align="center">
<img src="/2021/05/06/WiscKey-Separating-Keys-from-Values-in-SSD-conscious-Storage/3.png" style="zoom:100%;">
</div>

<p>KV 分离的设计要点如下：</p>
<ul>
<li>key 存在 LSM-Tree 上。</li>
<li>value 存在单独的 value log 中。</li>
<li>插入/更新数据的时候，首先将 value 追加到value log，然后将 key 插入 LSM-Tree 中。</li>
<li>删除数据的时候，只是将 key 在 LSM-Tree 中删除，value log 的数据不需要改变，因为 WiscKey 会有垃圾回收机制处理对应的 value。</li>
<li>读取数据时，先读 LSM-Tree，然后读 value log。</li>
</ul>
<h3 id="KV-分离对应的-Challenges"><a href="#KV-分离对应的-Challenges" class="headerlink" title="KV 分离对应的 Challenges"></a>KV 分离对应的 Challenges</h3><h4 id="Parallel-Range-Query"><a href="#Parallel-Range-Query" class="headerlink" title="Parallel Range Query"></a>Parallel Range Query</h4><ol>
<li>范围查询时，WiscKey 从 LSM-Tree 中读取多个 key 的元数据信息 &lt;key, address&gt;。</li>
<li>将这些 &lt;key, address&gt; 放入队列。</li>
<li>预读线程（默认32个）会从队列中获取 value 的地址，然后并行读取 value 数据。</li>
</ol>
<h4 id="Garbage-Collection"><a href="#Garbage-Collection" class="headerlink" title="Garbage Collection"></a>Garbage Collection</h4><div align="center">
<img src="/2021/05/06/WiscKey-Separating-Keys-from-Values-in-SSD-conscious-Storage/4.png" style="zoom:100%;">
</div>

<p>Value log 结构如图所示，其由 value_entry 组成，每个value_entry 是一个四元组 <code>(key size, value size, key, value)</code>。另外，Value log 有两个指针 head 和 tail，tail 指向 Value log 的起点；head 指向文件的尾部，所有新的数据都将追加到 head 位置。</p>
<p>垃圾回收时，线程将从 tail 指向的位置开始，每次读取一个 chunk 的数据（比如几MB），对于 chunk 中的每一个 value_entry，在 LSM-Tree 中查找 key 以便判断该 value_entry 是否仍然有效。如果有效，则将该条目追加到 head 指针指向的位置，并且需要更新 LSM-Tree 的记录，因为 value 的地址已经变了；如果无效，则将其舍弃。</p>
<p>同时，为了避免出现数据不一致（如在垃圾回收过程中发生了 crash），需要保证在释放对应的存储空间之前追加写入的新的有效 value 和新的 tail 指针持久化到了设备上。具体的步骤如下：</p>
<ul>
<li>垃圾回收在将 value 追加到 vLog 之后，在 vLog 上调用 fsync()</li>
<li>同步地将新的 value 地址和 tail 指针地址写入到 LSM-Tree 中。（tail 指针的存储形式为 <code>&lt;‘‘tail’’, tail-vLog-offset&gt;</code>）</li>
<li>最后回收 vLog 旧的数据空间</li>
</ul>
<h4 id="Crash-Consistency"><a href="#Crash-Consistency" class="headerlink" title="Crash Consistency"></a>Crash Consistency</h4><ol>
<li>如果不能在 LSM-Tree 中查询到对应的 key，那么处理方式和传统的 LSM-Tree 一样，返回空或者 key 不存在，即便其 value 已经写入到了 vLog 文件中，也会对其进行垃圾回收。</li>
<li>如果 LSM-Tree 中存在要查询的 Key，则会进行校验。校验首先校验从 LSM-Tree 中查询到的 value 地址信息是否在有效的 vLog 文件范围内；其次校验该地址对应的 value 上存取的 key 和要查询的 key 是否一致。如果校验失败，则删除 LSM-Tree 中相应 key，并返回 key 不存在。</li>
<li>另外，还可以引入 magic number 或 checksum 来校验 key 和 value 是否匹配。</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>WiscKey 基于 LevelDB，设计了一个针对 SSD 进行优化的持久化 KV 存储方案，它的核心思想就是将 key 和 value 分离，key 存储在 LSM-Tree 中，value 存储在 value log 中，保留了 LSM-Tree 的优势，减少读写放大，发挥了 SSD 顺序写与并行随机读性能好的优势，但在小 value 场景以及大数据集范围查询下，WiscKey 的性能比 LevelDB 差。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li>Lu L, Pillai T S, Arpaci-Dusseau A C, et al. WiscKey: separating keys from values in SSD-conscious storage[C] 14th USENIX Conference on File and Storage Technologies (FAST 16). 2016: 133-148.</li>
<li><a target="_blank" rel="noopener" href="http://cighao.com/2016/08/13/leveldb-source-analysis-01-introduction/">LevelDB 源码分析（一）：简介</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/30953751">WiscKey: Separating Keys from Values in SSD-conscious Storage</a></li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-04-05T08:33:37.000Z" title="2021/4/5 下午4:33:37">2021-04-05</time>发表</span><span class="level-item">16 分钟读完 (大约2329个字)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/04/05/RocksDB-WriteImpl-%E6%B5%81%E7%A8%8B/">RocksDB WriteImpl 流程</a></h1><div class="content"><p><code>本文对 RocksDB 6.7.3 版本的 WriteImpl 流程进行分析。</code></p>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>RocksDB 写入实现主要在 DBImpl::WriteImpl 中，过程主要分为以下三步：</p>
<ul>
<li>把 WriteBatch 加入队列，多个 WriteBatch 成为一个 WriteGroup</li>
<li>将该 WriteGroup 所有的记录对应的日志写到 WAL 文件中</li>
<li>将该 WriteGroup 所有的 WriteBatch 中的一条或者多条记录写到内存中的 Memtable 中</li>
</ul>
<p>其中，每个 WriteBatch 代表一个事务的提交，可以包含多条操作，可以通过调用 WriteBatch::Put/Delete 等操作将对应多条的 key/value 记录加入 WriteBatch 中。</p>
<h2 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h2><h3 id="WriteThread-JoinBatchGroup"><a href="#WriteThread-JoinBatchGroup" class="headerlink" title="WriteThread::JoinBatchGroup"></a>WriteThread::JoinBatchGroup</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">static WriteThread::AdaptationContext jbg_ctx(<span class="string">&quot;JoinBatchGroup&quot;</span>);</span><br><span class="line">void WriteThread::JoinBatchGroup(Writer* w) &#123;</span><br><span class="line">  TEST_SYNC_POINT_CALLBACK(<span class="string">&quot;WriteThread::JoinBatchGroup:Start&quot;</span>, w);</span><br><span class="line">  assert(w-&gt;batch != nullptr);</span><br><span class="line"></span><br><span class="line">  bool linked_as_leader = LinkOne(w, &amp;newest_writer_);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (linked_as_leader) &#123;</span><br><span class="line">    SetState(w, STATE_GROUP_LEADER);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  TEST_SYNC_POINT_CALLBACK(<span class="string">&quot;WriteThread::JoinBatchGroup:Wait&quot;</span>, w);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (!linked_as_leader) &#123;</span><br><span class="line">    /**</span><br><span class="line">     * Wait util:</span><br><span class="line">     * 1) An existing leader pick us as the new leader when it finishes</span><br><span class="line">     * 2) An existing leader pick us as its follewer and</span><br><span class="line">     * 2.1) finishes the memtable writes on our behalf</span><br><span class="line">     * 2.2) Or tell us to finish the memtable writes <span class="keyword">in</span> pralallel</span><br><span class="line">     * 3) (pipelined write) An existing leader pick us as its follower and</span><br><span class="line">     *    finish book-keeping and WAL write <span class="keyword">for</span> us, enqueue us as pending</span><br><span class="line">     *    memtable writer, and</span><br><span class="line">     * 3.1) we become memtable writer group leader, or</span><br><span class="line">     * 3.2) an existing memtable writer group leader tell us to finish memtable</span><br><span class="line">     *      writes <span class="keyword">in</span> parallel.</span><br><span class="line">     */</span><br><span class="line">    TEST_SYNC_POINT_CALLBACK(<span class="string">&quot;WriteThread::JoinBatchGroup:BeganWaiting&quot;</span>, w);</span><br><span class="line">    AwaitState(w, STATE_GROUP_LEADER | STATE_MEMTABLE_WRITER_LEADER |</span><br><span class="line">                      STATE_PARALLEL_MEMTABLE_WRITER | STATE_COMPLETED,</span><br><span class="line">               &amp;jbg_ctx);</span><br><span class="line">    TEST_SYNC_POINT_CALLBACK(<span class="string">&quot;WriteThread::JoinBatchGroup:DoneWaiting&quot;</span>, w);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>每个事务提交请求都会生成一个 WriteBatch 对象，进入 WriteImpl 函数后各自的线程首先调用 JoinBatchGroup 来加入到队列。该队列主要核心的实现在于 LinkOne 函数，通过 CAS 无锁将多个线程的请求组成请求链表：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">bool WriteThread::LinkOne(Writer* w, std::atomic&lt;Writer*&gt;* newest_writer) &#123;</span><br><span class="line">  assert(newest_writer != nullptr);</span><br><span class="line">  assert(w-&gt;state == STATE_INIT);</span><br><span class="line">  Writer* writers = newest_writer-&gt;load(std::memory_order_relaxed);</span><br><span class="line">  <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    // If write stall <span class="keyword">in</span> effect, and w-&gt;no_slowdown is not <span class="literal">true</span>,</span><br><span class="line">    // block here until stall is cleared. If its <span class="literal">true</span>, <span class="keyword">then</span> <span class="built_in">return</span></span><br><span class="line">    // immediately</span><br><span class="line">    <span class="keyword">if</span> (writers == &amp;write_stall_dummy_) &#123;</span><br><span class="line">      <span class="keyword">if</span> (w-&gt;no_slowdown) &#123;</span><br><span class="line">        w-&gt;status = Status::Incomplete(<span class="string">&quot;Write stall&quot;</span>);</span><br><span class="line">        SetState(w, STATE_COMPLETED);</span><br><span class="line">        <span class="built_in">return</span> <span class="literal">false</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      // Since no_slowdown is <span class="literal">false</span>, <span class="built_in">wait</span> here to be notified of the write</span><br><span class="line">      // stall clearing</span><br><span class="line">      &#123;</span><br><span class="line">        MutexLock lock(&amp;stall_mu_);</span><br><span class="line">        writers = newest_writer-&gt;load(std::memory_order_relaxed);</span><br><span class="line">        <span class="keyword">if</span> (writers == &amp;write_stall_dummy_) &#123;</span><br><span class="line">          stall_cv_.Wait();</span><br><span class="line">          // Load newest_writers_ again since it may have changed</span><br><span class="line">          writers = newest_writer-&gt;load(std::memory_order_relaxed);</span><br><span class="line">          <span class="built_in">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    w-&gt;link_older = writers;</span><br><span class="line">    <span class="keyword">if</span> (newest_writer-&gt;compare_exchange_weak(writers, w)) &#123;</span><br><span class="line">      <span class="built_in">return</span> (writers == nullptr);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>write_group 链表结构如下：</p>
<img src="/2021/04/05/RocksDB-WriteImpl-%E6%B5%81%E7%A8%8B/write_group.png" style="zoom:100%;">

<p>每个 writer 在头部插入，插入时如果发现 link_older 为空，则此 writer 成为 write_group 的 Leader（即链表尾为 Leader）。</p>
<p>在 JoinBatchGroup 中，如果 writer 不是 Leader（在后文把不是 Leader 的 writer 称为 Follower），则会调用 AwaitState 等待被唤醒。</p>
<blockquote>
<p>PS：由于条件锁 Context Switches 代价高，Rocksdb 在 AwaitState 也做了优化，将 pthread_cond_wait 拆成 3 步来做，本文不对该优化进行详细描述。</p>
</blockquote>
<h3 id="WriteImpl-写日志"><a href="#WriteImpl-写日志" class="headerlink" title="WriteImpl 写日志"></a>WriteImpl 写日志</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (w.state == WriteThread::STATE_GROUP_LEADER) &#123;</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  last_batch_group_size_ =</span><br><span class="line">      write_thread_.EnterAsBatchGroupLeader(&amp;w, &amp;wal_write_group);</span><br><span class="line">  const SequenceNumber current_sequence =</span><br><span class="line">      write_thread_.UpdateLastSequence(versions_-&gt;LastSequence()) + 1;</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (w.status.ok() &amp;&amp; !write_options.disableWAL) &#123;</span><br><span class="line">    PERF_TIMER_GUARD(write_wal_time);</span><br><span class="line">    stats-&gt;AddDBStats(InternalStats::kIntStatsWriteDoneBySelf, 1);</span><br><span class="line">    RecordTick(stats_, WRITE_DONE_BY_SELF, 1);</span><br><span class="line">    <span class="keyword">if</span> (wal_write_group.size &gt; 1) &#123;</span><br><span class="line">      stats-&gt;AddDBStats(InternalStats::kIntStatsWriteDoneByOther,</span><br><span class="line">                        wal_write_group.size - 1);</span><br><span class="line">      RecordTick(stats_, WRITE_DONE_BY_OTHER, wal_write_group.size - 1);</span><br><span class="line">    &#125;</span><br><span class="line">    w.status = WriteToWAL(wal_write_group, log_writer, log_used,</span><br><span class="line">                          need_log_sync, need_log_dir_sync, current_sequence);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  write_thread_.ExitAsBatchGroupLeader(wal_write_group, w.status);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>成为 Leader 的 writer，负责批量写入 WAL。在写 WAL 前，首先调用 EnterAsBatchGroupLeader 函数：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">size_t WriteThread::EnterAsBatchGroupLeader(Writer* leader,</span><br><span class="line">                                            WriteGroup* write_group) &#123;</span><br><span class="line">  assert(leader-&gt;link_older == nullptr);</span><br><span class="line">  assert(leader-&gt;batch != nullptr);</span><br><span class="line">  assert(write_group != nullptr);</span><br><span class="line"></span><br><span class="line">  size_t size = WriteBatchInternal::ByteSize(leader-&gt;batch);</span><br><span class="line"></span><br><span class="line">  // Allow the group to grow up to a maximum size, but <span class="keyword">if</span> the</span><br><span class="line">  // original write is small, <span class="built_in">limit</span> the growth so we <span class="keyword">do</span> not slow</span><br><span class="line">  // down the small write too much.</span><br><span class="line">  size_t max_size = max_write_batch_group_size_bytes;</span><br><span class="line">  const uint64_t min_batch_size_bytes = max_write_batch_group_size_bytes / 8;</span><br><span class="line">  <span class="keyword">if</span> (size &lt;= min_batch_size_bytes) &#123;</span><br><span class="line">    max_size = size + min_batch_size_bytes;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  leader-&gt;write_group = write_group;</span><br><span class="line">  write_group-&gt;leader = leader;</span><br><span class="line">  write_group-&gt;last_writer = leader;</span><br><span class="line">  write_group-&gt;size = 1;</span><br><span class="line">  Writer* newest_writer = newest_writer_.load(std::memory_order_acquire);</span><br><span class="line"></span><br><span class="line">  // This is safe regardless of any db mutex status of the <span class="built_in">caller</span>. Previous</span><br><span class="line">  // calls to ExitAsGroupLeader either didn<span class="string">&#x27;t call CreateMissingNewerLinks</span></span><br><span class="line"><span class="string">  // (they emptied the list and then we added ourself as leader) or had to</span></span><br><span class="line"><span class="string">  // explicitly wake us up (the list was non-empty when we added ourself,</span></span><br><span class="line"><span class="string">  // so we have already received our MarkJoined).</span></span><br><span class="line"><span class="string">  CreateMissingNewerLinks(newest_writer);</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  // Tricky. Iteration start (leader) is exclusive and finish</span></span><br><span class="line"><span class="string">  // (newest_writer) is inclusive. Iteration goes from old to new.</span></span><br><span class="line"><span class="string">  Writer* w = leader;</span></span><br><span class="line"><span class="string">  while (w != newest_writer) &#123;</span></span><br><span class="line"><span class="string">    w = w-&gt;link_newer;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    if (w-&gt;sync &amp;&amp; !leader-&gt;sync) &#123;</span></span><br><span class="line"><span class="string">      // Do not include a sync write into a batch handled by a non-sync write.</span></span><br><span class="line"><span class="string">      break;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    if (w-&gt;no_slowdown != leader-&gt;no_slowdown) &#123;</span></span><br><span class="line"><span class="string">      // Do not mix writes that are ok with delays with the ones that</span></span><br><span class="line"><span class="string">      // request fail on delays.</span></span><br><span class="line"><span class="string">      break;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    if (w-&gt;disable_wal != leader-&gt;disable_wal) &#123;</span></span><br><span class="line"><span class="string">      // Do not mix writes that enable WAL with the ones whose</span></span><br><span class="line"><span class="string">      // WAL disabled.</span></span><br><span class="line"><span class="string">      break;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    if (w-&gt;batch == nullptr) &#123;</span></span><br><span class="line"><span class="string">      // Do not include those writes with nullptr batch. Those are not writes,</span></span><br><span class="line"><span class="string">      // those are something else. They want to be alone</span></span><br><span class="line"><span class="string">      break;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    if (w-&gt;callback != nullptr &amp;&amp; !w-&gt;callback-&gt;AllowWriteBatching()) &#123;</span></span><br><span class="line"><span class="string">      // dont batch writes that don&#x27;</span>t want to be batched</span><br><span class="line">      <span class="built_in">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    auto batch_size = WriteBatchInternal::ByteSize(w-&gt;batch);</span><br><span class="line">    <span class="keyword">if</span> (size + batch_size &gt; max_size) &#123;</span><br><span class="line">      // Do not make batch too big</span><br><span class="line">      <span class="built_in">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    w-&gt;write_group = write_group;</span><br><span class="line">    size += batch_size;</span><br><span class="line">    write_group-&gt;last_writer = w;</span><br><span class="line">    write_group-&gt;size++;</span><br><span class="line">  &#125;</span><br><span class="line">  TEST_SYNC_POINT_CALLBACK(<span class="string">&quot;WriteThread::EnterAsBatchGroupLeader:End&quot;</span>, w);</span><br><span class="line">  <span class="built_in">return</span> size;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在这里，通过 CreateMissingNewerLinks 函数来生成一个双向链表，使得可以从 Leader 开始顺序写。创建完成反向写请求链表之后，则开始计算有多少个写请求可以批量的进行，同时更新 write_group 中的批量写尺寸以及个数等信息，EnterAsBatchGroupLeader 取队列时会把此刻所有的 writer 一次性全取完。</p>
<p>该操作完成之后，则进入写 WAL 的流程了。调用 WriteToWAL，在 MergeBatch 函数中，将根据 write_group 生成一个 merged_batch，该 merged_batch 中记录着应当被写入 WAL 的内容。接着就通过 WriteToWAL 将 merged_batch 写入 WAL 中，这里会根据是否设置了 sync 来决定是否对 WAL 进行落盘操作。</p>
<blockquote>
<p>PS：这里有一个优化点，在生成 merged_batch 的时候，假设该写请求的尺寸为一并且该请求需要写 WAL，则 merged_batch 直接复用了该写请求；反之则会复用一个 tmp_batch_ 对象避免频繁的生成 WriteBatch 对象。在写完 WAL 之后，假设复用了 tmp_batch_，则会清空该对象。</p>
</blockquote>
<p>最后，调用 ExitAsBatchGroupLeader，该函数会决定该 Leader 是否为 STATE_MEMTABLE_WRITER_LEADER（MEMTABLE_WRITER_LEADER数量 &lt;= GROUP_LEADER数量），从而进行写 Memtable 流程。</p>
<h3 id="WriteImpl-写-Memtable"><a href="#WriteImpl-写-Memtable" class="headerlink" title="WriteImpl 写 Memtable"></a>WriteImpl 写 Memtable</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">WriteThread::WriteGroup memtable_write_group;</span><br><span class="line">  <span class="keyword">if</span> (w.state == WriteThread::STATE_MEMTABLE_WRITER_LEADER) &#123;</span><br><span class="line">    PERF_TIMER_GUARD(write_memtable_time);</span><br><span class="line">    assert(w.ShouldWriteToMemtable());</span><br><span class="line">    write_thread_.EnterAsMemTableWriter(&amp;w, &amp;memtable_write_group);</span><br><span class="line">    <span class="keyword">if</span> (memtable_write_group.size &gt; 1 &amp;&amp;</span><br><span class="line">        immutable_db_options_.allow_concurrent_memtable_write) &#123;</span><br><span class="line">      write_thread_.LaunchParallelMemTableWriters(&amp;memtable_write_group);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      memtable_write_group.status = WriteBatchInternal::InsertInto(</span><br><span class="line">          memtable_write_group, w.sequence, column_family_memtables_.get(),</span><br><span class="line">          &amp;flush_scheduler_, &amp;trim_history_scheduler_,</span><br><span class="line">          write_options.ignore_missing_column_families, 0 /*log_number*/, this,</span><br><span class="line">          <span class="literal">false</span> /*concurrent_memtable_writes*/, seq_per_batch_, batch_per_txn_);</span><br><span class="line">      versions_-&gt;SetLastSequence(memtable_write_group.last_sequence);</span><br><span class="line">      write_thread_.ExitAsMemTableWriter(&amp;w, memtable_write_group);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (w.state == WriteThread::STATE_PARALLEL_MEMTABLE_WRITER) &#123;</span><br><span class="line">    assert(w.ShouldWriteToMemtable());</span><br><span class="line">    ColumnFamilyMemTablesImpl column_family_memtables(</span><br><span class="line">        versions_-&gt;GetColumnFamilySet());</span><br><span class="line">    w.status = WriteBatchInternal::InsertInto(</span><br><span class="line">        &amp;w, w.sequence, &amp;column_family_memtables, &amp;flush_scheduler_,</span><br><span class="line">        &amp;trim_history_scheduler_, write_options.ignore_missing_column_families,</span><br><span class="line">        0 /*log_number*/, this, <span class="literal">true</span> /*concurrent_memtable_writes*/,</span><br><span class="line">        <span class="literal">false</span> /*seq_per_batch*/, 0 /*batch_cnt*/, <span class="literal">true</span> /*batch_per_txn*/,</span><br><span class="line">        write_options.memtable_insert_hint_per_batch);</span><br><span class="line">    <span class="keyword">if</span> (write_thread_.CompleteParallelMemTableWriter(&amp;w)) &#123;</span><br><span class="line">      MemTableInsertStatusCheck(w.status);</span><br><span class="line">      versions_-&gt;SetLastSequence(w.write_group-&gt;last_sequence);</span><br><span class="line">      write_thread_.ExitAsMemTableWriter(&amp;w, *w.write_group);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>RocksDB 有一个 allow_concurrent_memtable_write 的配置项，开启后可以并发写 memtable（memtable 能设置并发写，但是 WAL 文件不能，因为 WAL 是一个追加写的文件，多个 writer 必须要串行化），所以接下来分为串行写和并行写来进行分析。</p>
<h4 id="串行写-Memtable"><a href="#串行写-Memtable" class="headerlink" title="串行写 Memtable"></a>串行写 Memtable</h4><p>Leader 调用 InsertInto，对 write_group 进行遍历，将 Leader 和 Follower 的 WriteBatch 写入。之后调用 ExitAsMemTableWriter，把所有 Follower 的状态设置为 STATE_COMPLETED，将它们唤醒，最后再把 Leader 的状态设置为 STATE_COMPLETED。</p>
<h4 id="并行写-Memtable"><a href="#并行写-Memtable" class="headerlink" title="并行写 Memtable"></a>并行写 Memtable</h4><p>调用 LaunchParallelMemTableWriters，遍历 write_group 把 Leader 和 Follower 的状态都设置为 STATE_PARALLEL_MEMTABLE_WRITER，将等待的线程唤醒。最后所有 writer 通过调用 InsertInto 来将 WriteBatch 写入 MemTable 中。writer 完成了 MemTable 的写操作之后，都会调用 CompleteParallelMemTableWriter 函数。该函数会将该 write_group 中运行的任务数减一，当运行中的任务数为零的时候就代表了所有的线程都完成了操作，调用 ExitAsMemTableWriter 把 Leader 的状态设置为 STATE_COMPLETED，反之则会进入等待状态，等待当前其他的写任务完成。</p>
<p>无论是串行写还是并行写，写入 MemTable 完成之后，还有一项工作，就是在取队列时获取 newest_writer_ 和当前时间点处，可能又有很多的写请求产生了，所以批量任务中最后一个完成的线程必须负责重新指定 Leader 给堆积写请求链表的尾部，让其接过 Leader 角色继续进行批量提交。可以看到，串行写和并行写最后都会调用 ExitAsMemTableWriter，正是在该函数中完成了该项工作。</p>
<blockquote>
<p>PS：在高并发场景下，Follow 调用 AwaitState 的平均等待时延差不多是写 WAL 时延的两倍。因为获取 newest_writer_ 后，可能又来了许多写请求，这些写请求先要等待此时的 Leader 完成写流程，还要等待下个 Leader，也就是和这些写请求是同一个 write_group 的 Leader 完成写 WAL 才能被唤醒。</p>
</blockquote>
<h2 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h2><img src="/2021/04/05/RocksDB-WriteImpl-%E6%B5%81%E7%A8%8B/rocksdb_write.jpg" style="zoom:100%;">

<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/facebook/rocksdb/tree/v6.7.3">Rocksdb Source Code 6.7.3</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_43479736/article/details/109056437">rocksdb写流程DBImpl::WriteImpl()源代码分析</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/161797527">RocksDB写入流程</a></li>
<li><a target="_blank" rel="noopener" href="https://gocode.cc/project/13/article/183">RocksDB 写流程分析</a></li>
</ol>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/2/">上一页</a></div><div class="pagination-next"><a href="/page/4/">下一页</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><a class="pagination-link" href="/page/2/">2</a></li><li><a class="pagination-link is-current" href="/page/3/">3</a></li><li><a class="pagination-link" href="/page/4/">4</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/7/">7</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/dog.jpg" alt="Iggie Wang (王亮)"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Iggie Wang (王亮)</p><p class="is-size-6 is-block">WNLO, HUST</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Wuhan, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">21</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">0</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">9</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/hey-kong" target="_blank" rel="noopener">关注我</a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/hey-kong" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="iggiewang@gmail.com" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Email</span></span><span class="level-right"><span class="level-item tag">iggiewang@gmail.com</span></span></a></li></ul></div></div></div><!--!--><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2022-01-26T13:27:39.000Z">2022-01-26</time></p><p class="title"><a href="/2022/01/26/CentOS7-6%E9%83%A8%E7%BD%B2k8s/">CentOS7.6部署k8s</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-10-01T06:46:46.000Z">2021-10-01</time></p><p class="title"><a href="/2021/10/01/ALEX-An-Updatable-Adaptive-Learned-Index/">ALEX: An Updatable Adaptive Learned Index</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-09-30T07:54:13.000Z">2021-09-30</time></p><p class="title"><a href="/2021/09/30/gRPC-%E5%9F%BA%E7%A1%80/">gRPC 基础</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-09-20T06:13:26.000Z">2021-09-20</time></p><p class="title"><a href="/2021/09/20/From-WiscKey-to-Bourbon-A-Learned-Index-for-Log-Structured-Merge-Trees/">From WiscKey to Bourbon: A Learned Index for Log-Structured Merge Trees</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-08-09T05:19:49.000Z">2021-08-09</time></p><p class="title"><a href="/2021/08/09/Octopus-an-RDMA-enabled-Distributed-Persistent-Memory-File-System/">Octopus: an RDMA-enabled Distributed Persistent Memory File System</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2022/01/"><span class="level-start"><span class="level-item">一月 2022</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/10/"><span class="level-start"><span class="level-item">十月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/09/"><span class="level-start"><span class="level-item">九月 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/08/"><span class="level-start"><span class="level-item">八月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/07/"><span class="level-start"><span class="level-item">七月 2021</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/05/"><span class="level-start"><span class="level-item">五月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/04/"><span class="level-start"><span class="level-item">四月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/03/"><span class="level-start"><span class="level-item">三月 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/02/"><span class="level-start"><span class="level-item">二月 2021</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/01/"><span class="level-start"><span class="level-item">一月 2021</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/CSAPP/"><span class="tag">CSAPP</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Cache/"><span class="tag">Cache</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Database/"><span class="tag">Database</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/File-System/"><span class="tag">File System</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Key-Value-Store/"><span class="tag">Key-Value Store</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Learned-Index/"><span class="tag">Learned Index</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/RDMA/"><span class="tag">RDMA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Rocksdb/"><span class="tag">Rocksdb</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/k8s/"><span class="tag">k8s</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Iggie Wang&#039;s Cyberspace</a><p class="is-size-7"><span>&copy; 2022 王亮</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>